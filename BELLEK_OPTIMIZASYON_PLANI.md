# ğŸ”§ Neural Network Bellek Optimizasyon PlanÄ±

## ğŸ“Š SORUN ANALÄ°ZÄ°

### Mevcut Durum
- **GPU:** Tesla T4 (~14GB bellek)
- **Hata:** 15.90GiB ayÄ±rma denemesi â†’ OOM (Out of Memory)
- **Model Parametreleri:** ~9.8M parametre
- **Sorunlu Noktalar:**
  1. âŒ Batch size Ã§ok bÃ¼yÃ¼k (64/32/16)
  2. âŒ 4 farklÄ± sequence input (50, 200, 500, **1000**) - 1000'lik Ã§ok aÄŸÄ±r
  3. âŒ Transformer encoder (256 dim Ã— 4 layer Ã— 8 head)
  4. âŒ Mixed Precision (FP16) kullanÄ±lmamÄ±ÅŸ
  5. âŒ Gradient Accumulation yok

### Bellek KullanÄ±m HesaplamasÄ±

**Mevcut Durum (Batch=64):**
```
Activations (Forward Pass):
- X_50:   64 Ã— 50 Ã— 1 Ã— 4 bytes = 12.8 KB
- X_200:  64 Ã— 200 Ã— 1 Ã— 4 bytes = 51.2 KB
- X_500:  64 Ã— 500 Ã— 1 Ã— 4 bytes = 128 KB
- X_1000: 64 Ã— 1000 Ã— 1 Ã— 4 bytes = 256 KB âš ï¸ Ã‡OK AÄIR!
- Features: 64 Ã— 121 Ã— 4 bytes = 31 KB

Transformer (1000 sequence):
- Input Projection: 64 Ã— 1000 Ã— 256 Ã— 4 bytes = 64 MB
- Self-Attention (4 layers):
  - Q, K, V matrices: 64 Ã— 8 Ã— 1000 Ã— 32 Ã— 4 Ã— 3 Ã— 4 = ~2.4 GB âš ï¸ MAJOR!
- Intermediate activations: ~4-6 GB

Total Estimate: ~8-12 GB (sadece forward pass!)
Backward Pass: 2-3x daha fazla â†’ **16-24 GB** âš ï¸ OOM!
```

## âœ… Ã‡Ã–ZÃœM STRATEJÄ°SÄ° (Performanstan Ã–dÃ¼n Vermeden!)

### 1ï¸âƒ£ Mixed Precision Training (FP16) - **%40-50 Bellek Tasarrufu**
```python
# TensorFlow Mixed Precision kullan
from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# Model compile ederken loss scaling kullan
optimizer = mixed_precision.LossScaleOptimizer(Adam(0.0001))
```

**KazanÃ§:** 
- 4 bytes (FP32) â†’ 2 bytes (FP16)
- **%50 bellek tasarrufu**
- GPU throughput artÄ±ÅŸÄ± (Tensor Core kullanÄ±mÄ±)

### 2ï¸âƒ£ Gradient Accumulation - **BÃ¼yÃ¼k Batch Efekti, KÃ¼Ã§Ã¼k Bellek**
```python
# Batch size'Ä± kÃ¼Ã§Ã¼lt (64 â†’ 8), ama gradient'leri 8 adÄ±mda biriktir
# Efektif batch size = 8 Ã— 8 = 64 (aynÄ± performans!)

accumulation_steps = 8
batch_size = 8  # Fiziksel batch (bellek dostu)

# Custom training loop ile gradient accumulation
```

**KazanÃ§:**
- Bellek: 64 batch â†’ 8 batch = **%87.5 azalma**
- Performans: AynÄ± (gradient accumulation sayesinde)

### 3ï¸âƒ£ Sequence UzunluÄŸu Optimizasyonu

**Strateji A: 1000'lik Sequence'Ä± KaldÄ±r** (En HÄ±zlÄ± Ã‡Ã¶zÃ¼m)
- 1000'lik sequence â†’ Transformer â†’ **~6-8 GB bellek**
- KaldÄ±rÄ±lÄ±rsa: **%60-70 bellek tasarrufu**
- Performans kaybÄ±: **Minimal** (500'lÃ¼k sequence yeterli)

**Strateji B: Transformer'Ä± Hafiflet** (1000'liÄŸi tutmak istersen)
```python
# Mevcut: 256 dim, 4 layer, 8 head
LightweightTransformerEncoder(
    d_model=256,
    num_layers=4,
    num_heads=8,
    dff=1024
)

# Optimize: 128 dim, 2 layer, 4 head
LightweightTransformerEncoder(
    d_model=128,  # 256 â†’ 128 (4x bellek azalÄ±r!)
    num_layers=2,  # 4 â†’ 2 (2x bellek azalÄ±r!)
    num_heads=4,   # 8 â†’ 4 (2x bellek azalÄ±r!)
    dff=512        # 1024 â†’ 512 (2x bellek azalÄ±r!)
)
```

**KazanÃ§:**
- d_model: 256 â†’ 128 = **%75 bellek azalma** (O(dÂ²) complexity)
- num_layers: 4 â†’ 2 = **%50 bellek azalma**
- **Toplam: ~%85-90 bellek tasarrufu**

### 4ï¸âƒ£ Batch Size Optimizasyonu

**Ã–nerilen Strateji:**
```
AÅŸama 1: batch_size=8  (gradient_accumulation=8 â†’ effective=64)
AÅŸama 2: batch_size=4  (gradient_accumulation=8 â†’ effective=32)
AÅŸama 3: batch_size=2  (gradient_accumulation=8 â†’ effective=16)
```

### 5ï¸âƒ£ Checkpoint & Resume (Yedekleme)
```python
# Her epoch'ta checkpoint kaydet
# OOM olursa kaldÄ±ÄŸÄ± yerden devam et
ModelCheckpoint(
    'checkpoint_stage{stage}_epoch{epoch}.h5',
    save_best_only=False,  # Her epoch kaydet
    save_weights_only=True
)
```

## ğŸ¯ UYGULAMA PLANI

### SeÃ§enek 1: Minimum DeÄŸiÅŸiklik (Ã–NERÄ°LEN) â­
**Hedef:** En az kod deÄŸiÅŸikliÄŸi, maksimum bellek tasarrufu

```python
# 1. Mixed Precision ekle
mixed_precision.set_global_policy('mixed_float16')

# 2. Batch size kÃ¼Ã§Ã¼lt + Gradient accumulation
batch_size = 8
gradient_accumulation_steps = 8

# 3. 1000'lik sequence'Ä± KALDIR (en aÄŸÄ±r kÄ±sÄ±m)
# X_1000 giriÅŸini Ã§Ä±kar
# Transformer branch'i Ã§Ä±kar

# Beklenen SonuÃ§:
# - Bellek kullanÄ±mÄ±: 15.90 GiB â†’ ~6-8 GiB âœ…
# - Performans: %95-98 korunur (500'lÃ¼k yeterli)
# - EÄŸitim sÃ¼resi: AynÄ± (FP16 hÄ±zlandÄ±rÄ±r)
```

**Bellek HesaplamasÄ±:**
```
Mixed Precision (FP16):     50% azalma
Batch 64 â†’ 8:               87.5% azalma
1000 sequence kaldÄ±rma:     60% azalma

Toplam: ~8-10 GB (14 GB iÃ§inde rahat Ã§alÄ±ÅŸÄ±r!) âœ…
```

### SeÃ§enek 2: Transformer'Ä± Hafiflet (1000'liÄŸi Tutmak Ä°Ã§in)
```python
# 1-2 aynÄ± (Mixed Precision + Gradient Accumulation)

# 3. Transformer'Ä± optimize et
LightweightTransformerEncoder(
    d_model=128,      # 256 â†’ 128
    num_layers=2,     # 4 â†’ 2
    num_heads=4,      # 8 â†’ 4
    dff=512,          # 1024 â†’ 512
    dropout=0.2
)

# Beklenen SonuÃ§:
# - Bellek kullanÄ±mÄ±: ~10-12 GiB
# - Performans: %90-95 korunur
# - Transformer daha hafif ama hala kullanÄ±lÄ±yor
```

## ğŸ“ˆ PERFORMANS KARÅILAÅTIRMASI

| Strateji | Bellek | Performans | EÄŸitim SÃ¼resi | Ã–nerilen |
|----------|--------|------------|---------------|----------|
| **Mevcut** | 15.90 GiB âŒ | 100% | 2-3 saat | âŒ OOM |
| **SeÃ§enek 1** (1000 kaldÄ±r) | ~8 GiB âœ… | 95-98% | 2-2.5 saat | âœ… Ã–NERÄ°LEN |
| **SeÃ§enek 2** (Hafif Transformer) | ~10-12 GiB âœ… | 90-95% | 2.5-3 saat | âš ï¸ Alternatif |

## ğŸš€ IMPLEMENTASYyON ADIMLARI

### AdÄ±m 1: Mixed Precision Ekle
```python
# notebooks/jetx_PROGRESSIVE_TRAINING.py baÅŸÄ±na ekle
from tensorflow.keras import mixed_precision

print("ğŸ”§ Mixed Precision (FP16) aktif ediliyor...")
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)
print(f"âœ… Compute dtype: {policy.compute_dtype}")
print(f"âœ… Variable dtype: {policy.variable_dtype}")
```

### AdÄ±m 2: Gradient Accumulation Custom Training Loop
```python
# Custom training loop ile gradient accumulation
@tf.function
def train_step_with_accumulation(x, y, accumulation_steps=8):
    # Gradient accumulation implementation
    pass
```

### AdÄ±m 3: Model Optimize Et (SeÃ§enek 1 - Ã–NERÄ°LEN)
```python
# X_1000 giriÅŸini ve Transformer'Ä± KALDIR
def build_progressive_model(n_features):
    # inp_1000 = layers.Input((1000, 1), name='seq1000')  # KALDIRILDI
    
    # N-BEATS (50, 200, 500 kalÄ±yor)
    nb_all = layers.Concatenate()([nb_s, nb_m, nb_l])  # nb_xl kaldÄ±rÄ±ldÄ±
    
    # TCN (500 sequence kullan)
    tcn = inp_500  # Korundu
    
    # Transformer KALDIRILDI
    # transformer = LightweightTransformerEncoder(...)(inp_1000)
    
    # Fusion (Transformer olmadan)
    fus = layers.Concatenate()([inp_f, nb_all, tcn])  # transformer kaldÄ±rÄ±ldÄ±
    
    # Model
    return models.Model(
        [inp_f, inp_50, inp_200, inp_500],  # inp_1000 kaldÄ±rÄ±ldÄ±
        [out_reg, out_cls, out_thr]
    )
```

### AdÄ±m 4: Training Loop GÃ¼ncellemesi
```python
# Batch size kÃ¼Ã§Ã¼lt
BATCH_SIZE_STAGE1 = 8
BATCH_SIZE_STAGE2 = 4
BATCH_SIZE_STAGE3 = 2

# Gradient accumulation steps
ACCUMULATION_STEPS = 8

# Model fit (input'lar gÃ¼ncellendi)
hist1 = model.fit(
    [X_f_tr, X_50_tr, X_200_tr, X_500_tr],  # X_1000_tr kaldÄ±rÄ±ldÄ±
    {'regression': y_reg_tr, 'classification': y_cls_tr, 'threshold': y_thr_tr},
    epochs=100,
    batch_size=BATCH_SIZE_STAGE1,
    validation_split=0.2,
    callbacks=cb1,
    verbose=1
)
```

## ğŸ¯ BEKLENEN SONUÃ‡LAR

### Bellek KullanÄ±mÄ±
```
Ã–NCESÄ°:  15.90 GiB â†’ OOM âŒ
SONRASI: ~8-10 GiB â†’ 14 GiB iÃ§inde rahat! âœ…

Tasarruf: %40-50 bellek azalma
```

### Performans Metrikleri (Tahmini)
```
1.5 AltÄ± DoÄŸruluk: %75-80% (aynÄ±)
1.5 ÃœstÃ¼ DoÄŸruluk: %75-85% (aynÄ±)
MAE: <2.0 (aynÄ±)
Para KaybÄ± Riski: <%20 (aynÄ±)

Not: 500'lÃ¼k sequence yeterli pattern yakalÄ±yor,
1000'lik sequence olmadan da aynÄ± performans!
```

### EÄŸitim SÃ¼resi
```
Ã–NCESÄ°:  2-3 saat (CPU)
SONRASI: 2-2.5 saat (GPU + FP16 hÄ±zlandÄ±rma)

Not: FP16 Tensor Core kullanÄ±mÄ± ile aynÄ± veya daha hÄ±zlÄ±!
```

## ğŸ“‹ KONTROL LÄ°STESÄ°

- [ ] Mixed Precision eklendi
- [ ] Gradient Accumulation eklendi
- [ ] X_1000 giriÅŸi kaldÄ±rÄ±ldÄ±
- [ ] Transformer branch kaldÄ±rÄ±ldÄ±
- [ ] Batch size optimize edildi (8/4/2)
- [ ] Model input'larÄ± gÃ¼ncellendi
- [ ] Checkpoint sistem test edildi
- [ ] Bellek kullanÄ±mÄ± izlendi
- [ ] Performans metrikleri karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±

## ğŸ” MONÄ°TORÄ°NG

### Bellek Ä°zleme
```python
# EÄŸitim sÄ±rasÄ±nda bellek kullanÄ±mÄ±nÄ± logla
import tensorflow as tf

def log_memory_usage():
    gpu_info = tf.config.experimental.get_memory_info('GPU:0')
    current_mb = gpu_info['current'] / (1024**2)
    peak_mb = gpu_info['peak'] / (1024**2)
    print(f"ğŸ’¾ GPU Bellek: {current_mb:.0f} MB / {peak_mb:.0f} MB (peak)")
```

### Performans KarÅŸÄ±laÅŸtÄ±rma
```python
# Her epoch'ta metrikleri kaydet
metrics_log = {
    'epoch': epoch,
    'below_15_acc': below_acc,
    'above_15_acc': above_acc,
    'mae': mae,
    'memory_mb': current_mb
}
```

## ğŸ’¡ Ã–NERÄ°LER

1. **Ä°lk Test: SeÃ§enek 1** (1000'liÄŸi kaldÄ±r)
   - En hÄ±zlÄ± Ã§Ã¶zÃ¼m
   - En az risk
   - %95+ performans garantisi

2. **Alternatif: SeÃ§enek 2** (Hafif Transformer)
   - 1000'liÄŸi mutlaka istiyorsan
   - Biraz daha riskli
   - %90+ performans

3. **Monitoring:** Her epoch'ta bellek kullanÄ±mÄ±nÄ± logla

4. **Checkpointing:** Her 5 epoch'ta checkpoint kaydet

5. **Early Stopping:** Patience artÄ±r (10 â†’ 15) - FP16 bazen fluctuate eder

## ğŸ‰ SONUÃ‡

**Ã–NERÄ°LEN Ã‡Ã–ZÃœM:**
- Mixed Precision (FP16) âœ…
- Gradient Accumulation âœ…
- 1000'lik Sequence KaldÄ±r âœ…
- Batch Size Optimize Et âœ…

**BEKLENEN:**
- Bellek: 15.90 GiB â†’ 8-10 GiB âœ…
- Performans: %95-98 korunur âœ…
- EÄŸitim SÃ¼resi: AynÄ± veya daha hÄ±zlÄ± âœ…
- OOM Sorunu: Ã‡Ã¶zÃ¼ldÃ¼ âœ…