{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ JetX Predictor - ModÃ¼ler EÄŸitim Orkestrasyonu (v6.1 - FIXED)\n",
    "\n",
    "Bu notebook, JetX modellerini **tek tek** veya **hepsini bir arada** eÄŸitmenize olanak tanÄ±r.\n",
    "Google Colab T4 GPU ortamÄ± iÃ§in optimize edilmiÅŸ ve dosya yolu/veri hatalarÄ± giderilmiÅŸtir.\n",
    "\n",
    "### ğŸ“‹ Ã–zellikler:\n",
    "- **Otomatik Veri TemizliÄŸi:** VeritabanÄ±ndaki bozuk karakterleri temizler.\n",
    "- **Path DÃ¼zeltmesi:** Alt iÅŸlemlerin proje dosyalarÄ±nÄ± bulmasÄ±nÄ± saÄŸlar.\n",
    "- **ParÃ§alÄ± EÄŸitim:** Ä°stediÄŸiniz modeli seÃ§ip sadece onu eÄŸitebilirsiniz.\n",
    "- **AnÄ±nda Ä°ndirme:** Her eÄŸitim bitiminde model dosyalarÄ± otomatik iner.\n",
    "- **T4 Optimizasyonu:** BÃ¼yÃ¼k batch size ve mixed precision ayarlarÄ±.\n",
    "\n",
    "### ğŸ› ï¸ NasÄ±l KullanÄ±lÄ±r:\n",
    "1. **HÃ¼cre 2 (Kontrol Paneli)**'den eÄŸitmek istediÄŸiniz modelleri `True` yapÄ±n.\n",
    "2. **HÃ¼cre 1** ve **HÃ¼cre 3**'Ã¼ Ã§alÄ±ÅŸtÄ±rarak ortamÄ± hazÄ±rlayÄ±n (Veri temizliÄŸi burada yapÄ±lÄ±r).\n",
    "3. Ä°lgili modelin hÃ¼cresini Ã§alÄ±ÅŸtÄ±rÄ±n.\n",
    "\n",
    "ğŸ’¾ **Yedekleme:** Dosyalar `/content/drive/MyDrive/JetX_Models_Backup_v6/` klasÃ¶rÃ¼ne de yedeklenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ› ï¸ HÃœCRE 1: ROBUST ORTAM KURULUMU VE YARDIMCI FONKSÄ°YONLAR\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸš€ JetX Predictor v6.1 - Environment Setup\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Google Drive BaÄŸlantÄ±sÄ±\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    from google.colab import files\n",
    "    drive.mount('/content/drive')\n",
    "    BACKUP_DIR = '/content/drive/MyDrive/JetX_Models_Backup_v6'\n",
    "    os.makedirs(BACKUP_DIR, exist_ok=True)\n",
    "    print(f\"âœ… Google Drive baÄŸlandÄ±. Yedekleme: {BACKUP_DIR}\")\n",
    "except:\n",
    "    BACKUP_DIR = None\n",
    "    print(\"âš ï¸ Google Drive baÄŸlanamadÄ±. Sadece lokal indirme yapÄ±lacak.\")\n",
    "\n",
    "# 2. Paket KurulumlarÄ±\n",
    "print(\"\\nğŸ“¦ GEREKLÄ° PAKETLER KURULUYOR...\")\n",
    "install_order = [\n",
    "    ['numpy', 'pandas', 'scikit-learn', 'joblib', 'scipy'],\n",
    "    ['tensorflow', 'torch'],\n",
    "    ['catboost', 'xgboost', 'lightgbm', 'autogluon', 'pytorch-tabnet'],\n",
    "    ['optuna', 'shap', 'nolds', 'PyWavelets'],\n",
    "    ['matplotlib', 'seaborn', 'tqdm', 'pyyaml']\n",
    "]\n",
    "for packages in install_order:\n",
    "    try:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q'] + packages, check=False)\n",
    "    except: pass\n",
    "print(\"âœ… Paketler hazÄ±r.\")\n",
    "\n",
    "# 3. Proje Ã‡ekme\n",
    "PROJECT_ROOT = '/content/jetxpredictor'\n",
    "if not os.path.exists(PROJECT_ROOT):\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/onndd/jetxpredictor.git'], check=True)\n",
    "else:\n",
    "    # GÃ¼ncel kodu Ã§ek\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    subprocess.run(['git', 'pull'], check=False)\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"âœ… Proje dizini: {os.getcwd()}\")\n",
    "\n",
    "# 4. Ä°NDÄ°RME VE YEDEKLEME FONKSÄ°YONU (KRÄ°TÄ°K)\n",
    "def backup_and_download(file_path, description=\"Dosya\"):\n",
    "    \"\"\"DosyayÄ± Drive'a yedekler ve tarayÄ±cÄ±dan indirir\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ {description} bulunamadÄ±: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Drive'a kopyala\n",
    "    if BACKUP_DIR:\n",
    "        try:\n",
    "            # EÄŸer klasÃ¶rse zip'le\n",
    "            if os.path.isdir(file_path):\n",
    "                zip_name = f\"{file_path}.zip\"\n",
    "                if not os.path.exists(zip_name):\n",
    "                     shutil.make_archive(file_path, 'zip', file_path)\n",
    "                file_to_copy = zip_name\n",
    "            else:\n",
    "                file_to_copy = file_path\n",
    "                \n",
    "            dest = os.path.join(BACKUP_DIR, os.path.basename(file_to_copy))\n",
    "            if os.path.isdir(file_to_copy):\n",
    "                 if os.path.exists(dest):\n",
    "                     shutil.rmtree(dest)\n",
    "                 shutil.copytree(file_to_copy, dest)\n",
    "            else:\n",
    "                 shutil.copy(file_to_copy, dest)\n",
    "            print(f\"ğŸ’¾ Drive'a yedeklendi: {dest}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Drive yedekleme hatasÄ±: {e}\")\n",
    "    \n",
    "    # Ä°ndir\n",
    "    try:\n",
    "        if os.path.isdir(file_path):\n",
    "            # KlasÃ¶rse ziplediÄŸimizi indir\n",
    "            if not os.path.exists(f\"{file_path}.zip\"):\n",
    "                shutil.make_archive(file_path, 'zip', file_path)\n",
    "            files.download(f\"{file_path}.zip\")\n",
    "        else:\n",
    "            files.download(file_path)\n",
    "        print(f\"â¬‡ï¸ {description} indiriliyor...\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Ä°ndirme hatasÄ±: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ HÃœCRE 2: KONTROL PANELÄ° (BURAYI DÃœZENLEYÄ°N)\n",
    "# Hangi modeli eÄŸitmek istiyorsanÄ±z onu True yapÄ±n.\n",
    "\n",
    "# ğŸš€ MODEL SEÃ‡Ä°MÄ° (Tek tek True yapabilirsiniz)\n",
    "TRAIN_PROGRESSIVE = True     # 1. Ana Model (NN Transformer - En Ã–nemli)\n",
    "TRAIN_ULTRA = False          # 2. Ultra Aggressive\n",
    "TRAIN_CATBOOST = False       # 3. CatBoost Ensemble\n",
    "TRAIN_AUTOGLUON = False      # 4. AutoGluon\n",
    "TRAIN_TABNET = False         # 5. TabNet\n",
    "TRAIN_RL_AGENT = False       # 6. RL Agent\n",
    "TRAIN_META_MODEL = False     # 7. Meta Model\n",
    "\n",
    "# âš¡ AYARLAR\n",
    "QUICK_TEST_MODE = False  # HÄ±zlÄ± test iÃ§in True yapÄ±n\n",
    "MODELS_DIR = 'models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# ğŸ“Š EÄÄ°TÄ°M KONFIGURASYONU (T4 GPU OPTIMIZE)\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 5 if QUICK_TEST_MODE else 100,\n",
    "    'catboost_iterations': 100 if QUICK_TEST_MODE else 1500,\n",
    "    'autogluon_time': 60 if QUICK_TEST_MODE else 3600,\n",
    "    # T4 GPU 16GB VRAM olduÄŸu iÃ§in batch size'Ä± artÄ±rÄ±yoruz\n",
    "    'batch_size': 16 if QUICK_TEST_MODE else 256 \n",
    "}\n",
    "\n",
    "print(f\"âš¡ QUICK_TEST_MODE: {QUICK_TEST_MODE}\")\n",
    "print(f\"ğŸ”§ BATCH SIZE: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"ğŸ¯ SEÃ‡Ä°LEN MODELLER:\")\n",
    "if TRAIN_PROGRESSIVE: print(\"  - Progressive Transformer\")\n",
    "if TRAIN_ULTRA: print(\"  - Ultra Aggressive\")\n",
    "if TRAIN_CATBOOST: print(\"  - CatBoost Ensemble\")\n",
    "if TRAIN_AUTOGLUON: print(\"  - AutoGluon\")\n",
    "if TRAIN_TABNET: print(\"  - TabNet\")\n",
    "if TRAIN_RL_AGENT: print(\"  - RL Agent\")\n",
    "if TRAIN_META_MODEL: print(\"  - Meta Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š HÃœCRE 3: VERÄ° HAZIRLIÄI VE TEMÄ°ZLÄ°ÄÄ° (FIXED)\n",
    "# VeritabanÄ±ndaki bozuk karakterleri temizleyip diske yazar.\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“Š Veri ve VeritabanÄ± kontrol ediliyor...\")\n",
    "db_path = 'jetx_data.db'\n",
    "\n",
    "# EÄŸer veritabanÄ± yoksa sentetik oluÅŸtur\n",
    "if not os.path.exists(db_path):\n",
    "    print(\"âš ï¸ VeritabanÄ± yok. Sentetik veri oluÅŸturuluyor (EÄŸitim iÃ§in)...\")\n",
    "    values = np.random.lognormal(0.5, 0.8, 3000)\n",
    "    values = np.clip(values, 1.0, 100.0)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS jetx_results (id INTEGER PRIMARY KEY AUTOINCREMENT, value REAL, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)\")\n",
    "    for val in values:\n",
    "        cursor.execute(\"INSERT INTO jetx_results (value) VALUES (?)\", (val,))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"âœ… {len(values)} adet sentetik veri oluÅŸturuldu.\")\n",
    "\n",
    "# VeritabanÄ± varsa TEMÄ°ZLE ve ONAR\n",
    "else:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    # TÃ¼m veriyi Ã§ek\n",
    "    df = pd.read_sql_query(\"SELECT * FROM jetx_results\", conn)\n",
    "    \n",
    "    print(f\"ğŸ” Mevcut veri sayÄ±sÄ±: {len(df)}\")\n",
    "    print(\"ğŸ§¹ Veri temizliÄŸi yapÄ±lÄ±yor...\")\n",
    "    \n",
    "    clean_values = []\n",
    "    \n",
    "    # Temizlik DÃ¶ngÃ¼sÃ¼\n",
    "    for index, row in df.iterrows():\n",
    "        val = row['value']\n",
    "        try:\n",
    "            # String ise temizle\n",
    "            val_str = str(val).replace('\\u2028', '').replace('\\u2029', '').strip()\n",
    "            if ' ' in val_str: val_str = val_str.split()[0]\n",
    "            clean_float = float(val_str)\n",
    "            clean_values.append(clean_float)\n",
    "        except:\n",
    "            continue # Bozuk veriyi atla\n",
    "            \n",
    "    # VeritabanÄ±nÄ± temiz veriyle gÃ¼ncelleme\n",
    "    # (En gÃ¼venli yol: Tabloyu yeniden oluÅŸturmak)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS jetx_results_clean\")\n",
    "    cursor.execute(\"CREATE TABLE jetx_results_clean (id INTEGER PRIMARY KEY AUTOINCREMENT, value REAL, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)\")\n",
    "    \n",
    "    # Toplu ekleme\n",
    "    data_to_insert = [(v,) for v in clean_values]\n",
    "    cursor.executemany(\"INSERT INTO jetx_results_clean (value) VALUES (?)\", data_to_insert)\n",
    "    \n",
    "    # Eski tabloyu silip yenisinin adÄ±nÄ± deÄŸiÅŸtir\n",
    "    cursor.execute(\"DROP TABLE jetx_results\")\n",
    "    cursor.execute(\"ALTER TABLE jetx_results_clean RENAME TO jetx_results\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"âœ… VeritabanÄ± onarÄ±ldÄ± ve temizlendi. Yeni veri sayÄ±sÄ±: {len(clean_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  HÃœCRE 4: PROGRESSIVE TRANSFORMER EÄÄ°TÄ°MÄ° (MONOLITHIC)\n",
    "# En Ã¶nemli model. N-Beats, TCN ve Transformer iÃ§erir.\n",
    "\n",
    "if TRAIN_PROGRESSIVE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ§  PROGRESSIVE TRANSFORMER EÄÄ°TÄ°MÄ° BAÅLIYOR\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Ortam deÄŸiÅŸkenlerini ayarla (Path sorunu iÃ§in)\n",
    "    my_env = os.environ.copy()\n",
    "    my_env[\"PYTHONPATH\"] = os.getcwd()\n",
    "    \n",
    "    try:\n",
    "        # env=my_env ile Ã§alÄ±ÅŸtÄ±rÄ±yoruz\n",
    "        subprocess.run([sys.executable, 'notebooks/jetx_PROGRESSIVE_TRAINING.py'], check=True, env=my_env)\n",
    "        \n",
    "        # Ã‡Ä±ktÄ±larÄ± yedekle ve indir\n",
    "        backup_and_download('models/jetx_progressive_final.h5', 'Progressive Model (Final)')\n",
    "        \n",
    "        # Transformer kopyasÄ±nÄ± da al (sistem bunu arÄ±yor olabilir)\n",
    "        if os.path.exists('models/jetx_progressive_final.h5'):\n",
    "            import shutil\n",
    "            shutil.copy('models/jetx_progressive_final.h5', 'models/jetx_progressive_transformer.h5')\n",
    "            backup_and_download('models/jetx_progressive_transformer.h5', 'Transformer Model (Copy)')\n",
    "            \n",
    "        backup_and_download('models/scaler_progressive.pkl', 'Progressive Scaler')\n",
    "        backup_and_download('models/model_info.json', 'Model Info')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ HATA: {e}\")\n",
    "else:\n",
    "    print(\"â­ï¸ Progressive eÄŸitimi atlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ HÃœCRE 5: ULTRA AGGRESSIVE MODEL EÄÄ°TÄ°MÄ°\n",
    "\n",
    "if TRAIN_ULTRA:\n",
    "    print(\"\\nâš¡ ULTRA AGGRESSIVE MODEL EÄÄ°TÄ°LÄ°YOR...\")\n",
    "    \n",
    "    # Ortam deÄŸiÅŸkenlerini ayarla\n",
    "    my_env = os.environ.copy()\n",
    "    my_env[\"PYTHONPATH\"] = os.getcwd()\n",
    "\n",
    "    try:\n",
    "        subprocess.run([sys.executable, 'notebooks/jetx_model_training_ULTRA_AGGRESSIVE.py'], check=True, env=my_env)\n",
    "        \n",
    "        backup_and_download('models/jetx_ultra_model.h5', 'Ultra Model')\n",
    "        backup_and_download('models/scaler_ultra.pkl', 'Ultra Scaler')\n",
    "        backup_and_download('ultra_model_info.json', 'Ultra Info')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ HATA: {e}\")\n",
    "else:\n",
    "    print(\"â­ï¸ Ultra atlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ HÃœCRE 6: CATBOOST ENSEMBLE EÄÄ°TÄ°MÄ°\n",
    "\n",
    "if TRAIN_CATBOOST:\n",
    "    print(\"\\nğŸš€ CATBOOST ENSEMBLE EÄÄ°TÄ°LÄ°YOR...\")\n",
    "    \n",
    "    # Ortam deÄŸiÅŸkenlerini ayarla\n",
    "    my_env = os.environ.copy()\n",
    "    my_env[\"PYTHONPATH\"] = os.getcwd()\n",
    "\n",
    "    try:\n",
    "        subprocess.run([sys.executable, 'notebooks/jetx_CATBOOST_TRAINING_MULTISCALE.py'], check=True, env=my_env)\n",
    "        \n",
    "        # KlasÃ¶rÃ¼ zipleyip indir\n",
    "        backup_and_download('models/catboost_multiscale', 'CatBoost Models (Folder)')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ HATA: {e}\")\n",
    "else:\n",
    "    print(\"â­ï¸ CatBoost atlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– HÃœCRE 7: AUTOGLUON EÄÄ°TÄ°MÄ°\n",
    "\n",
    "if TRAIN_AUTOGLUON:\n",
    "    print(\"\\nğŸ¤– AUTOGLUON EÄÄ°TÄ°LÄ°YOR...\")\n",
    "    try:\n",
    "        # AutoGluon kodunu burada simÃ¼le ediyoruz (Gerekirse script'e taÅŸÄ±yÄ±n)\n",
    "        # Bu blok doÄŸrudan notebook iÃ§inde Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in env sorun yaÅŸamaz, \n",
    "        # ancak veri tabanÄ± temiz olduÄŸu iÃ§in sorun Ã§Ä±kmaz.\n",
    "        from utils.autogluon_predictor import AutoGluonPredictor\n",
    "        from utils.database import DatabaseManager\n",
    "        from category_definitions import FeatureEngineering\n",
    "        \n",
    "        db = DatabaseManager('jetx_data.db')\n",
    "        # Temiz veriyi Ã§ek (HÃ¼cre 3 zaten temizledi)\n",
    "        raw_data = db.get_all_results()\n",
    "        data = raw_data # Zaten temiz\n",
    "        \n",
    "        if len(data) > 500:\n",
    "            X, y = [], []\n",
    "            limit = min(len(data), 3000)\n",
    "            for i in range(500, limit):\n",
    "                 hist = data[i-500:i]\n",
    "                 target = data[i]\n",
    "                 feats = FeatureEngineering.extract_all_features(hist)\n",
    "                 X.append(list(feats.values()))\n",
    "                 y.append(1 if target >= 1.5 else 0)\n",
    "            \n",
    "            X_df = pd.DataFrame(X)\n",
    "            y_series = pd.Series(y)\n",
    "            \n",
    "            print(f\"ğŸš€ AutoGluon eÄŸitiliyor (SÃ¼re limiti: {TRAINING_CONFIG['autogluon_time']}s)...\")\n",
    "            predictor = AutoGluonPredictor()\n",
    "            predictor.train(X_df, y_series, time_limit=TRAINING_CONFIG['autogluon_time'])\n",
    "            \n",
    "            # Ä°ndir\n",
    "            backup_and_download('models/autogluon_model', 'AutoGluon Model (Folder)')\n",
    "            backup_and_download('models/autogluon_scaler.pkl', 'AutoGluon Scaler')\n",
    "        else:\n",
    "            print(\"âš ï¸ Yetersiz veri.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ HATA: {e}\")\n",
    "else:\n",
    "    print(\"â­ï¸ AutoGluon atlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ HÃœCRE 8: TABNET EÄÄ°TÄ°MÄ° (DÃœZELTÄ°LDÄ°)\n",
    "\n",
    "if TRAIN_TABNET:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ¯ TABNET EÄÄ°TÄ°MÄ° BAÅLIYOR (YÃ¼ksek X Specialist)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Gerekli importlar\n",
    "        from utils.tabnet_predictor import TabNetHighXPredictor\n",
    "        from category_definitions import CategoryDefinitions, FeatureEngineering\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        import joblib\n",
    "        import torch\n",
    "        from tqdm.auto import tqdm\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import sqlite3\n",
    "\n",
    "        # 1. Veri YÃ¼kleme\n",
    "        print(\"ğŸ“Š Veri hazÄ±rlanÄ±yor...\")\n",
    "        db_path = 'jetx_data.db'\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        # Veri zaten temiz\n",
    "        data = pd.read_sql_query(\"SELECT value FROM jetx_results ORDER BY id\", conn)\n",
    "        conn.close()\n",
    "        \n",
    "        all_values = data['value'].values\n",
    "        print(f\"âœ… {len(all_values)} veri yÃ¼klendi.\")\n",
    "\n",
    "        # 2. Feature Extraction\n",
    "        print(\"ğŸ”§ Feature extraction (TabNet iÃ§in)...\")\n",
    "        window_size = 50 # TabNet iÃ§in daha kÄ±sa pencere\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for i in tqdm(range(window_size, len(all_values)), desc=\"Processing\"):\n",
    "            hist = all_values[:i].tolist()\n",
    "            target = all_values[i]\n",
    "            \n",
    "            # Feature'larÄ± Ã§Ä±kar\n",
    "            feats = FeatureEngineering.extract_all_features(hist)\n",
    "            X.append(list(feats.values()))\n",
    "            \n",
    "            # Hedef: Kategori (0: DÃ¼ÅŸÃ¼k, 1: Orta, 2: YÃ¼ksek, 3: Mega)\n",
    "            if target < 1.5: c = 0\n",
    "            elif target < 10: c = 1\n",
    "            elif target < 50: c = 2\n",
    "            else: c = 3\n",
    "            y.append(c)\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # 3. Normalizasyon & Split\n",
    "        print(f\"ğŸ“Š Veri Seti: {len(X)} Ã¶rnek\")\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_scaled, y, test_size=0.2, shuffle=False\n",
    "        )\n",
    "\n",
    "        # 4. Model EÄŸitimi\n",
    "        print(f\"ğŸ”¥ TabNet eÄŸitimi baÅŸlÄ±yor (Train: {len(X_train)}, Val: {len(X_val)})...\")\n",
    "        \n",
    "        predictor = TabNetHighXPredictor(\n",
    "            model_path='models/tabnet_high_x.pkl',\n",
    "            scaler_path='models/tabnet_scaler.pkl'\n",
    "        )\n",
    "        \n",
    "        # Parametreler\n",
    "        training_params = {\n",
    "            'max_epochs': 100 if not QUICK_TEST_MODE else 5,\n",
    "            'patience': 20 if not QUICK_TEST_MODE else 2,\n",
    "            'batch_size': 1024, # T4 GPU iÃ§in yÃ¼ksek batch\n",
    "            'n_d': 16,\n",
    "            'n_a': 16,\n",
    "            'n_steps': 3\n",
    "        }\n",
    "        \n",
    "        predictor.train(\n",
    "            X_train, y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            **training_params\n",
    "        )\n",
    "        \n",
    "        # 5. Kaydetme\n",
    "        print(\"ğŸ’¾ Model ve Scaler kaydediliyor...\")\n",
    "        predictor.save_model() # Modeli kaydeder\n",
    "        joblib.dump(scaler, 'models/tabnet_scaler.pkl') # Scaler'Ä± ayrÄ±ca kaydet\n",
    "        \n",
    "        # Ä°ndirme\n",
    "        backup_and_download('models/tabnet_high_x.zip', 'TabNet Model (Zip)') # TabNet zip olarak kaydeder\n",
    "        backup_and_download('models/tabnet_scaler.pkl', 'TabNet Scaler')\n",
    "        \n",
    "        print(\"âœ… TabNet eÄŸitimi tamamlandÄ±!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ TABNET EÄÄ°TÄ°M HATASI: {e}\")\n",
    "else:\n",
    "    print(\"â­ï¸ TabNet atlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– HÃœCRE 9: RL AGENT EÄÄ°TÄ°MÄ°\n",
    "\n",
    "if TRAIN_RL_AGENT:\n",
    "    print(\"\\nğŸ¤– RL AGENT EÄÄ°TÄ°LÄ°YOR...\")\n",
    "    \n",
    "    # Ortam deÄŸiÅŸkenlerini ayarla\n",
    "    my_env = os.environ.copy()\n",
    "    my_env[\"PYTHONPATH\"] = os.getcwd()\n",
    "\n",
    "    try:\n",
    "        subprocess.run([sys.executable, 'notebooks/train_rl_agent.py'], check=True, env=my_env)\n",
    "        backup_and_download('models/rl_agent_model.h5', 'RL Model')\n",
    "        backup_and_download('models/rl_agent_info.json', 'RL Info')\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ HATA: {e}\")\n",
    "else:\n",
    "    print(\"â­ï¸ RL Agent atlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ† HÃœCRE 10: META-MODEL EÄÄ°TÄ°MÄ°\n",
    "\n",
    "if TRAIN_META_MODEL:\n",
    "    print(\"\\nğŸ† META-MODEL EÄÄ°TÄ°LÄ°YOR...\")\n",
    "    \n",
    "    # Ortam deÄŸiÅŸkenlerini ayarla\n",
    "    my_env = os.environ.copy()\n",
    "    my_env[\"PYTHONPATH\"] = os.getcwd()\n",
    "\n",
    "    try:\n",
    "        subprocess.run([sys.executable, 'notebooks/TRAIN_META_MODEL.py'], check=True, env=my_env)\n",
    "        backup_and_download('models/meta_model.json', 'Meta Model')\n",
    "        backup_and_download('models/meta_model_info.json', 'Meta Info')\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ HATA: {e}\")\n",
    "else:\n",
    "    print(\"â­ï¸ Meta-Model atlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ HÃœCRE 11: TOPLU PAKETLEME VE Ä°NDÄ°RME (Opsiyonel)\n",
    "# EÄŸer yukarÄ±da tek tek indirdiyseniz buna gerek kalmayabilir\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“¦ TOPLU PAKETLEME VE Ä°NDÄ°RME\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "zip_filename = f'JetX_FULL_ARMY_v6.0_{timestamp}.zip'\n",
    "\n",
    "# DosyalarÄ± topla\n",
    "files_to_zip = [\n",
    "    'models/jetx_progressive_transformer.h5',\n",
    "    'models/scaler_progressive.pkl',\n",
    "    'models/jetx_ultra_model.h5',\n",
    "    'models/scaler_ultra.pkl',\n",
    "    'models/meta_model.json',\n",
    "    'models/meta_model_info.json',\n",
    "    'models/rl_agent_model.h5',\n",
    "    'models/rl_agent_info.json',\n",
    "    'models/autogluon_model', # KlasÃ¶r\n",
    "    'models/autogluon_scaler.pkl',\n",
    "    'models/tabnet_high_x.pkl',\n",
    "    'models/tabnet_scaler.pkl',\n",
    "    'models/catboost_multiscale' # KlasÃ¶r\n",
    "]\n",
    "\n",
    "print(\"Dosyalar sÄ±kÄ±ÅŸtÄ±rÄ±lÄ±yor...\")\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for item in files_to_zip:\n",
    "        if os.path.exists(item):\n",
    "            if os.path.isdir(item):\n",
    "                for root, dirs, files in os.walk(item):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        arcname = os.path.relpath(file_path, os.path.join(item, '..'))\n",
    "                        zipf.write(file_path, arcname)\n",
    "            else:\n",
    "                zipf.write(item, os.path.basename(item))\n",
    "            print(f\"  Adding: {item}\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸ BulunamadÄ±: {item}\")\n",
    "\n",
    "backup_and_download(zip_filename, 'TOPLU ZIP DOSYASI')\n",
    "\n",
    "print(\"\\nğŸ‰ TÃœM Ä°ÅLEMLER TAMAMLANDI!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
