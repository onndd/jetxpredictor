# ğŸš¨ Model EÄŸitim Ä°nstabilite Sorunu - Analiz ve Ã‡Ã¶zÃ¼mler

**Tarih:** 2025-10-09  
**Sorun:** Model eÄŸitimi sÄ±rasÄ±nda threshold prediction'lar bir uÃ§tan diÄŸerine savrulÄ±yor

---

## ğŸ“Š GÃ¶zlemlenen Sorun

### Epoch Ä°lerleme Analizi

| Epoch | 1.5 AltÄ± DoÄŸruluÄŸu | 1.5 ÃœstÃ¼ DoÄŸruluÄŸu | Para KaybÄ± Riski | Durum |
|-------|--------------------|--------------------|------------------|-------|
| 1 | 0.0% | 100.0% | 100.0% | âŒ Ã‡ok kÃ¶tÃ¼ |
| **6** | **54.3%** | **42.9%** | **45.7%** | **âœ… Ä°YÄ°!** |
| 11 | 0.0% | 99.9% | 100.0% | âŒ Ã‡ok kÃ¶tÃ¼ |
| 16 | 13.2% | 90.9% | 86.8% | âŒ KÃ¶tÃ¼ |
| 21 | 100.0% | 0.0% | 0.0% | âŒ Ters uÃ§ta kÃ¶tÃ¼ |
| 26 | 0.0% | 100.0% | 100.0% | âŒ Ã‡ok kÃ¶tÃ¼ |
| 31 | 0.0% | 99.7% | 100.0% | âŒ Ã‡ok kÃ¶tÃ¼ |
| 36 | 100.0% | 0.0% | 0.0% | âŒ Ters uÃ§ta kÃ¶tÃ¼ |
| 41 | 100.0% | 0.0% | 0.0% | âŒ Ters uÃ§ta kÃ¶tÃ¼ |
| 46 | 100.0% | 0.0% | 0.0% | âŒ Ters uÃ§ta kÃ¶tÃ¼ |

### ğŸ”´ Kritik Bulgular

1. **Epoch 6'da Peak Performans** - Model dengelenmiÅŸ tahminler yapÄ±yordu
2. **Epoch 11'den sonra bozulma** - Model bir sÄ±nÄ±fa kilitleniyor
3. **Epoch 21'den sonra ters tarafa kayma** - Tamamen 1.5 altÄ± tahmin ediyor
4. **SÃ¼rekli savrulma** - SaÄŸlÄ±klÄ± bir Ã¶ÄŸrenme curve'u yok

---

## ğŸ” KÃ¶k Nedenleri

### 1. âŒ YanlÄ±ÅŸ Training Strategy (AÅAMA 1)

```python
# MEVCUT KOD (notebooks/jetx_PROGRESSIVE_TRAINING.py)
loss_weights={
    'regression': 1.0, 
    'classification': 0.0,  # âŒ KAPALI!
    'threshold': 0.0        # âŒ KAPALI!
}
```

**Sorun:** AÅAMA 1'de sadece regression eÄŸitiliyor, threshold output hiÃ§ kullanÄ±lmÄ±yor!

**SonuÃ§:** Model threshold prediction yapmayÄ± Ã¶ÄŸrenemiyor, sonraki aÅŸamalarda instabil oluyor.

---

### 2. âŒ Ã‡ok Agresif Class Weights

```python
# AÅAMA 2 & 3
TARGET_MULTIPLIER = 25.0  # AÅAMA 2
TARGET_MULTIPLIER = 30.0  # AÅAMA 3

w0 = (len(y_thr_tr) / (2 * c0)) * TARGET_MULTIPLIER
```

**Sorun:** 25x-30x class weight Ã§ok fazla! Model azÄ±nlÄ±k sÄ±nÄ±fÄ±na aÅŸÄ±rÄ± odaklanÄ±yor.

**Beklenen:** 3-7x arasÄ± dengeli bir aÄŸÄ±rlÄ±k olmalÄ±.

---

### 3. âŒ YanlÄ±ÅŸ Monitoring Metric

```python
# AÅAMA 1
callbacks.ModelCheckpoint(
    'stage1_best.h5',
    monitor='val_regression_mae',  # âŒ YANLIÅ!
    save_best_only=True,
    mode='min'
)
```

**Sorun:** Regression MAE monitÃ¶r ediliyor, ama asÄ±l hedef threshold accuracy!

**OlmasÄ± gereken:** `val_threshold_accuracy` monitÃ¶r edilmeli.

---

### 4. âŒ Ã‡ok DÃ¼ÅŸÃ¼k Batch Size

```python
# AÅAMA 1: batch_size=16
# AÅAMA 2: batch_size=8
# AÅAMA 3: batch_size=4
```

**Sorun:** Batch size 4-8 Ã§ok dÃ¼ÅŸÃ¼k! Gradient tahminleri gÃ¼rÃ¼ltÃ¼lÃ¼ oluyor.

**Beklenen:** En az 32-64 batch size.

---

### 5. âŒ AÅŸÄ±rÄ± Patience

```python
callbacks.EarlyStopping(
    patience=50   # AÅAMA 1
    patience=40   # AÅAMA 2
    patience=50   # AÅAMA 3
)
```

**Sorun:** Model epoch 6'da peak yapmÄ±ÅŸ ama 50 epoch daha devam ediyor ve bozuluyor!

**Beklenen:** Patience 10-20 arasÄ± olmalÄ±.

---

### 6. âŒ Learning Rate Schedule Ã‡ok HÄ±zlÄ±

```python
def lr_schedule(epoch, lr):
    if epoch < 50:    # Ã–ne Ã§ekildi: 200 â†’ 50
        return initial_lr
    elif epoch < 150: # Ã–ne Ã§ekildi: 500 â†’ 150
        return initial_lr * 0.5
```

**Sorun:** LR Ã§ok erken dÃ¼ÅŸÃ¼yor, model stabilize olamÄ±yor.

---

## âœ… Ã–NERÄ°LEN Ã‡Ã–ZÃœMLER

### Ã‡Ã¶zÃ¼m 1: AÅAMA 1'i DÃ¼zelt - Threshold'u Dahil Et

```python
# YENÄ° AÅAMA 1: Regression + Threshold Birlikte
model.compile(
    optimizer=Adam(0.0001),  # Daha dÃ¼ÅŸÃ¼k LR
    loss={
        'regression': threshold_killer_loss,
        'classification': 'categorical_crossentropy',
        'threshold': 'binary_crossentropy'  # âœ… AKTÄ°F!
    },
    loss_weights={
        'regression': 0.60,      # âœ… Ana odak regression
        'classification': 0.10,  # âœ… Hafif classification
        'threshold': 0.30        # âœ… Threshold Ã¶ÄŸrenmeye baÅŸla!
    },
    metrics={
        'regression': ['mae'], 
        'classification': ['accuracy'], 
        'threshold': ['accuracy', 'binary_crossentropy']
    }
)
```

**Neden:** Model baÅŸtan threshold prediction Ã¶ÄŸrenmeye baÅŸlamalÄ±!

---

### Ã‡Ã¶zÃ¼m 2: Class Weight'leri YumuÅŸat

```python
# DENGELI CLASS WEIGHTS
# AÅAMA 1
TARGET_MULTIPLIER = 3.0  # âœ… YumuÅŸak baÅŸlangÄ±Ã§

# AÅAMA 2
TARGET_MULTIPLIER = 5.0  # âœ… Orta seviye

# AÅAMA 3
TARGET_MULTIPLIER = 7.0  # âœ… Final odaklanma
```

**Neden:** AÅŸÄ±rÄ± aÄŸÄ±rlÄ±k model instabilitesine neden oluyor.

---

### Ã‡Ã¶zÃ¼m 3: DoÄŸru Metric'i MonitÃ¶r Et

```python
# AÅAMA 1
callbacks.ModelCheckpoint(
    'stage1_best.h5',
    monitor='val_threshold_accuracy',  # âœ… DOÄRU!
    save_best_only=True,
    mode='max'  # âœ… Max accuracy
)

callbacks.EarlyStopping(
    monitor='val_threshold_accuracy',  # âœ… DOÄRU!
    patience=15,  # âœ… Daha kÄ±sa patience
    mode='max',
    restore_best_weights=True
)
```

**Neden:** AsÄ±l hedefimiz threshold accuracy!

---

### Ã‡Ã¶zÃ¼m 4: Batch Size'Ä± ArtÄ±r

```python
# AÅAMA 1: batch_size=64   # âœ… Daha stabil
# AÅAMA 2: batch_size=32   # âœ… Orta seviye
# AÅAMA 3: batch_size=16   # âœ… Fine-tuning iÃ§in
```

**Neden:** Daha bÃ¼yÃ¼k batch size = daha stabil gradient.

---

### Ã‡Ã¶zÃ¼m 5: Learning Rate Schedule DeÄŸiÅŸtir

```python
def lr_schedule(epoch, lr):
    # âœ… Daha yavaÅŸ dÃ¼ÅŸÃ¼ÅŸ
    if epoch < 100:
        return initial_lr
    elif epoch < 300:
        return initial_lr * 0.7  # Daha yumuÅŸak
    elif epoch < 500:
        return initial_lr * 0.5
    else:
        return initial_lr * 0.3
```

**Neden:** Model stabilize olmak iÃ§in daha fazla zamana ihtiyaÃ§ duyuyor.

---

### Ã‡Ã¶zÃ¼m 6: Focal Loss Parametrelerini YumuÅŸat

```python
# MEVCUT: gamma=4.0, alpha=0.85
# Ã–NERÄ°LEN: gamma=2.0, alpha=0.75

def ultra_focal_loss(gamma=2.0, alpha=0.75):  # âœ… Daha yumuÅŸak
    def loss(y_true, y_pred):
        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())
        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        focal_weight = alpha * K.pow(1 - pt, gamma)
        return -K.mean(focal_weight * K.log(pt))
    return loss
```

**Neden:** Gamma=4 Ã§ok agresif, model instabil oluyor.

---

### Ã‡Ã¶zÃ¼m 7: Epoch 6'dan Devam Et (HÄ±zlÄ± Ã‡Ã¶zÃ¼m)

```python
# EÄŸer epoch 6'da iyi performans varsa:
# 1. stage1_best.h5 dosyasÄ±nÄ± epoch 6'dan kaydet
# 2. AÅAMA 2'ye geÃ§
# 3. Daha dÃ¼ÅŸÃ¼k class weight ile devam et (5x)
```

---

## ğŸ¯ YENÄ° PROGRESSIVE TRAINING STRATEJÄ°SÄ°

### AÅAMA 1: Balanced Foundation (150 epoch)
- **Hedef:** Regression + Threshold birlikte Ã¶ÄŸren
- **Loss Weights:** `regression: 0.60, classification: 0.10, threshold: 0.30`
- **Batch Size:** 64
- **LR:** 0.0001
- **Class Weight:** 3x (yumuÅŸak)
- **Patience:** 15
- **Monitor:** `val_threshold_accuracy`

### AÅAMA 2: Threshold Focus (100 epoch)
- **Hedef:** Threshold accuracy'yi artÄ±r
- **Loss Weights:** `regression: 0.40, classification: 0.10, threshold: 0.50`
- **Batch Size:** 32
- **LR:** 0.00005
- **Class Weight:** 5x (orta)
- **Patience:** 12
- **Monitor:** `val_threshold_accuracy`

### AÅAMA 3: Final Polish (100 epoch)
- **Hedef:** TÃ¼m output'larÄ± optimize et
- **Loss Weights:** `regression: 0.30, classification: 0.15, threshold: 0.55`
- **Batch Size:** 16
- **LR:** 0.00003
- **Class Weight:** 7x (final)
- **Patience:** 10
- **Monitor:** `val_threshold_accuracy`

---

## ğŸ“ UYGULAMA PLANI

### 1. Yeni Training Script OluÅŸtur

**Dosya:** `notebooks/jetx_STABLE_PROGRESSIVE_TRAINING.py`

**DeÄŸiÅŸiklikler:**
- [ ] AÅAMA 1'de threshold loss'u aktif et
- [ ] Class weight'leri 3x-5x-7x yap
- [ ] Batch size'larÄ± 64-32-16 yap
- [ ] Patience'leri 15-12-10 yap
- [ ] Monitor metric'i `val_threshold_accuracy` yap
- [ ] LR schedule'u yumuÅŸat
- [ ] Focal loss gamma'yÄ± 2.0'a dÃ¼ÅŸÃ¼r

### 2. Test Et

```bash
# Google Colab'da Ã§alÄ±ÅŸtÄ±r
python notebooks/jetx_STABLE_PROGRESSIVE_TRAINING.py
```

### 3. Beklenen SonuÃ§lar

**AÅAMA 1 SonrasÄ± (Epoch ~50-100):**
- 1.5 altÄ± doÄŸruluÄŸu: %50-60
- 1.5 Ã¼stÃ¼ doÄŸruluÄŸu: %60-70
- **STABÄ°L** - Bir uÃ§tan diÄŸerine savr kullanÄ±mÄ± yok

**AÅAMA 2 SonrasÄ±:**
- 1.5 altÄ± doÄŸruluÄŸu: %60-70
- 1.5 Ã¼stÃ¼ doÄŸruluÄŸu: %70-80

**AÅAMA 3 SonrasÄ±:**
- 1.5 altÄ± doÄŸruluÄŸu: %70-80+ (HEDEF!)
- 1.5 Ã¼stÃ¼ doÄŸruluÄŸu: %75-85+

---

## ğŸ”§ HIZLI FÄ°X (Åu Anki EÄŸitimi Kurtar)

EÄŸer eÄŸitim hala devam ediyorsa:

1. **HEMEN DURDUR** - Epoch 6'dan sonra modeli kaybet
2. **Epoch 6 Model'ini YÃ¼kle:**
   ```python
   model.load_weights('stage1_best.h5')
   ```
3. **AÅAMA 2'ye yumuÅŸak geÃ§:**
   ```python
   # Class weight 5x (30x deÄŸil!)
   TARGET_MULTIPLIER = 5.0
   
   # Batch size 32 (8 deÄŸil!)
   batch_size = 32
   
   # Patience 15 (40 deÄŸil!)
   patience = 15
   ```

---

## ğŸ“Š Ä°ZLEME METRÄ°KLERÄ°

Her epoch'ta izle:
1. **1.5 AltÄ± DoÄŸruluÄŸu** - %50+ olmalÄ±, sÃ¼rekli
2. **1.5 ÃœstÃ¼ DoÄŸruluÄŸu** - %60+ olmalÄ±, sÃ¼rekli
3. **Para KaybÄ± Riski** - %50 altÄ±nda olmalÄ±, sÃ¼rekli
4. **Stabilite** - Bir uÃ§tan diÄŸerine savrulmama

**UyarÄ± Ä°ÅŸaretleri:**
- Bir sÄ±nÄ±f doÄŸruluÄŸu %100'e Ã§Ä±kÄ±yorsa â†’ DURDUR!
- Bir sÄ±nÄ±f doÄŸruluÄŸu %0'a dÃ¼ÅŸÃ¼yorsa â†’ DURDUR!
- Para kaybÄ± riski %80+ â†’ DURDUR!

---

## ğŸ“ DERS Ã‡IKARILANLAR

1. **Progressive training** = AÅŸamalÄ± aÄŸÄ±rlÄ±k artÄ±ÅŸÄ±, hepsi baÅŸtan kapalÄ± DEÄÄ°L!
2. **Class weight** Ã§ok yÃ¼ksek olursa model instabil olur
3. **Batch size** Ã§ok kÃ¼Ã§Ã¼k olursa gradient gÃ¼rÃ¼ltÃ¼lÃ¼ olur
4. **Patience** Ã§ok yÃ¼ksek olursa overfitting olur
5. **DoÄŸru metric monitÃ¶r etmek kritik!**

---

## ğŸ“ REFERANSLAR

- Mevcut kod: [`notebooks/jetx_PROGRESSIVE_TRAINING.py`](notebooks/jetx_PROGRESSIVE_TRAINING.py)
- Loss fonksiyonlarÄ±: [`utils/custom_losses.py`](utils/custom_losses.py)
- Kategori tanÄ±mlarÄ±: [`category_definitions.py`](category_definitions.py)

---

**SonuÃ§:** Model eÄŸitimi ÅŸu anda saÄŸlÄ±ksÄ±z. YukarÄ±daki dÃ¼zeltmeler uygulanmalÄ±!