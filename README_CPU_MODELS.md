# ğŸš€ JetX CPU Lightweight Models

CPU ile eÄŸitilebilen hafif modeller iÃ§in Ã¶zelleÅŸtirilmiÅŸ Streamlit uygulamasÄ±. TabNet, AutoGluon, LightGBM, CatBoost gibi hafif modelleri destekler.

## ğŸ“‹ Ä°Ã§indekiler

- [Ã–zellikler](#-Ã¶zellikler)
- [Kurulum](#-kurulum)
- [KullanÄ±m](#-kullanÄ±m)
- [Model Ã–zellikleri](#-model-Ã¶zellikleri)
- [EÄŸitim Rehberi](#-eÄŸitim-rehberi)
- [Hyperparameter Tuning](#-hyperparameter-tuning)
- [Ensemble OluÅŸturma](#-ensemble-oluÅŸturma)
- [API ReferansÄ±](#-api-referansÄ±)
- [Best Practices](#-best-practices)
- [Sorun Giderme](#-sorun-giderme)

## ğŸ¯ Ã–zellikler

### ğŸš€ Desteklenen Modeller
- **LightGBM**: CPU optimized gradient boosting
- **CatBoost**: Categorical boosting
- **TabNet**: Attention-based deep learning
- **AutoGluon**: Automated ML

### ğŸ”§ GeliÅŸmiÅŸ Ã–zellikler
- **Model EÄŸitimi**: GeliÅŸmiÅŸ eÄŸitim arayÃ¼zÃ¼
- **Hyperparameter Tuning**: Optuna entegrasyonu
- **Model KarÅŸÄ±laÅŸtÄ±rma**: Side-by-side metrikler
- **Ensemble Builder**: Voting ve stacking
- **Prediction & Backtesting**: Real-time tahmin ve historical backtesting
- **Virtual Bankroll Simulation**: ROI hesaplama

### ğŸ’» CPU Optimized
- TÃ¼m modeller CPU Ã¼zerinde optimize edilmiÅŸ
- Memory efficient training
- Fast inference
- Cross-platform compatibility

## ğŸ› ï¸ Kurulum

### Gereksinimler
- Python 3.8+
- 8GB+ RAM (16GB Ã¶nerilen)
- 2GB+ disk alanÄ±

### AdÄ±m 1: Repository Clone
```bash
git clone https://github.com/yourusername/jetxpredictor.git
cd jetxpredictor
```

### AdÄ±m 2: Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# veya
venv\Scripts\activate  # Windows
```

### AdÄ±m 3: Dependencies
```bash
pip install -r requirements.txt
```

### AdÄ±m 4: Veri HazÄ±rlÄ±ÄŸÄ±
```bash
# jetx_data.db dosyasÄ±nÄ±n mevcut olduÄŸundan emin olun
# EÄŸer yoksa, ana uygulamadan veri yÃ¼kleyin
```

### AdÄ±m 5: UygulamayÄ± BaÅŸlat
```bash
streamlit run app_cpu_models.py --server.port 8502
```

## ğŸ® KullanÄ±m

### Ana Dashboard
Ana sayfa model durumunu, sistem bilgilerini ve hÄ±zlÄ± eylemleri gÃ¶sterir.

### Model EÄŸitimi
1. **Model Tipi SeÃ§in**: LightGBM, CatBoost, TabNet, AutoGluon
2. **Model Modu**: Classification, Regression, Multiclass
3. **Hyperparameters**: Model-specific parametreler
4. **Class Weights**: Imbalanced data iÃ§in aÄŸÄ±rlÄ±klar
5. **EÄŸitimi BaÅŸlat**: Real-time progress tracking

### Hyperparameter Tuning
1. **Model SeÃ§in**: Optimize edilecek model
2. **Search Space**: Parametre aralÄ±klarÄ± tanÄ±mlayÄ±n
3. **Optimization**: Trial sayÄ±sÄ± ve timeout
4. **SonuÃ§lar**: Best parameters ve visualization

### Model KarÅŸÄ±laÅŸtÄ±rma
1. **Modelleri SeÃ§in**: KarÅŸÄ±laÅŸtÄ±rÄ±lacak modeller
2. **Test Verisi**: Test set boyutu
3. **KarÅŸÄ±laÅŸtÄ±r**: Side-by-side metrikler
4. **GÃ¶rselleÅŸtirme**: Charts ve tablolar

### Ensemble Builder
1. **Modelleri SeÃ§in**: Ensemble'e dahil edilecek modeller
2. **Strateji**: Voting (hard/soft) veya Stacking
3. **AÄŸÄ±rlÄ±klar**: Weighted voting iÃ§in
4. **Test**: Ensemble performance

### Prediction & Backtesting
1. **Model SeÃ§in**: Tahmin yapacak model
2. **Mod**: Real-time veya Historical backtesting
3. **Tahmin**: Live prediction veya backtest
4. **Analiz**: Performance metrics ve visualization

## ğŸ¤– Model Ã–zellikleri

### LightGBM
- **Tip**: Gradient Boosting
- **Modlar**: Classification, Regression, Multiclass
- **Avantajlar**: HÄ±zlÄ± eÄŸitim, memory efficient
- **Hyperparameters**: num_leaves, max_depth, learning_rate, feature_fraction

### CatBoost
- **Tip**: Categorical Boosting
- **Modlar**: Classification, Regression
- **Avantajlar**: Categorical features, overfitting resistance
- **Hyperparameters**: iterations, depth, learning_rate, l2_leaf_reg

### TabNet
- **Tip**: Attention-based Deep Learning
- **Modlar**: Classification, Multiclass
- **Avantajlar**: Interpretable, attention mechanism
- **Hyperparameters**: n_d, n_a, n_steps, gamma

### AutoGluon
- **Tip**: Automated ML
- **Modlar**: Classification, Regression
- **Avantajlar**: Auto-tuning, ensemble, no hyperparameter tuning needed
- **Hyperparameters**: time_limit, presets, eval_metric

## ğŸ“š EÄŸitim Rehberi

### Temel EÄŸitim
1. **Veri HazÄ±rlÄ±ÄŸÄ±**: Database'den veri yÃ¼kleme
2. **Feature Engineering**: Otomatik feature extraction
3. **Model SeÃ§imi**: Use case'e gÃ¶re model seÃ§imi
4. **EÄŸitim**: Parametrelerle model eÄŸitimi
5. **DeÄŸerlendirme**: Metrics ve visualization

### Ä°leri Seviye EÄŸitim
1. **Hyperparameter Tuning**: Optuna ile optimization
2. **Cross-Validation**: Robust evaluation
3. **Ensemble**: Multiple models combination
4. **Backtesting**: Historical performance analysis

### Best Practices
- **Veri Split**: 70% train, 15% validation, 15% test
- **Class Weights**: Imbalanced data iÃ§in
- **Early Stopping**: Overfitting prevention
- **Cross-Validation**: Model stability
- **Feature Importance**: Model interpretability

## ğŸ”§ Hyperparameter Tuning

### Optuna Integration
```python
# Search space tanÄ±mlama
search_space = {
    'num_leaves': (10, 100),
    'max_depth': (3, 15),
    'learning_rate': (0.01, 0.3)
}

# Optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)
```

### Visualization
- **Optimization History**: Trial progress
- **Parameter Importances**: Feature importance
- **Parallel Coordinate**: Parameter relationships

### Best Practices
- **Trial SayÄ±sÄ±**: 50-100 trials
- **Timeout**: 1-2 hours
- **CV Folds**: 5-fold cross-validation
- **Pruning**: Early stopping for bad trials

## ğŸ¤ Ensemble OluÅŸturma

### Voting Strategy
- **Hard Voting**: Majority vote
- **Soft Voting**: Average probabilities
- **Weighted Voting**: Custom weights

### Stacking Strategy
- **Meta Model**: Secondary model
- **Cross-Validation**: Out-of-fold predictions
- **Probability/Class**: Input type

### Best Practices
- **Model Diversity**: Different algorithms
- **Performance**: Individual model quality
- **Correlation**: Low correlation between models
- **Validation**: Proper validation strategy

## ğŸ“– API ReferansÄ±

### LightweightModelManager
```python
# Model oluÅŸturma
model, model_id = manager.create_model(
    model_type='lightgbm',
    mode='classification',
    config=config
)

# Model eÄŸitimi
metrics = manager.train_model(
    model_id=model_id,
    X=X_train,
    y=y_train
)

# Model karÅŸÄ±laÅŸtÄ±rma
results = manager.compare_models(
    model_ids=['model1', 'model2'],
    X_test=X_test,
    y_test=y_test
)
```

### CPUTrainingEngine
```python
# Tek model eÄŸitimi
model_id, metrics = engine.train_single_model(
    model_type='lightgbm',
    X=X_train,
    y=y_train,
    mode='classification'
)

# Hyperparameter search
results = engine.hyperparameter_search(
    model_type='lightgbm',
    X=X_train,
    y=y_train,
    n_trials=50
)
```

### LightGBMPredictor
```python
# Model eÄŸitimi
predictor = LightGBMPredictor(mode='classification')
metrics = predictor.train(X_train, y_train)

# Tahmin
predictions = predictor.predict(X_test)
probabilities = predictor.predict_proba(X_test)

# Feature importance
importance = predictor.get_feature_importance()
```

## ğŸ† Best Practices

### Model SeÃ§imi
- **LightGBM**: HÄ±zlÄ± eÄŸitim, memory efficient
- **CatBoost**: Categorical features, robust
- **TabNet**: Interpretable, attention
- **AutoGluon**: No tuning, automated

### Hyperparameter Tuning
- **Search Space**: Reasonable ranges
- **Trial Count**: 50-100 trials
- **Validation**: Cross-validation
- **Early Stopping**: Prevent overfitting

### Ensemble
- **Diversity**: Different algorithms
- **Quality**: Good individual models
- **Validation**: Proper evaluation
- **Weighting**: Performance-based weights

### Performance
- **CPU Usage**: Monitor resource usage
- **Memory**: Efficient data handling
- **Speed**: Fast inference
- **Accuracy**: Balanced performance

## ğŸ” Sorun Giderme

### YaygÄ±n Sorunlar

#### Model EÄŸitimi BaÅŸarÄ±sÄ±z
- **Veri KontrolÃ¼**: Yeterli veri var mÄ±?
- **Feature KontrolÃ¼**: Feature extraction Ã§alÄ±ÅŸÄ±yor mu?
- **Memory**: Yeterli RAM var mÄ±?
- **Dependencies**: TÃ¼m paketler yÃ¼klÃ¼ mÃ¼?

#### Hyperparameter Tuning YavaÅŸ
- **Trial SayÄ±sÄ±**: AzaltÄ±n
- **Timeout**: ArtÄ±rÄ±n
- **CV Folds**: AzaltÄ±n
- **Search Space**: KÃ¼Ã§Ã¼ltÃ¼n

#### Ensemble Performance DÃ¼ÅŸÃ¼k
- **Model Quality**: Individual model performance
- **Diversity**: Model Ã§eÅŸitliliÄŸi
- **Validation**: Proper validation
- **Weighting**: Optimal weights

#### Memory Issues
- **Batch Size**: AzaltÄ±n
- **Data Size**: KÃ¼Ã§Ã¼ltÃ¼n
- **Model Size**: Simpler models
- **Cleanup**: Memory cleanup

### Log DosyalarÄ±
- **App Log**: `data/cpu_models.log`
- **Training Log**: Console output
- **Error Log**: Exception details

### Debug Mode
```bash
# Debug mode ile baÅŸlat
streamlit run app_cpu_models.py --logger.level debug
```

## ğŸ“ Destek

### DokÃ¼mantasyon
- **GitHub**: [Repository](https://github.com/yourusername/jetxpredictor)
- **Issues**: [GitHub Issues](https://github.com/yourusername/jetxpredictor/issues)
- **Wiki**: [Project Wiki](https://github.com/yourusername/jetxpredictor/wiki)

### Topluluk
- **Discord**: [Community Server](https://discord.gg/yourinvite)
- **Reddit**: [r/jetxpredictor](https://reddit.com/r/jetxpredictor)
- **Stack Overflow**: `jetx-predictor` tag

### GeliÅŸtirici
- **Email**: developer@jetxpredictor.com
- **Twitter**: [@jetxpredictor](https://twitter.com/jetxpredictor)
- **LinkedIn**: [JetX Predictor](https://linkedin.com/company/jetxpredictor)

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in [LICENSE](LICENSE) dosyasÄ±na bakÄ±n.

## ğŸ™ TeÅŸekkÃ¼rler

- **Streamlit**: Web framework
- **LightGBM**: Gradient boosting
- **CatBoost**: Categorical boosting
- **TabNet**: Attention-based models
- **AutoGluon**: Automated ML
- **Optuna**: Hyperparameter optimization
- **Plotly**: Visualization

---

**ğŸš€ JetX CPU Lightweight Models v1.0** - CPU Optimized Training & Prediction
