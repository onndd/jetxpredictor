# ğŸ”§ TIME-SERIES SPLIT Ä°MPLEMENTASYON PLANI

**Tarih**: 2025-10-12  
**AmaÃ§**: Modelin zaman serisi Ã¼zerindeki sÄ±ralÄ± paternleri ezberlemesini saÄŸlamak

---

## ğŸ“‹ TEMEL PRENSÄ°PLER

### ğŸ¯ Hedef
Modelin **kronolojik veri akÄ±ÅŸÄ±na** ne kadar iyi adapte olduÄŸunu test etmek ve **ezberleme (overfitting) eÄŸilimlerini** netleÅŸtirmek.

### âš ï¸ KRÄ°TÄ°K KISITLAR

1. **RASTGELELIK YASAK**: Veri setinin hiÃ§bir aÅŸamasÄ±nda rastgele karÄ±ÅŸtÄ±rma (shuffle) veya rastgele Ã¶rnekleme (random sampling) yapÄ±lmayacak.

2. **KRONOLOJÄ°K BÃ–LME**: 
   - Test Seti: Son 1,000 kayÄ±t (kesinlikle)
   - EÄŸitim Seti: Ä°lk 5,000 kayÄ±t (geri kalan)

3. **MODEL.FIT() KISITLARI**:
   - `shuffle=False` (validation split'te de)
   - Batch akÄ±ÅŸÄ± kronolojik sÄ±rayla (en eskiden en yeniye)

4. **VALÄ°DASYON SETÄ°**: EÄŸitim setinin son %20'si (kronolojik olarak)

---

## ğŸ”„ MEVCUT DURUM vs HEDEF DURUM

### âŒ Mevcut Durum (YanlÄ±ÅŸ)
```python
# Progressive Training
tr_idx, te_idx = train_test_split(idx, test_size=0.2, 
                                   shuffle=True,      # âŒ YANLIÅ!
                                   stratify=y_cls,    # âŒ Zaman serisini bozuyor!
                                   random_state=42)

model.fit(..., validation_split=0.2, ...)  # VarsayÄ±lan shuffle=True
```

**Sorunlar**:
- Veriler karÄ±ÅŸtÄ±rÄ±lÄ±yor (shuffle=True)
- Gelecekten geÃ§miÅŸe "sÄ±zÄ±ntÄ±" oluÅŸuyor (data leakage)
- Model ezberliyor, gerÃ§ek dÃ¼nya performansÄ± kÃ¶tÃ¼
- Stratified sampling zaman serisi yapÄ±sÄ±nÄ± bozuyor

### âœ… Hedef Durum (DoÄŸru)
```python
# Kronolojik split (shuffle yok!)
test_size = 1000
train_end = len(X) - test_size

X_train = X[:train_end]
X_test = X[train_end:]
y_train = y[:train_end]
y_test = y[train_end:]

# Validation split de kronolojik
val_size = int(len(X_train) * 0.2)
val_start = len(X_train) - val_size

X_tr = X_train[:val_start]
X_val = X_train[val_start:]
y_tr = y_train[:val_start]
y_val = y_train[val_start:]

model.fit(X_tr, y_tr, 
          validation_data=(X_val, y_val),  # Manuel validation
          shuffle=False,                     # âœ… Kronolojik!
          ...)
```

**Avantajlar**:
- Zaman serisi yapÄ±sÄ± korunuyor
- GerÃ§ek dÃ¼nya senaryosunu simÃ¼le ediyor
- Model ezberleme eÄŸilimi net gÃ¶rÃ¼nÃ¼yor
- Data leakage yok

---

## ğŸ“Š VERÄ° BÃ–LME STRATEJÄ°SÄ°

### Veri Seti YapÄ±sÄ±
```
Toplam: 5,090 Ã¶rnek (window_size=1000 sonrasÄ±)

â”œâ”€ EÄÄ°TÄ°M SETÄ°: 4,090 Ã¶rnek (ilk %80.3)
â”‚  â”œâ”€ Train: 3,272 Ã¶rnek (eÄŸitim setinin %80'i)
â”‚  â””â”€ Validation: 818 Ã¶rnek (eÄŸitim setinin %20'si, kronolojik)
â”‚
â””â”€ TEST SETÄ°: 1,000 Ã¶rnek (son %19.7, kesinlikle test!)
```

### Zaman Serisi GÃ¶rselleÅŸtirme
```
[Ã–rnek 0] â”€â”€â”€â”€â”€â–º [Ã–rnek 3,271] | [Ã–rnek 3,272] â”€â”€â”€â”€â”€â–º [Ã–rnek 4,089] | [Ã–rnek 4,090] â”€â”€â”€â”€â”€â–º [Ã–rnek 5,089]
     TRAIN (3,272)                    VALIDATION (818)                      TEST (1,000)
     â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                                    Zaman AkÄ±ÅŸÄ± (Kronolojik)
```

### Kod Ä°mplementasyonu
```python
# 1. Feature extraction (kronolojik sÄ±rada)
window_size = 1000
X, y_reg, y_cls = [], [], []

for i in range(window_size, len(all_values)-1):  # Kronolojik sÄ±ra
    hist = all_values[:i].tolist()
    target = all_values[i]
    
    # Features
    feats = FeatureEngineering.extract_all_features(hist)
    X.append(list(feats.values()))
    y_reg.append(target)
    y_cls.append(1 if target >= 1.5 else 0)

X = np.array(X)
y_reg = np.array(y_reg)
y_cls = np.array(y_cls)

# 2. Normalizasyon (sadece train set Ã¼zerinde fit!)
test_size = 1000
train_end = len(X) - test_size

scaler = StandardScaler()
X_train = scaler.fit_transform(X[:train_end])  # Sadece train'de fit
X_test = scaler.transform(X[train_end:])        # Test'te sadece transform

y_train = y_reg[:train_end]
y_test = y_reg[train_end:]

# 3. Validation split (kronolojik)
val_size = int(len(X_train) * 0.2)
val_start = len(X_train) - val_size

X_tr = X_train[:val_start]
X_val = X_train[val_start:]
y_tr = y_train[:val_start]
y_val = y_train[val_start:]

print(f"âœ… Train: {len(X_tr):,}, Validation: {len(X_val):,}, Test: {len(X_test):,}")
print(f"ğŸ“Š Toplam: {len(X_tr) + len(X_val) + len(X_test):,}")
```

---

## ğŸ§  NEURAL NETWORK DEÄÄ°ÅÄ°KLÄ°KLERÄ°

### 1. Data Split DeÄŸiÅŸiklikleri

**Dosya**: `notebooks/jetx_PROGRESSIVE_TRAINING.py`

**DeÄŸiÅŸtirilecek SatÄ±rlar**: 323-332

#### Eski Kod (YanlÄ±ÅŸ):
```python
# Train/Test split - STRATIFIED SAMPLING EKLENDI
idx = np.arange(len(X_f))
tr_idx, te_idx = train_test_split(idx, test_size=0.2, shuffle=True, 
                                   stratify=y_cls, random_state=42)

X_f_tr, X_50_tr, X_200_tr, X_500_tr, X_1000_tr = X_f[tr_idx], X_50[tr_idx], ...
y_reg_tr, y_cls_tr, y_thr_tr = y_reg[tr_idx], y_cls[tr_idx], y_thr[tr_idx]

X_f_te, X_50_te, X_200_te, X_500_te, X_1000_te = X_f[te_idx], X_50[te_idx], ...
y_reg_te, y_cls_te, y_thr_te = y_reg[te_idx], y_cls[te_idx], y_thr[te_idx]
```

#### Yeni Kod (DoÄŸru):
```python
# =============================================================================
# TIME-SERIES SPLIT (KRONOLOJIK) - SHUFFLE YOK!
# =============================================================================
print("\nğŸ“Š TIME-SERIES SPLIT (Kronolojik BÃ¶lme)...")
print("âš ï¸  UYARI: Shuffle devre dÄ±ÅŸÄ± - Zaman serisi yapÄ±sÄ± korunuyor!")

# Test seti: Son 1000 kayÄ±t
test_size = 1000
train_end = len(X_f) - test_size

# Train/Test split (kronolojik)
X_f_train = X_f[:train_end]
X_50_train = X_50[:train_end]
X_200_train = X_200[:train_end]
X_500_train = X_500[:train_end]
X_1000_train = X_1000[:train_end]
y_reg_train = y_reg[:train_end]
y_cls_train = y_cls[:train_end]
y_thr_train = y_thr[:train_end]

X_f_te = X_f[train_end:]
X_50_te = X_50[train_end:]
X_200_te = X_200[train_end:]
X_500_te = X_500[train_end:]
X_1000_te = X_1000[train_end:]
y_reg_te = y_reg[train_end:]
y_cls_te = y_cls[train_end:]
y_thr_te = y_thr[train_end:]

# Validation split (eÄŸitim setinin son %20'si, kronolojik)
val_size = int(len(X_f_train) * 0.2)
val_start = len(X_f_train) - val_size

X_f_tr = X_f_train[:val_start]
X_50_tr = X_50_train[:val_start]
X_200_tr = X_200_train[:val_start]
X_500_tr = X_500_train[:val_start]
X_1000_tr = X_1000_train[:val_start]
y_reg_tr = y_reg_train[:val_start]
y_cls_tr = y_cls_train[:val_start]
y_thr_tr = y_thr_train[:val_start]

X_f_val = X_f_train[val_start:]
X_50_val = X_50_train[val_start:]
X_200_val = X_200_train[val_start:]
X_500_val = X_500_train[val_start:]
X_1000_val = X_1000_train[val_start:]
y_reg_val = y_reg_train[val_start:]
y_cls_val = y_cls_train[val_start:]
y_thr_val = y_thr_train[val_start:]

print(f"âœ… Train: {len(X_f_tr):,}")
print(f"âœ… Validation: {len(X_f_val):,} (eÄŸitim setinin son %20'si)")
print(f"âœ… Test: {len(X_f_te):,} (tÃ¼m verinin son {test_size} kaydÄ±)")
print(f"ğŸ“Š Toplam: {len(X_f_tr) + len(X_f_val) + len(X_f_te):,}")
```

### 2. Model.fit() DeÄŸiÅŸiklikleri

**Her 3 aÅŸamada da** (AÅAMA 1, 2, 3):

#### Eski Kod:
```python
hist1 = model.fit(
    [X_f_tr, X_50_tr, X_200_tr, X_500_tr, X_1000_tr],
    {'regression': y_reg_tr, 'classification': y_cls_tr, 'threshold': y_thr_tr},
    epochs=100,
    batch_size=64,
    validation_split=0.2,  # âŒ Otomatik split (shuffle yapÄ±yor!)
    callbacks=cb1,
    verbose=1
)
```

#### Yeni Kod:
```python
hist1 = model.fit(
    [X_f_tr, X_50_tr, X_200_tr, X_500_tr, X_1000_tr],
    {'regression': y_reg_tr, 'classification': y_cls_tr, 'threshold': y_thr_tr},
    epochs=100,
    batch_size=64,
    validation_data=(  # âœ… Manuel validation (kronolojik!)
        [X_f_val, X_50_val, X_200_val, X_500_val, X_1000_val],
        {'regression': y_reg_val, 'classification': y_cls_val, 'threshold': y_thr_val}
    ),
    shuffle=False,  # âœ… KRITIK: Shuffle devre dÄ±ÅŸÄ±!
    callbacks=cb1,
    verbose=1
)
```

### 3. Class Weight ArtÄ±rÄ±mÄ±

**TÃ¼m aÅŸamalarda class weight'i artÄ±r**:

```python
# AÅAMA 1
w0_stage1 = 15.0  # 1.2 â†’ 15.0 (12.5x artÄ±ÅŸ!)
w1_stage1 = 1.0

# AÅAMA 2
w0 = 20.0  # 1.5 â†’ 20.0 (13.3x artÄ±ÅŸ!)
w1 = 1.0

# AÅAMA 3
w0_final = 25.0  # 2.0 â†’ 25.0 (12.5x artÄ±ÅŸ!)
w1_final = 1.0
```

### 4. Adaptive Weight Scheduler GÃ¼ncelleme

`max_weight` parametresini artÄ±r:

```python
adaptive_scheduler_2 = AdaptiveWeightScheduler(
    initial_weight=20.0,   # 1.5 â†’ 20.0
    min_weight=10.0,       # 1.0 â†’ 10.0
    max_weight=50.0,       # 4.0 â†’ 50.0 (lazy learning iÃ§in yeterli!)
    target_below_acc=0.70,
    target_above_acc=0.75,
    test_data=([X_f_te, X_50_te, X_200_te, X_500_te, X_1000_te], y_reg_te),
    threshold=1.5,
    check_interval=5
)
```

---

## ğŸ¤– CATBOOST DEÄÄ°ÅÄ°KLÄ°KLERÄ°

### 1. Data Split DeÄŸiÅŸiklikleri

**Dosya**: `notebooks/jetx_CATBOOST_TRAINING.py`

**DeÄŸiÅŸtirilecek SatÄ±rlar**: 134-138

#### Eski Kod:
```python
# Train/Test split - STRATIFIED SAMPLING
X_train, X_test, y_reg_train, y_reg_test, y_cls_train, y_cls_test = train_test_split(
    X, y_reg, y_cls, test_size=0.2, shuffle=True, stratify=y_cls, random_state=42
)
```

#### Yeni Kod:
```python
# =============================================================================
# TIME-SERIES SPLIT (KRONOLOJIK) - SHUFFLE YOK!
# =============================================================================
print("\nğŸ“Š TIME-SERIES SPLIT (Kronolojik BÃ¶lme)...")
print("âš ï¸  UYARI: Shuffle devre dÄ±ÅŸÄ± - Zaman serisi yapÄ±sÄ± korunuyor!")

# Test seti: Son 1000 kayÄ±t
test_size = 1000
train_end = len(X) - test_size

# Train/Test split (kronolojik)
X_train = X[:train_end]
X_test = X[train_end:]
y_reg_train = y_reg[:train_end]
y_reg_test = y_reg[train_end:]
y_cls_train = y_cls[:train_end]
y_cls_test = y_cls[train_end:]

print(f"âœ… Train: {len(X_train):,}")
print(f"âœ… Test: {len(X_test):,} (tÃ¼m verinin son {test_size} kaydÄ±)")
print(f"ğŸ“Š Toplam: {len(X_train) + len(X_test):,}")

# Validation iÃ§in train setini bÃ¶l (kronolojik)
val_size = int(len(X_train) * 0.2)
val_start = len(X_train) - val_size

X_tr = X_train[:val_start]
X_val = X_train[val_start:]
y_reg_tr = y_reg_train[:val_start]
y_reg_val = y_reg_train[val_start:]
y_cls_tr = y_cls_train[:val_start]
y_cls_val = y_cls_train[val_start:]

print(f"   â”œâ”€ Actual Train: {len(X_tr):,}")
print(f"   â””â”€ Validation: {len(X_val):,} (train'in son %20'si)")
```

### 2. GPU Callback HatasÄ± DÃ¼zeltmesi

**3 SeÃ§enek**:

#### SeÃ§enek 1: CPU Kullan (En Kolay)
```python
regressor = CatBoostRegressor(
    iterations=1500,
    depth=10,
    learning_rate=0.03,
    l2_leaf_reg=5,
    bootstrap_type='Bernoulli',
    subsample=0.8,
    loss_function='MAE',
    eval_metric='MAE',
    task_type='CPU',  # âœ… GPU â†’ CPU
    verbose=100,
    random_state=42,
    early_stopping_rounds=100
)

# Callback kullanÄ±labilir
regressor.fit(
    X_tr, y_reg_tr,
    eval_set=(X_val, y_reg_val),  # âœ… Manuel eval set (kronolojik!)
    verbose=100,
    callbacks=[virtual_bankroll_reg]  # âœ… CPU'da Ã§alÄ±ÅŸÄ±r
)
```

#### SeÃ§enek 2: Callback'i KaldÄ±r (GPU HÄ±zÄ±nÄ± Korur)
```python
regressor = CatBoostRegressor(
    iterations=1500,
    task_type='GPU',  # GPU korunur
    ...
)

# Callback YOK
regressor.fit(
    X_tr, y_reg_tr,
    eval_set=(X_val, y_reg_val),  # âœ… Manuel eval set (kronolojik!)
    verbose=100
    # callbacks parametresi YOK
)
```

#### SeÃ§enek 3: KoÅŸullu Callback (Ã–nerilen!)
```python
# GPU kontrolÃ¼
try:
    import GPUtil
    has_gpu = len(GPUtil.getGPUs()) > 0
except:
    has_gpu = False

task_type = 'GPU' if has_gpu else 'CPU'

regressor = CatBoostRegressor(
    iterations=1500,
    task_type=task_type,
    ...
)

# Callback sadece CPU'da
callbacks_list = [virtual_bankroll_reg] if task_type == 'CPU' else []

regressor.fit(
    X_tr, y_reg_tr,
    eval_set=(X_val, y_reg_val),
    verbose=100,
    callbacks=callbacks_list  # âœ… KoÅŸullu
)
```

### 3. Class Weight ArtÄ±rÄ±mÄ±

```python
# CatBoost iÃ§in class_weights
class_weights = {0: 20.0, 1: 1.0}  # 2.0 â†’ 20.0 (10x artÄ±ÅŸ!)

classifier = CatBoostClassifier(
    iterations=1500,
    depth=9,
    learning_rate=0.03,
    l2_leaf_reg=5,
    bootstrap_type='Bernoulli',
    subsample=0.8,
    loss_function='Logloss',
    eval_metric='Accuracy',
    task_type='CPU',  # veya koÅŸullu
    class_weights=class_weights,  # âœ… 20x weight
    verbose=100,
    random_state=42,
    early_stopping_rounds=100
)
```

---

## ğŸ“‹ Ä°MPLEMENTASYON ADIMLARI

### AdÄ±m 1: Neural Network Scripti GÃ¼ncelle
```bash
# Dosya: notebooks/jetx_PROGRESSIVE_TRAINING.py
```

**DeÄŸiÅŸiklikler**:
1. âœ… Data split: Kronolojik bÃ¶lme (satÄ±r 323-332)
2. âœ… Model.fit(): shuffle=False, manuel validation (satÄ±r 907-916, 1018-1027, 1125-1134)
3. âœ… Class weights: 15x, 20x, 25x (satÄ±r 866-868, 965-967, 1072-1074)
4. âœ… Adaptive scheduler: max_weight=50.0 (satÄ±r 987-997, 1092-1101)

### AdÄ±m 2: CatBoost Scripti GÃ¼ncelle
```bash
# Dosya: notebooks/jetx_CATBOOST_TRAINING.py
```

**DeÄŸiÅŸiklikler**:
1. âœ… Data split: Kronolojik bÃ¶lme (satÄ±r 134-138)
2. âœ… GPU callback hatasÄ±: KoÅŸullu callback veya CPU (satÄ±r 189-194, 280-285)
3. âœ… Class weights: 20x (satÄ±r 232)
4. âœ… Manual eval set: Kronolojik validation (satÄ±r 189-194, 280-285)

### AdÄ±m 3: Test ve KarÅŸÄ±laÅŸtÄ±rma
```bash
# 1. Neural Network eÄŸit
python notebooks/jetx_PROGRESSIVE_TRAINING.py

# 2. CatBoost eÄŸit
python notebooks/jetx_CATBOOST_TRAINING.py

# 3. SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r
```

---

## ğŸ“Š BEKLENEN SONUÃ‡LAR

### BaÅŸarÄ± Kriterleri

#### Neural Network:
- **1.5 AltÄ± DoÄŸruluk**: %70+  (ÅŸu an: %0.0 âŒ)
- **1.5 ÃœstÃ¼ DoÄŸruluk**: %70-80 (ÅŸu an: %99.85 ama yanÄ±ltÄ±cÄ±)
- **Para KaybÄ± Riski**: <%30  (ÅŸu an: %100 âŒ)
- **Sanal Kasa ROI**: >0%  (ÅŸu an: -2.21% âŒ)
- **Kazanma OranÄ±**: >66.7%  (ÅŸu an: 65.2% âš ï¸)

#### CatBoost:
- **1.5 AltÄ± DoÄŸruluk**: %65+
- **1.5 ÃœstÃ¼ DoÄŸruluk**: %70+
- **Para KaybÄ± Riski**: <%35
- **MAE**: <10.0

### âš ï¸ Ã–nemli Notlar

1. **Daha KÃ¶tÃ¼ Metrikler Beklenebilir**:
   - Shuffle=False olduÄŸu iÃ§in metrikler dÃ¼ÅŸebilir
   - Bu NORMALDIR - gerÃ§ek dÃ¼nya performansÄ±nÄ± gÃ¶sterir!

2. **Lazy Learning AzalmalÄ±**:
   - Model artÄ±k her ÅŸeyi "1.5 Ã¼stÃ¼" diye tahmin etmemeli
   - 1.5 altÄ± doÄŸruluk %0'dan yukarÄ± Ã§Ä±kmalÄ±

3. **Ezberleme Testi**:
   - Time-series split modelin ezberleme eÄŸilimini netleÅŸtirir
   - Train vs Test performans farkÄ± bÃ¼yÃ¼kse â†’ overfitting var

---

## ğŸ” TEST SENARYOLARI

### Senaryo 1: Baseline Test (Åu Anki Kod)
```python
# Shuffle=True (mevcut)
ROI: -2.21%
1.5 AltÄ±: %0.0
Para KaybÄ±: %100
```

### Senaryo 2: Time-Series Split (Hedef)
```python
# Shuffle=False + Kronolojik split
Beklenen ROI: -5% ile +5% arasÄ± (daha gerÃ§ekÃ§i)
Beklenen 1.5 AltÄ±: %40-70 (lazy learning azalÄ±r)
Beklenen Para KaybÄ±: %25-40 (dÃ¼zelir ama hala yÃ¼ksek)
```

### Senaryo 3: Time-Series + 20x Class Weight
```python
# Shuffle=False + 20x weight
Beklenen ROI: -2% ile +8% arasÄ±
Beklenen 1.5 AltÄ±: %60-80 (hedef!)
Beklenen Para KaybÄ±: %20-30 (hedef: <%20)
```

---

## ğŸ“ DOKÃœMANTASYON

EÄŸitim sonrasÄ± oluÅŸturulacak rapor:

```markdown
# TIME-SERIES SPLIT EÄÄ°TÄ°M RAPORU

## YapÄ±landÄ±rma
- Shuffle: False
- Train Size: 3,272
- Validation Size: 818 (kronolojik)
- Test Size: 1,000 (kronolojik)
- Class Weight: 15x â†’ 20x â†’ 25x

## SonuÃ§lar
| Metrik | Shuffle=True (Eski) | Shuffle=False (Yeni) | Fark |
|--------|---------------------|----------------------|------|
| 1.5 AltÄ± DoÄŸruluk | 0.0% | XX.X% | +XX.X% |
| ROI | -2.21% | +X.XX% | +X.XX% |
| Para KaybÄ± Riski | 100% | XX% | -XX% |

## durumu: [AzaldÄ±/Devam ediyor]
- Ezberleme: [Var/Yok]
- Ã–neriler: [...]
```

---

## âš¡ HIZLI BAÅLANGIÃ‡

DeÄŸiÅŸiklikleri uygulamak iÃ§in:

```bash
# 1. Code mode'a geÃ§
switch_mode code

# 2. Her iki scripti gÃ¼ncelle
# - notebooks/jetx_PROGRESSIVE_TRAINING.py
# - notebooks/jetx_CATBOOST_TRAINING.py

# 3. Colab'da test et
# - Upload updated scripts
# - Run training
# - Compare results
```

---

**HazÄ±rlayan**: AI Architect  
**Tarih**: 2025-10-12 21:23  
**Durum**: Ä°mplementasyona hazÄ±r âœ…