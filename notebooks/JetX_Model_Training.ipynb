{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ JetX Predictor - GeliÅŸmiÅŸ Model EÄŸitimi\n",
    "\n",
    "**Model Mimarisi:** N-BEATS + TCN Hibrit Model\n",
    "\n",
    "Bu notebook JetX tahmin sistemi iÃ§in geliÅŸmiÅŸ hibrit modeli eÄŸitir.\n",
    "\n",
    "## ğŸ“‹ Ã–zellikler\n",
    "\n",
    "- **3 N-BEATS Pencere:** 50, 200, 500 el\n",
    "- **TCN ModÃ¼lÃ¼:** Dilated convolutions ile pattern yakalama\n",
    "- **Psikolojik Analiz:** Model Ã¶ÄŸreniyor (net kural yok)\n",
    "- **15 Kategori Seti:** Ã‡ok boyutlu feature extraction\n",
    "- **4 Ã‡Ä±ktÄ±:** Regression, Classification, Confidence, Pattern Risk\n",
    "\n",
    "---\n",
    "\n",
    "âš™ï¸ **Gereksinimler:** Google Colab (GPU Ã¶nerilir), GitHub hesabÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ KÃ¼tÃ¼phane Kurulumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow scikit-learn pandas numpy scipy matplotlib seaborn plotly joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ GitHub'dan Projeyi Klonla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/onndd/jetxpredictor.git\n",
    "%cd jetxpredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ KÃ¼tÃ¼phaneleri Import Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import joblib\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from category_definitions import CategoryDefinitions, FeatureEngineering\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Veriyi YÃ¼kle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('jetx_data.db')\n",
    "df = pd.read_sql_query(\"SELECT * FROM jetx_results ORDER BY id\", conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"ğŸ“Š Toplam veri: {len(df)}\")\n",
    "print(f\"Ä°lk 5 kayÄ±t:\\n{df.head()}\")\n",
    "print(f\"\\nğŸ“ˆ Ä°statistikler:\\n{df['value'].describe()}\")\n",
    "\n",
    "below_15 = len(df[df['value'] < 1.5])\n",
    "above_15 = len(df[df['value'] >= 1.5])\n",
    "print(f\"\\nğŸ¯ 1.5x Analizi:\")\n",
    "print(f\"< 1.5x: {below_15} ({below_15/len(df)*100:.2f}%)\")\n",
    "print(f\">= 1.5x: {above_15} ({above_15/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Ã–zellik Ã‡Ä±karma ve Dataset OluÅŸturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, window_size=50):\n",
    "    X_features = []\n",
    "    X_sequences = []\n",
    "    y_regression = []\n",
    "    y_classification = []\n",
    "    \n",
    "    for i in range(window_size, len(data)):\n",
    "        window = data[i-window_size:i].tolist()\n",
    "        target = data[i]\n",
    "        \n",
    "        features = FeatureEngineering.extract_all_features(window)\n",
    "        X_features.append(list(features.values()))\n",
    "        X_sequences.append(window)\n",
    "        \n",
    "        y_regression.append(target)\n",
    "        y_classification.append(1 if target >= 1.5 else 0)\n",
    "    \n",
    "    return {\n",
    "        'X_features': np.array(X_features),\n",
    "        'X_sequences': np.array(X_sequences),\n",
    "        'y_regression': np.array(y_regression),\n",
    "        'y_classification': np.array(y_classification),\n",
    "        'feature_names': list(features.keys())\n",
    "    }\n",
    "\n",
    "print(\"ğŸ”§ Dataset oluÅŸturuluyor (500 pencere)...\")\n",
    "dataset_500 = create_dataset(df['value'].values, window_size=500)\n",
    "\n",
    "print(f\"âœ… HazÄ±r!\")\n",
    "print(f\"Ã–zellik sayÄ±sÄ±: {dataset_500['X_features'].shape[1]}\")\n",
    "print(f\"Ã–rnek sayÄ±sÄ±: {len(dataset_500['y_regression'])}\")\n",
    "print(f\"Hedef daÄŸÄ±lÄ±mÄ±: {np.bincount(dataset_500['y_classification'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Model BloklarÄ±nÄ± OluÅŸtur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nbeats_block(input_shape, units=64, name_prefix='nbeats'):\n",
    "    inputs = layers.Input(shape=input_shape, name=f'{name_prefix}_input')\n",
    "    \n",
    "    x = layers.Dense(units * 4, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(units * 2, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    backward = layers.Dense(units, activation='relu', name=f'{name_prefix}_backward')(x)\n",
    "    forward = layers.Dense(1, name=f'{name_prefix}_forecast')(x)\n",
    "    features = layers.Dense(units, activation='relu', name=f'{name_prefix}_features')(backward)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[forward, features], name=name_prefix)\n",
    "    return model\n",
    "\n",
    "def create_tcn_block(input_shape, filters=64, kernel_size=3, dilations=[1, 2, 4, 8], name='tcn'):\n",
    "    inputs = layers.Input(shape=input_shape, name=f'{name}_input')\n",
    "    x = layers.Reshape((input_shape[0], 1))(inputs)\n",
    "    \n",
    "    for i, dilation in enumerate(dilations):\n",
    "        conv = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation,\n",
    "            padding='causal',\n",
    "            activation='relu',\n",
    "            name=f'{name}_conv_{i}'\n",
    "        )(x)\n",
    "        \n",
    "        if x.shape[-1] != filters:\n",
    "            x = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "        x = layers.Add()([x, conv])\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    features = layers.Dense(512, activation='relu', name=f'{name}_features')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=features, name=name)\n",
    "    return model\n",
    "\n",
    "def create_psychological_analyzer(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape, name='psych_input')\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    \n",
    "    trap_score = layers.Dense(1, activation='sigmoid', name='trap_score')(x)\n",
    "    cooling_score = layers.Dense(1, activation='sigmoid', name='cooling_score')(x)\n",
    "    momentum_score = layers.Dense(1, activation='tanh', name='momentum_score')(x)\n",
    "    \n",
    "    combined = layers.Concatenate()([trap_score, cooling_score, momentum_score])\n",
    "    features = layers.Dense(32, activation='relu', name='psych_features')(combined)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=features, name='psychological_analyzer')\n",
    "    return model\n",
    "\n",
    "nbeats_short = create_nbeats_block((50,), units=64, name_prefix='nbeats_short')\n",
    "nbeats_medium = create_nbeats_block((200,), units=128, name_prefix='nbeats_medium')\n",
    "nbeats_long = create_nbeats_block((500,), units=256, name_prefix='nbeats_long')\n",
    "tcn_model = create_tcn_block((50,), filters=64, dilations=[1, 2, 4, 8])\n",
    "psych_model = create_psychological_analyzer((dataset_500['X_features'].shape[1],))\n",
    "\n",
    "print(\"âœ… Model bloklarÄ± oluÅŸturuldu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Hibrit Modeli OluÅŸtur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_hybrid_model(feature_dim):\n",
    "    feature_input = layers.Input(shape=(feature_dim,), name='feature_input')\n",
    "    seq_50_input = layers.Input(shape=(50,), name='seq_50_input')\n",
    "    seq_200_input = layers.Input(shape=(200,), name='seq_200_input')\n",
    "    seq_500_input = layers.Input(shape=(500,), name='seq_500_input')\n",
    "    \n",
    "    nbeats_short_forecast, nbeats_short_features = nbeats_short(seq_50_input)\n",
    "    nbeats_medium_forecast, nbeats_medium_features = nbeats_medium(seq_200_input)\n",
    "    nbeats_long_forecast, nbeats_long_features = nbeats_long(seq_500_input)\n",
    "    \n",
    "    weighted_forecast = layers.Average()([
    "        layers.Lambda(lambda x: x * 0.5)(nbeats_short_forecast),\n",
    "        layers.Lambda(lambda x: x * 0.3)(nbeats_medium_forecast),\n",
    "        layers.Lambda(lambda x: x * 0.2)(nbeats_long_forecast)\n",
    "    ])\n",
    "    \n",
    "    nbeats_combined = layers.Concatenate()([nbeats_short_features, nbeats_medium_features, nbeats_long_features])\n",
    "    tcn_features = tcn_model(seq_50_input)\n",
    "    \n",
    "    time_series_features = layers.Concatenate()([nbeats_combined, tcn_features])\n",
    "    time_series_features = layers.Dense(256, activation='relu')(time_series_features)\n",
    "    time_series_features = layers.Dropout(0.3)(time_series_features)\n",
    "    \n",
    "    psych_features = psych_model(feature_input)\n",
    "    stat_features = layers.Dense(16, activation='relu')(feature_input)\n",
    "    \n",
    "    all_features = layers.Concatenate()([time_series_features, psych_features, stat_features])\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(all_features)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    regression_output = layers.Dense(1, name='regression_output')(x)\n",
    "    classification_output = layers.Dense(1, activation='sigmoid', name='classification_output')(x)\n",
    "    confidence_output = layers.Dense(1, activation='sigmoid', name='confidence_output')(x)\n",
    "    pattern_risk_output = layers.Dense(1, activation='sigmoid', name='pattern_risk_output')(x)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[feature_input, seq_50_input, seq_200_input, seq_500_input],\n",
    "        outputs=[regression_output, classification_output, confidence_output, pattern_risk_output],\n",
    "        name='JetX_Advanced_Hybrid_Model'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"ğŸ”¨ Hibrit model oluÅŸturuluyor...\")\n",
    "hybrid_model = create_advanced_hybrid_model(feature_dim=dataset_500['X_features'].shape[1])\n",
    "\n",
    "print(\"âœ… Model oluÅŸturuldu!\")\n",
    "hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Veriyi HazÄ±rla ve Modeli EÄŸit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri bÃ¶lÃ¼mleme\n",
    "split_idx_test = int(len(dataset_500['X_features']) * 0.85)\n",
    "X_feat_train_val = dataset_500['X_features'][:split_idx_test]\n",
    "X_feat_test = dataset_500['X_features'][split_idx_test:]\n",
    "y_reg_train_val = dataset_500['y_regression'][:split_idx_test]\n",
    "y_reg_test = dataset_500['y_regression'][split_idx_test:]\n",
    "y_class_train_val = dataset_500['y_classification'][:split_idx_test]\n",
    "y_class_test = dataset_500['y_classification'][split_idx_test:]\n",
    "\n",
    "split_idx_val = int(len(X_feat_train_val) * 0.85)\n",
    "X_feat_train = X_feat_train_val[:split_idx_val]\n",
    "X_feat_val = X_feat_train_val[split_idx_val:]\n",
    "y_reg_train = y_reg_train_val[:split_idx_val]\n",
    "y_reg_val = y_reg_train_val[split_idx_val:]\n",
    "y_class_train = y_class_train_val[:split_idx_val]\n",
    "y_class_val = y_class_train_val[split_idx_val:]\n",
    "\n",
    "# Sequence veriler\n",
    "seq_50_train_val = dataset_500['X_sequences'][:split_idx_test, -50:]\n",
    "seq_50_test = dataset_500['X_sequences'][split_idx_test:, -50:]\n",
    "seq_50_train = seq_50_train_val[:split_idx_val]\n",
    "seq_50_val = seq_50_train_val[split_idx_val:]\n",
    "\n",
    "seq_200_train_val = dataset_500['X_sequences'][:split_idx_test, -200:]\n",
    "seq_200_test = dataset_500['X_sequences'][split_idx_test:, -200:]\n",
    "seq_200_train = seq_200_train_val[:split_idx_val]\n",
    "seq_200_val = seq_200_train_val[split_idx_val:]\n",
    "\n",
    "seq_500_train_val = dataset_500['X_sequences'][:split_idx_test]\n",
    "seq_500_test = dataset_500['X_sequences'][split_idx_test:]\n",
    "seq_500_train = seq_500_train_val[:split_idx_val]\n",
    "seq_500_val = seq_500_train_val[split_idx_val:]\n",
    "\n",
    "print(f\"ğŸ“Š Veri BÃ¶lÃ¼mleme:\")\n",
    "print(f\"Train: {len(X_feat_train)} ({len(X_feat_train)/len(dataset_500['X_features'])*100:.1f}%)\")\n",
    "print(f\"Val: {len(X_feat_val)} ({len(X_feat_val)/len(dataset_500['X_features'])*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_feat_test)} ({len(X_feat_test)/len(dataset_500['X_features'])*100:.1f}%)\")\n",
    "\n",
    "# Normalizasyon\n",
    "scaler = StandardScaler()\n",
    "X_feat_train_scaled = scaler.fit_transform(X_feat_train)\n",
    "X_feat_val_scaled = scaler.transform(X_feat_val)\n",
    "X_feat_test_scaled = scaler.transform(X_feat_test)\n",
    "\n",
    "# Dummy skorlar\n",
    "y_conf_train = np.random.uniform(0.6, 1.0, size=(len(y_reg_train),))\n",
    "y_conf_val = np.random.uniform(0.6, 1.0, size=(len(y_reg_val),))\n",
    "y_pattern_risk_train = np.random.uniform(0.0, 0.5, size=(len(y_reg_train),))\n",
    "y_pattern_risk_val = np.random.uniform(0.0, 0.5, size=(len(y_reg_val),))\n",
    "\n",
    "# Model derleme\n",
    "loss_weights = {\n",
    "    'regression_output': 0.25,\n",
    "    'classification_output': 0.45,\n",
    "    'confidence_output': 0.15,\n",
    "    'pattern_risk_output': 0.15\n",
    "}\n",
    "\n",
    "hybrid_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'regression_output': 'huber',\n",
    "        'classification_output': 'binary_crossentropy',\n",
    "        'confidence_output': 'mse',\n",
    "        'pattern_risk_output': 'mse'\n",
    "    },\n",
    "    loss_weights=loss_weights,\n",
    "    metrics={\n",
    "        'regression_output': ['mae', 'mse'],\n",
    "        'classification_output': ['accuracy'],\n",
    "        'confidence_output': ['mae'],\n",
    "        'pattern_risk_output': ['mae']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_classification_output_accuracy', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_hybrid_model.h5', monitor='val_classification_output_accuracy', save_best_only=True, mode='max', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "]\n",
    "\n",
    "# EÄŸitim\n",
    "print(\"\\nğŸš€ Model eÄŸitimi baÅŸlÄ±yor...\")\n",
    "print(\"ğŸ’¡ GPU varsa ~10-20 dakika sÃ¼rer\\n\")\n",
    "\n",
    "history = hybrid_model.fit(\n",
    "    [X_feat_train_scaled, seq_50_train, seq_200_train, seq_500_train],\n",
    "    {\n",
    "        'regression_output': y_reg_train,\n",
    "        'classification_output': y_class_train,\n",
    "        'confidence_output': y_conf_train,\n",
    "        'pattern_risk_output': y_pattern_risk_train\n",
    "    },\n",
    "    validation_data=(\n",
    "        [X_feat_val_scaled, seq_50_val, seq_200_val, seq_500_val],\n",
    "        {\n",
    "            'regression_output': y_reg_val,\n",
    "            'classification_output': y_class_val,\n",
    "            'confidence_output': y_conf_val,\n",
    "            'pattern_risk_output': y_pattern_risk_val\n",
    "        }\n",
    "    ),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… EÄŸitim tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Model DeÄŸerlendirmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_conf_test = np.random.uniform(0.6, 1.0, size=(len(y_reg_test),))\n",
    "y_pattern_risk_test = np.random.uniform(0.0, 0.5, size=(len(y_reg_test),))\n",
    "\n",
    "predictions = hybrid_model.predict([X_feat_test_scaled, seq_50_test, seq_200_test, seq_500_test])\n",
    "y_reg_pred, y_class_pred_proba, y_conf_pred, y_pattern_risk_pred = predictions\n",
    "\n",
    "y_class_pred = (y_class_pred_proba > 0.5).astype(int).flatten()\n",
    "threshold_accuracy = accuracy_score(y_class_test, y_class_pred)\n",
    "\n",
    "print(f\"\\nğŸ¯ 1.5x EÅŸik DoÄŸruluÄŸu: {threshold_accuracy:.4f} ({threshold_accuracy*100:.2f}%)\")\n",
    "print(f\"Hedef: %75+\")\n",
    "\n",
    "if threshold_accuracy >= 0.75:\n",
    "    print(\"âœ… Hedef baÅŸarÄ±ldÄ±!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Hedef henÃ¼z ulaÅŸÄ±lamadÄ±\")\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred))\n",
    "r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Regresyon Metrikleri:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"RÂ²: {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nâš ï¸ Ortalama Pattern Risk: {np.mean(y_pattern_risk_pred):.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_class_test, y_class_pred, target_names=['< 1.5x', '>= 1.5x']))\n",
    "\n",
    "cm = confusion_matrix(y_class_test, y_class_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['< 1.5x', '>= 1.5x'],\n",
    "            yticklabels=['< 1.5x', '>= 1.5x'])\n",
    "plt.title('Confusion Matrix - 1.5x EÅŸik Tahmini')\n",
    "plt.ylabel('GerÃ§ek')\n",
    "plt.xlabel('Tahmin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ Modeli Kaydet ve Ä°ndir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model.save('jetx_hybrid_model.h5')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"âœ… Model kaydedildi: jetx_hybrid_model.h5\")\n",
    "print(\"âœ… Scaler kaydedildi: scaler.pkl\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download('jetx_hybrid_model.h5')\n",
    "files.download('scaler.pkl')\n",
    "print(\"\\nâœ… Dosyalar indiriliyor...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ TamamlandÄ±!\n",
    "\n",
    "### Sonraki AdÄ±mlar:\n",
    "\n",
    "1. âœ… Ä°ndirilen dosyalarÄ± (`jetx_hybrid_model.h5` ve `scaler.pkl`) lokal `models/` klasÃ¶rÃ¼ne kopyalayÄ±n\n",
    "2. âœ… Lokal terminalde: `pip install -r requirements.txt`\n",
    "3. âœ… UygulamayÄ± Ã§alÄ±ÅŸtÄ±rÄ±n: `streamlit run app.py`\n",
    "4. âœ… Tahminleri test edin!\n",
    "\n",
    "**GitHub:** https://github.com/onndd/jetxpredictor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
