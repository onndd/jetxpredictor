{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ JetX Predictor - Geli≈ümi≈ü Model Eƒüitimi\n",
    "\n",
    "**Model Mimarisi:** N-BEATS + TCN Hibrit Model\n",
    "\n",
    "Bu notebook JetX tahmin sistemi i√ßin geli≈ümi≈ü hibrit modeli eƒüitir.\n",
    "\n",
    "## üìã √ñzellikler\n",
    "\n",
    "- **3 N-BEATS Pencere:** 50, 200, 500 el\n",
    "- **TCN Mod√ºl√º:** Dilated convolutions ile pattern yakalama\n",
    "- **Psikolojik Analiz:** Model √∂ƒüreniyor (net kural yok)\n",
    "- **15 Kategori Seti:** √áok boyutlu feature extraction\n",
    "- **4 √áƒ±ktƒ±:** Regression, Classification, Confidence, Pattern Risk\n",
    "\n",
    "---\n",
    "\n",
    "‚öôÔ∏è **Gereksinimler:** Google Colab (GPU √∂nerilir), GitHub hesabƒ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ K√ºt√ºphane Kurulumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow scikit-learn pandas numpy scipy matplotlib seaborn plotly joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ GitHub'dan Projeyi Klonla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/onndd/jetxpredictor.git\n",
    "%cd jetxpredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ K√ºt√ºphaneleri Import Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import joblib\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from category_definitions import CategoryDefinitions, FeatureEngineering\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Veriyi Y√ºkle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('jetx_data.db')\n",
    "df = pd.read_sql_query(\"SELECT * FROM jetx_results ORDER BY id\", conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"üìä Toplam veri: {len(df)}\")\n",
    "print(f\"ƒ∞lk 5 kayƒ±t:\\n{df.head()}\")\n",
    "print(f\"\\nüìà ƒ∞statistikler:\\n{df['value'].describe()}\")\n",
    "\n",
    "below_15 = len(df[df['value'] < 1.5])\n",
    "above_15 = len(df[df['value'] >= 1.5])\n",
    "print(f\"\\nüéØ 1.5x Analizi:\")\n",
    "print(f\"< 1.5x: {below_15} ({below_15/len(df)*100:.2f}%)\")\n",
    "print(f\">= 1.5x: {above_15} ({above_15/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ √ñzellik √áƒ±karma ve Dataset Olu≈üturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, window_size=50):\n",
    "    X_features = []\n",
    "    X_sequences = []\n",
    "    y_regression = []\n",
    "    y_classification = []\n",
    "    \n",
    "    for i in range(window_size, len(data)):\n",
    "        window = data[i-window_size:i].tolist()\n",
    "        target = data[i]\n",
    "        \n",
    "        features = FeatureEngineering.extract_all_features(window)\n",
    "        X_features.append(list(features.values()))\n",
    "        X_sequences.append(window)\n",
    "        \n",
    "        y_regression.append(target)\n",
    "        y_classification.append(1 if target >= 1.5 else 0)\n",
    "    \n",
    "    return {\n",
    "        'X_features': np.array(X_features),\n",
    "        'X_sequences': np.array(X_sequences),\n",
    "        'y_regression': np.array(y_regression),\n",
    "        'y_classification': np.array(y_classification),\n",
    "        'feature_names': list(features.keys())\n",
    "    }\n",
    "\n",
    "print(\"üîß Dataset olu≈üturuluyor (500 pencere)...\")\n",
    "dataset_500 = create_dataset(df['value'].values, window_size=500)\n",
    "\n",
    "print(f\"‚úÖ Hazƒ±r!\")\n",
    "print(f\"√ñzellik sayƒ±sƒ±: {dataset_500['X_features'].shape[1]}\")\n",
    "print(f\"√ñrnek sayƒ±sƒ±: {len(dataset_500['y_regression'])}\")\n",
    "print(f\"Hedef daƒüƒ±lƒ±mƒ±: {np.bincount(dataset_500['y_classification'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Model Bloklarƒ±nƒ± Olu≈ütur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nbeats_block(input_shape, units=64, name_prefix='nbeats'):\n",
    "    inputs = layers.Input(shape=input_shape, name=f'{name_prefix}_input')\n",
    "    \n",
    "    x = layers.Dense(units * 4, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(units * 2, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    backward = layers.Dense(units, activation='relu', name=f'{name_prefix}_backward')(x)\n",
    "    forward = layers.Dense(1, name=f'{name_prefix}_forecast')(x)\n",
    "    features = layers.Dense(units, activation='relu', name=f'{name_prefix}_features')(backward)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[forward, features], name=name_prefix)\n",
    "    return model\n",
    "\n",
    "def create_tcn_block(input_shape, filters=64, kernel_size=3, dilations=[1, 2, 4, 8], name='tcn'):\n",
    "    inputs = layers.Input(shape=input_shape, name=f'{name}_input')\n",
    "    x = layers.Reshape((input_shape[0], 1))(inputs)\n",
    "    \n",
    "    for i, dilation in enumerate(dilations):\n",
    "        conv = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation,\n",
    "            padding='causal',\n",
    "            activation='relu',\n",
    "            name=f'{name}_conv_{i}'\n",
    "        )(x)\n",
    "        \n",
    "        if x.shape[-1] != filters:\n",
    "            x = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "        x = layers.Add()([x, conv])\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    features = layers.Dense(512, activation='relu', name=f'{name}_features')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=features, name=name)\n",
    "    return model\n",
    "\n",
    "def create_psychological_analyzer(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape, name='psych_input')\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    \n",
    "    trap_score = layers.Dense(1, activation='sigmoid', name='trap_score')(x)\n",
    "    cooling_score = layers.Dense(1, activation='sigmoid', name='cooling_score')(x)\n",
    "    momentum_score = layers.Dense(1, activation='tanh', name='momentum_score')(x)\n",
    "    \n",
    "    combined = layers.Concatenate()([trap_score, cooling_score, momentum_score])\n",
    "    features = layers.Dense(32, activation='relu', name='psych_features')(combined)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=features, name='psychological_analyzer')\n",
    "    return model\n",
    "\n",
    "nbeats_short = create_nbeats_block((50,), units=64, name_prefix='nbeats_short')\n",
    "nbeats_medium = create_nbeats_block((200,), units=128, name_prefix='nbeats_medium')\n",
    "nbeats_long = create_nbeats_block((500,), units=256, name_prefix='nbeats_long')\n",
    "tcn_model = create_tcn_block((50,), filters=64, dilations=[1, 2, 4, 8])\n",
    "psych_model = create_psychological_analyzer((dataset_500['X_features'].shape[1],))\n",
    "\n",
    "print(\"‚úÖ Model bloklarƒ± olu≈üturuldu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Hibrit Modeli Olu≈ütur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_hybrid_model(feature_dim):\n",
    "    feature_input = layers.Input(shape=(feature_dim,), name='feature_input')\n",
    "    seq_50_input = layers.Input(shape=(50,), name='seq_50_input')\n",
    "    seq_200_input = layers.Input(shape=(200,), name='seq_200_input')\n",
    "    seq_500_input = layers.Input(shape=(500,), name='seq_500_input')\n",
    "    \n",
    "    nbeats_short_forecast, nbeats_short_features = nbeats_short(seq_50_input)\n",
    "    nbeats_medium_forecast, nbeats_medium_features = nbeats_medium(seq_200_input)\n",
    "    nbeats_long_forecast, nbeats_long_features = nbeats_long(seq_500_input)\n",
    "    \n",
    "    weighted_forecast = layers.Average()([
    "        layers.Lambda(lambda x: x * 0.5)(nbeats_short_forecast),\n",
    "        layers.Lambda(lambda x: x * 0.3)(nbeats_medium_forecast),\n",
    "        layers.Lambda(lambda x: x * 0.2)(nbeats_long_forecast)\n",
    "    ])\n",
    "    \n",
    "    nbeats_combined = layers.Concatenate()([nbeats_short_features, nbeats_medium_features, nbeats_long_features])\n",
    "    tcn_features = tcn_model(seq_50_input)\n",
    "    \n",
    "    time_series_features = layers.Concatenate()([nbeats_combined, tcn_features])\n",
    "    time_series_features = layers.Dense(256, activation='relu')(time_series_features)\n",
    "    time_series_features = layers.Dropout(0.3)(time_series_features)\n",
    "    \n",
    "    psych_features = psych_model(feature_input)\n",
    "    stat_features = layers.Dense(16, activation='relu')(feature_input)\n",
    "    \n",
    "    all_features = layers.Concatenate()([time_series_features, psych_features, stat_features])\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(all_features)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    regression_output = layers.Dense(1, name='regression_output')(x)\n",
    "    classification_output = layers.Dense(1, activation='sigmoid', name='classification_output')(x)\n",
    "    confidence_output = layers.Dense(1, activation='sigmoid', name='confidence_output')(x)\n",
    "    pattern_risk_output = layers.Dense(1, activation='sigmoid', name='pattern_risk_output')(x)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[feature_input, seq_50_input, seq_200_input, seq_500_input],\n",
    "        outputs=[regression_output, classification_output, confidence_output, pattern_risk_output],\n",
    "        name='JetX_Advanced_Hybrid_Model'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"üî® Hibrit model olu≈üturuluyor...\")\n",
    "hybrid_model = create_advanced_hybrid_model(feature_dim=dataset_500['X_features'].shape[1])\n",
    "\n",
    "print(\"‚úÖ Model olu≈üturuldu!\")\n",
    "hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Veriyi Hazƒ±rla ve Modeli Eƒüit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri b√∂l√ºmleme\n",
    "split_idx_test = int(len(dataset_500['X_features']) * 0.85)\n",
    "X_feat_train_val = dataset_500['X_features'][:split_idx_test]\n",
    "X_feat_test = dataset_500['X_features'][split_idx_test:]\n",
    "y_reg_train_val = dataset_500['y_regression'][:split_idx_test]\n",
    "y_reg_test = dataset_500['y_regression'][split_idx_test:]\n",
    "y_class_train_val = dataset_500['y_classification'][:split_idx_test]\n",
    "y_class_test = dataset_500['y_classification'][split_idx_test:]\n",
    "\n",
    "split_idx_val = int(len(X_feat_train_val) * 0.85)\n",
    "X_feat_train = X_feat_train_val[:split_idx_val]\n",
    "X_feat_val = X_feat_train_val[split_idx_val:]\n",
    "y_reg_train = y_reg_train_val[:split_idx_val]\n",
    "y_reg_val = y_reg_train_val[split_idx_val:]\n",
    "y_class_train = y_class_train_val[:split_idx_val]\n",
    "y_class_val = y_class_train_val[split_idx_val:]\n",
    "\n",
    "# Sequence veriler\n",
    "seq_50_train_val = dataset_500['X_sequences'][:split_idx_test, -50:]\n",
    "seq_50_test = dataset_500['X_sequences'][split_idx_test:, -50:]\n",
    "seq_50_train = seq_50_train_val[:split_idx_val]\n",
    "seq_50_val = seq_50_train_val[split_idx_val:]\n",
    "\n",
    "seq_200_train_val = dataset_500['X_sequences'][:split_idx_test, -200:]\n",
    "seq_200_test = dataset_500['X_sequences'][split_idx_test:, -200:]\n",
    "seq_200_train = seq_200_train_val[:split_idx_val]\n",
    "seq_200_val = seq_200_train_val[split_idx_val:]\n",
    "\n",
    "seq_500_train_val = dataset_500['X_sequences'][:split_idx_test]\n",
    "seq_500_test = dataset_500['X_sequences'][split_idx_test:]\n",
    "seq_500_train = seq_500_train_val[:split_idx_val]\n",
    "seq_500_val = seq_500_train_val[split_idx_val:]\n",
    "\n",
    "print(f\"üìä Veri B√∂l√ºmleme:\")\n",
    "print(f\"Train: {len(X_feat_train)} ({len(X_feat_train)/len(dataset_500['X_features'])*100:.1f}%)\")\n",
    "print(f\"Val: {len(X_feat_val)} ({len(X_feat_val)/len(dataset_500['X_features'])*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_feat_test)} ({len(X_feat_test)/len(dataset_500['X_features'])*100:.1f}%)\")\n",
    "\n",
    "# Normalizasyon\n",
    "scaler = StandardScaler()\n",
    "X_feat_train_scaled = scaler.fit_transform(X_feat_train)\n",
    "X_feat_val_scaled = scaler.transform(X_feat_val)\n",
    "X_feat_test_scaled = scaler.transform(X_feat_test)\n",
    "\n",
    "# Dummy skorlar\n",
    "y_conf_train = np.random.uniform(0.6, 1.0, size=(len(y_reg_train),))\n",
    "y_conf_val = np.random.uniform(0.6, 1.0, size=(len(y_reg_val),))\n",
    "y_pattern_risk_train = np.random.uniform(0.0, 0.5, size=(len(y_reg_train),))\n",
    "y_pattern_risk_val = np.random.uniform(0.0, 0.5, size=(len(y_reg_val),))\n",
    "\n",
    "# Model derleme\n",
    "loss_weights = {\n",
    "    'regression_output': 0.25,\n",
    "    'classification_output': 0.45,\n",
    "    'confidence_output': 0.15,\n",
    "    'pattern_risk_output': 0.15\n",
    "}\n",
    "\n",
    "hybrid_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'regression_output': 'huber',\n",
    "        'classification_output': 'binary_crossentropy',\n",
    "        'confidence_output': 'mse',\n",
    "        'pattern_risk_output': 'mse'\n",
    "    },\n",
    "    loss_weights=loss_weights,\n",
    "    metrics={\n",
    "        'regression_output': ['mae', 'mse'],\n",
    "        'classification_output': ['accuracy'],\n",
    "        'confidence_output': ['mae'],\n",
    "        'pattern_risk_output': ['mae']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_classification_output_accuracy', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_hybrid_model.h5', monitor='val_classification_output_accuracy', save_best_only=True, mode='max', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "]\n",
    "\n",
    "# Eƒüitim\n",
    "print(\"\\nüöÄ Model eƒüitimi ba≈ülƒ±yor...\")\n",
    "print(\"üí° GPU varsa ~10-20 dakika s√ºrer\\n\")\n",
    "\n",
    "history = hybrid_model.fit(\n",
    "    [X_feat_train_scaled, seq_50_train, seq_200_train, seq_500_train],\n",
    "    {\n",
    "        'regression_output': y_reg_train,\n",
    "        'classification_output': y_class_train,\n",
    "        'confidence_output': y_conf_train,\n",
    "        'pattern_risk_output': y_pattern_risk_train\n",
    "    },\n",
    "    validation_data=(\n",
    "        [X_feat_val_scaled, seq_50_val, seq_200_val, seq_500_val],\n",
    "        {\n",
    "            'regression_output': y_reg_val,\n",
    "            'classification_output': y_class_val,\n",
    "            'confidence_output': y_conf_val,\n",
    "            'pattern_risk_output': y_pattern_risk_val\n",
    "        }\n",
    "    ),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Eƒüitim tamamlandƒ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Model Deƒüerlendirmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_conf_test = np.random.uniform(0.6, 1.0, size=(len(y_reg_test),))\n",
    "y_pattern_risk_test = np.random.uniform(0.0, 0.5, size=(len(y_reg_test),))\n",
    "\n",
    "predictions = hybrid_model.predict([X_feat_test_scaled, seq_50_test, seq_200_test, seq_500_test])\n",
    "y_reg_pred, y_class_pred_proba, y_conf_pred, y_pattern_risk_pred = predictions\n",
    "\n",
    "y_class_pred = (y_class_pred_proba > 0.5).astype(int).flatten()\n",
    "threshold_accuracy = accuracy_score(y_class_test, y_class_pred)\n",
    "\n",
    "print(f\"\\nüéØ 1.5x E≈üik Doƒüruluƒüu: {threshold_accuracy:.4f} ({threshold_accuracy*100:.2f}%)\")\n",
    "print(f\"Hedef: %75+\")\n",
    "\n",
    "if threshold_accuracy >= 0.75:\n",
    "    print(\"‚úÖ Hedef ba≈üarƒ±ldƒ±!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Hedef hen√ºz ula≈üƒ±lamadƒ±\")\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred))\n",
    "r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "\n",
    "print(f\"\\nüìà Regresyon Metrikleri:\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Ortalama Pattern Risk: {np.mean(y_pattern_risk_pred):.4f}\")\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_class_test, y_class_pred, target_names=['< 1.5x', '>= 1.5x']))\n",
    "\n",
    "cm = confusion_matrix(y_class_test, y_class_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['< 1.5x', '>= 1.5x'],\n",
    "            yticklabels=['< 1.5x', '>= 1.5x'])\n",
    "plt.title('Confusion Matrix - 1.5x E≈üik Tahmini')\n",
    "plt.ylabel('Ger√ßek')\n",
    "plt.xlabel('Tahmin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Modeli Kaydet ve ƒ∞ndir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model.save('jetx_hybrid_model.h5')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"‚úÖ Model kaydedildi: jetx_hybrid_model.h5\")\n",
    "print(\"‚úÖ Scaler kaydedildi: scaler.pkl\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download('jetx_hybrid_model.h5')\n",
    "files.download('scaler.pkl')\n",
    "print(\"\\n‚úÖ Dosyalar indiriliyor...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Tamamlandƒ±!\n",
    "\n",
    "### Sonraki Adƒ±mlar:\n",
    "\n",
    "1. ‚úÖ ƒ∞ndirilen dosyalarƒ± (`jetx_hybrid_model.h5` ve `scaler.pkl`) lokal `models/` klas√∂r√ºne kopyalayƒ±n\n",
    "2. ‚úÖ Lokal terminalde: `pip install -r requirements.txt`\n",
    "3. ‚úÖ Uygulamayƒ± √ßalƒ±≈ütƒ±rƒ±n: `streamlit run app.py`\n",
    "4. ‚úÖ Tahminleri test edin!\n",
    "\n",
    "**GitHub:** https://github.com/onndd/jetxpredictor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
