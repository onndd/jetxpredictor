{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 🎰 JetX Predictor - Full Model Training (N-BEATS + TCN)\n",
    "\n",
    "Bu notebook Google Colab'da JetX tahmin modelini eğitir.\n",
    "\n",
    "**Eğitim Süreci:**\n",
    "1. Kütüphaneleri yükle\n",
    "2. GitHub'dan projeyi klonla  \n",
    "3. SQLite veritabanından veri yükle (7000 değer)\n",
    "4. Özellik çıkarma (Feature Engineering - 100+ özellik)\n",
    "5. N-BEATS + TCN modelini oluştur\n",
    "6. Modeli eğit (~45-60 dakika GPU ile)\n",
    "7. Model dosyalarını otomatik indir\n",
    "\n",
    "**Tahmini Süre:** ~1 saat (GPU ile)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 📦 Adım 1: Kütüphaneleri Yükle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libs"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Gerekli kütüphaneleri sessizce yükle\n",
    "!pip install -q tensorflow scikit-learn pandas numpy scipy joblib matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"✅ TensorFlow versiyon: {tf.__version__}\")\n",
    "print(f\"✅ GPU kullanılabilir mi: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "## 🔄 Adım 2: GitHub'dan Projeyi Klonla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Eğer zaten klonlanmışsa tekrar klonlama\n",
    "if not os.path.exists('jetxpredictor'):\n",
    "    !git clone https://github.com/onndd/jetxpredictor.git\n",
    "    print(\"✅ Repo klonlandı\")\n",
    "else:\n",
    "    print(\"✅ Repo zaten mevcut\")\n",
    "\n",
    "# Proje dizinine geç\n",
    "%cd jetxpredictor\n",
    "\n",
    "# Python path'e ekle\n",
    "sys.path.append('/content/jetxpredictor')\n",
    "\n",
    "print(\"✅ Proje yüklendi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_definitions"
   },
   "source": [
    "## 📚 Adım 3: Kategori Tanımlarını Yükle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_definitions"
   },
   "outputs": [],
   "source": [
    "from category_definitions import CategoryDefinitions, FeatureEngineering, SEQUENCE_LENGTHS\n",
    "\n",
    "print(\"✅ CategoryDefinitions yüklendi\")\n",
    "print(f\"✅ Kritik eşik: {CategoryDefinitions.CRITICAL_THRESHOLD}x\")\n",
    "print(f\"✅ Sequence uzunlukları: {SEQUENCE_LENGTHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_data"
   },
   "source": [
    "## 📊 Adım 4: Veritabanından Veri Yükle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_db"
   },
   "outputs": [],
   "source": [
    "# SQLite veritabanını kontrol et\n",
    "db_path = 'jetx_data.db'\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    print(f'❌ HATA: {db_path} bulunamadı!')\n",
    "    print('Lütfen jetx_data.db dosyasını Colab\\'a yükleyin (Files sekmesinden).')\n",
    "    raise FileNotFoundError(f'{db_path} bulunamadı')\n",
    "\n",
    "# Veritabanından veri oku\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Tablo yapısını kontrol et\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(f'Tablolar: {tables}')\n",
    "\n",
    "# Veri yükle\n",
    "try:\n",
    "    data = pd.read_sql_query(\"SELECT value FROM jetx_data ORDER BY id\", conn)\n",
    "except:\n",
    "    data = pd.read_sql_query(\"SELECT actual_value as value FROM predictions WHERE actual_value IS NOT NULL ORDER BY id\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Veriyi numpy array'e çevir\n",
    "all_values = data['value'].values\n",
    "\n",
    "print(f'\\n✅ Toplam {len(all_values)} veri yüklendi')\n",
    "print(f'Veri aralığı: {all_values.min():.2f}x - {all_values.max():.2f}x')\n",
    "print(f'Ortalama: {all_values.mean():.2f}x')\n",
    "print(f'Medyan: {np.median(all_values):.2f}x')\n",
    "print(f'1.5x üstü oran: {(all_values >= 1.5).sum() / len(all_values) * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_engineering"
   },
   "source": [
    "## 🔧 Adım 5: Feature Engineering - Özellik Çıkarma\n",
    "\n",
    "Her değer için 100+ özellik çıkarıyoruz (sliding window yöntemi ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract_features"
   },
   "outputs": [],
   "source": [
    "window_size = 500  # Minimum geçmiş veri\n",
    "prediction_horizon = 1\n",
    "\n",
    "X_features_list = []\n",
    "X_seq_50_list = []\n",
    "X_seq_200_list = []\n",
    "X_seq_500_list = []\n",
    "y_regression_list = []\n",
    "y_classification_list = []\n",
    "\n",
    "print('Özellikler çıkarılıyor...')\n",
    "for i in range(window_size, len(all_values) - prediction_horizon):\n",
    "    history = all_values[:i].tolist()\n",
    "    target_value = all_values[i]\n",
    "    \n",
    "    # Özellikleri çıkar\n",
    "    features = FeatureEngineering.extract_all_features(history)\n",
    "    feature_values = np.array(list(features.values()))\n",
    "    \n",
    "    # Sequence'leri oluştur\n",
    "    seq_50 = all_values[i-50:i]\n",
    "    seq_200 = all_values[i-200:i]\n",
    "    seq_500 = all_values[i-500:i]\n",
    "    \n",
    "    # Hedef değerler\n",
    "    y_reg = target_value\n",
    "    y_class = CategoryDefinitions.get_category_numeric(target_value)\n",
    "    y_class_onehot = np.zeros(3)\n",
    "    y_class_onehot[y_class] = 1\n",
    "    \n",
    "    X_features_list.append(feature_values)\n",
    "    X_seq_50_list.append(seq_50)\n",
    "    X_seq_200_list.append(seq_200)\n",
    "    X_seq_500_list.append(seq_500)\n",
    "    y_regression_list.append(y_reg)\n",
    "    y_classification_list.append(y_class_onehot)\n",
    "    \n",
    "    if (i - window_size + 1) % 500 == 0:\n",
    "        print(f'İşlenen: {i - window_size + 1} / {len(all_values) - window_size - prediction_horizon}')\n",
    "\n",
    "# Numpy array'lere çevir\n",
    "X_features = np.array(X_features_list)\n",
    "X_seq_50 = np.array(X_seq_50_list).reshape(-1, 50, 1)\n",
    "X_seq_200 = np.array(X_seq_200_list).reshape(-1, 200, 1)\n",
    "X_seq_500 = np.array(X_seq_500_list).reshape(-1, 500, 1)\n",
    "y_regression = np.array(y_regression_list)\n",
    "y_classification = np.array(y_classification_list)\n",
    "\n",
    "print(f'\\n✅ Özellik çıkarma tamamlandı')\n",
    "print(f'X_features shape: {X_features.shape}')\n",
    "print(f'X_seq_50 shape: {X_seq_50.shape}')\n",
    "print(f'X_seq_200 shape: {X_seq_200.shape}')\n",
    "print(f'X_seq_500 shape: {X_seq_500.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "normalize"
   },
   "source": [
    "## 📐 Adım 6: Normalizasyon ve Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "normalize_split"
   },
   "outputs": [],
   "source": [
    "# Features için StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_features_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# Sequence'ler için log transform\n",
    "def normalize_sequence(seq):\n",
    "    return np.log10(seq + 1e-8)\n",
    "\n",
    "X_seq_50_norm = normalize_sequence(X_seq_50)\n",
    "X_seq_200_norm = normalize_sequence(X_seq_200)\n",
    "X_seq_500_norm = normalize_sequence(X_seq_500)\n",
    "\n",
    "# Train/Test split (%80/%20)\n",
    "test_size = 0.2\n",
    "indices = np.arange(len(X_features_scaled))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=test_size, shuffle=False)\n",
    "\n",
    "# Train data\n",
    "X_features_train = X_features_scaled[train_idx]\n",
    "X_seq_50_train = X_seq_50_norm[train_idx]\n",
    "X_seq_200_train = X_seq_200_norm[train_idx]\n",
    "X_seq_500_train = X_seq_500_norm[train_idx]\n",
    "y_reg_train = y_regression[train_idx]\n",
    "y_class_train = y_classification[train_idx]\n",
    "\n",
    "# Test data\n",
    "X_features_test = X_features_scaled[test_idx]\n",
    "X_seq_50_test = X_seq_50_norm[test_idx]\n",
    "X_seq_200_test = X_seq_200_norm[test_idx]\n",
    "X_seq_500_test = X_seq_500_norm[test_idx]\n",
    "y_reg_test = y_regression[test_idx]\n",
    "y_class_test = y_classification[test_idx]\n",
    "\n",
    "print(f'✅ Veri normalize edildi ve bölündü')\n",
    "print(f'Train samples: {len(X_features_train)}')\n",
    "print(f'Test samples: {len(X_features_test)}')\n",
    "print(f'Feature boyutu: {X_features_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_architecture"
   },
   "source": [
    "## 🏗️ Adım 7: N-BEATS + TCN Model Mimarisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "build_model"
   },
   "outputs": [],
   "source": [
    "n_features = X_features_train.shape[1]\n",
    "\n",
    "# INPUT LAYERS\n",
    "input_features = layers.Input(shape=(n_features,), name='input_features')\n",
    "input_seq_50 = layers.Input(shape=(50, 1), name='input_seq_50')\n",
    "input_seq_200 = layers.Input(shape=(200, 1), name='input_seq_200')\n",
    "input_seq_500 = layers.Input(shape=(500, 1), name='input_seq_500')\n",
    "\n",
    "# N-BEATS BLOKLARI\n",
    "def create_nbeats_block(input_layer, units, num_blocks, name_prefix):\n",
    "    x = input_layer\n",
    "    for i in range(num_blocks):\n",
    "        x = layers.Dense(units, activation='relu', name=f'{name_prefix}_block{i}_dense1')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Dense(units, activation='relu', name=f'{name_prefix}_block{i}_dense2')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "    return x\n",
    "\n",
    "nbeats_short = layers.Flatten()(input_seq_50)\n",
    "nbeats_short = create_nbeats_block(nbeats_short, 64, 3, 'nbeats_short')\n",
    "nbeats_short = layers.Dense(64, activation='relu', name='nbeats_short_output')(nbeats_short)\n",
    "\n",
    "nbeats_medium = layers.Flatten()(input_seq_200)\n",
    "nbeats_medium = create_nbeats_block(nbeats_medium, 128, 4, 'nbeats_medium')\n",
    "nbeats_medium = layers.Dense(128, activation='relu', name='nbeats_medium_output')(nbeats_medium)\n",
    "\n",
    "nbeats_long = layers.Flatten()(input_seq_500)\n",
    "nbeats_long = create_nbeats_block(nbeats_long, 256, 5, 'nbeats_long')\n",
    "nbeats_long = layers.Dense(256, activation='relu', name='nbeats_long_output')(nbeats_long)\n",
    "\n",
    "nbeats_combined = layers.Concatenate(name='nbeats_combined')([nbeats_short, nbeats_medium, nbeats_long])\n",
    "\n",
    "# TCN MODÜLÜ\n",
    "def create_tcn_block(input_layer, filters, kernel_size, dilation_rate, name_prefix):\n",
    "    conv = layers.Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding='causal',\n",
    "        activation='relu',\n",
    "        name=f'{name_prefix}_conv'\n",
    "    )(input_layer)\n",
    "    conv = layers.Dropout(0.2)(conv)\n",
    "    \n",
    "    if input_layer.shape[-1] != filters:\n",
    "        residual = layers.Conv1D(filters, 1, padding='same', name=f'{name_prefix}_residual')(input_layer)\n",
    "    else:\n",
    "        residual = input_layer\n",
    "    \n",
    "    return layers.Add(name=f'{name_prefix}_add')([conv, residual])\n",
    "\n",
    "tcn = input_seq_500\n",
    "tcn = create_tcn_block(tcn, 64, 3, 1, 'tcn_block1')\n",
    "tcn = create_tcn_block(tcn, 64, 3, 2, 'tcn_block2')\n",
    "tcn = create_tcn_block(tcn, 128, 3, 4, 'tcn_block3')\n",
    "tcn = create_tcn_block(tcn, 128, 3, 8, 'tcn_block4')\n",
    "tcn = create_tcn_block(tcn, 256, 3, 16, 'tcn_block5')\n",
    "tcn = create_tcn_block(tcn, 256, 3, 32, 'tcn_block6')\n",
    "\n",
    "tcn_output = layers.GlobalAveragePooling1D(name='tcn_global_pool')(tcn)\n",
    "tcn_output = layers.Dense(512, activation='relu', name='tcn_output')(tcn_output)\n",
    "\n",
    "# ENSEMBLE FUSION\n",
    "combined = layers.Concatenate(name='ensemble_fusion')([input_features, nbeats_combined, tcn_output])\n",
    "\n",
    "fusion = layers.Dense(512, activation='relu', name='fusion_dense1')(combined)\n",
    "fusion = layers.Dropout(0.3)(fusion)\n",
    "fusion = layers.Dense(256, activation='relu', name='fusion_dense2')(fusion)\n",
    "fusion = layers.Dropout(0.3)(fusion)\n",
    "fusion = layers.Dense(128, activation='relu', name='fusion_dense3')(fusion)\n",
    "\n",
    "# MULTI-OUTPUT HEADS\n",
    "regression_head = layers.Dense(64, activation='relu', name='regression_head')(fusion)\n",
    "output_regression = layers.Dense(1, activation='linear', name='output_regression')(regression_head)\n",
    "\n",
    "classification_head = layers.Dense(64, activation='relu', name='classification_head')(fusion)\n",
    "output_classification = layers.Dense(3, activation='softmax', name='output_classification')(classification_head)\n",
    "\n",
    "confidence_head = layers.Dense(32, activation='relu', name='confidence_head')(fusion)\n",
    "output_confidence = layers.Dense(1, activation='sigmoid', name='output_confidence')(confidence_head)\n",
    "\n",
    "risk_head = layers.Dense(32, activation='relu', name='risk_head')(fusion)\n",
    "output_pattern_risk = layers.Dense(1, activation='sigmoid', name='output_pattern_risk')(risk_head)\n",
    "\n",
    "# MODEL\n",
    "model = models.Model(\n",
    "    inputs=[input_features, input_seq_50, input_seq_200, input_seq_500],\n",
    "    outputs=[output_regression, output_classification, output_confidence, output_pattern_risk],\n",
    "    name='JetX_NBEATS_TCN_Model'\n",
    ")\n",
    "\n",
    "print('✅ Model oluşturuldu')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile"
   },
   "source": [
    "## ⚙️ Adım 8: Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile_model"
   },
   "outputs": [],
   "source": [
    "# Custom loss: 1.5x threshold için özel ağırlık\n",
    "def threshold_weighted_mae(y_true, y_pred):\n",
    "    mae = tf.abs(y_true - y_pred)\n",
    "    threshold_weight = tf.where(\n",
    "        tf.logical_and(y_true >= 1.4, y_true <= 1.6),\n",
    "        2.0,\n",
    "        1.0\n",
    "    )\n",
    "    weighted_mae = mae * threshold_weight\n",
    "    return tf.reduce_mean(weighted_mae)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'output_regression': threshold_weighted_mae,\n",
    "        'output_classification': 'categorical_crossentropy',\n",
    "        'output_confidence': 'binary_crossentropy',\n",
    "        'output_pattern_risk': 'binary_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'output_regression': 0.4,\n",
    "        'output_classification': 0.3,\n",
    "        'output_confidence': 0.2,\n",
    "        'output_pattern_risk': 0.1\n",
    "    },\n",
    "    metrics={\n",
    "        'output_regression': ['mae', 'mse'],\n",
    "        'output_classification': ['accuracy'],\n",
    "        'output_confidence': ['accuracy'],\n",
    "        'output_pattern_risk': ['accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "print('✅ Model compile edildi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "callbacks"
   },
   "source": [
    "## 🎛️ Adım 9: Callbacks Hazırlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_callbacks"
   },
   "outputs": [],
   "source": [
    "# Hedef değerleri hazırla\n",
    "y_confidence_train = np.where(\n",
    "    (y_reg_train >= 1.45) & (y_reg_train <= 1.55),\n",
    "    0.3,\n",
    "    0.7\n",
    ")\n",
    "\n",
    "y_risk_train = np.where(y_reg_train < 1.5, 0.8, 0.2)\n",
    "\n",
    "y_confidence_test = np.where(\n",
    "    (y_reg_test >= 1.45) & (y_reg_test <= 1.55),\n",
    "    0.3,\n",
    "    0.7\n",
    ")\n",
    "\n",
    "y_risk_test = np.where(y_reg_test < 1.5, 0.8, 0.2)\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    'jetx_model_best.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=7,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('✅ Callbacks hazırlandı')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 🚀 Adım 10: Model Eğitimi - BAŞLAT!\n",
    "\n",
    "⚠️ **Bu adım ~45-60 dakika sürebilir (GPU ile)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "print('🚀 Model eğitimi başlıyor...')\n",
    "print(f'Epochs: 100 (early stopping ile)')\n",
    "print(f'Batch size: 32')\n",
    "print(f'Validation split: 20%')\n",
    "print('\\n' + '='*50 + '\\n')\n",
    "\n",
    "history = model.fit(\n",
    "    [X_features_train, X_seq_50_train, X_seq_200_train, X_seq_500_train],\n",
    "    {\n",
    "        'output_regression': y_reg_train,\n",
    "        'output_classification': y_class_train,\n",
    "        'output_confidence': y_confidence_train,\n",
    "        'output_pattern_risk': y_risk_train\n",
    "    },\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('\\n' + '='*50)\n",
    "print('✅ Eğitim tamamlandı!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "## 📈 Adım 11: Training History Görselleştirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_history"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['output_regression_mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_output_regression_mae'], label='Val MAE')\n",
    "plt.title('Regression MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'En iyi validation loss: {min(history.history[\"val_loss\"]):.4f}')\n",
    "print(f'En iyi MAE: {min(history.history[\"val_output_regression_mae\"]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate"
   },
   "source": [
    "## 📊 Adım 12: Test Seti Değerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "print('Test seti değerlendiriliyor...')\n",
    "\n",
    "predictions = model.predict(\n",
    "    [X_features_test, X_seq_50_test, X_seq_200_test, X_seq_500_test],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "pred_regression = predictions[0].flatten()\n",
    "pred_classification = predictions[1]\n",
    "\n",
    "# Metrikleri hesapla\n",
    "mae = mean_absolute_error(y_reg_test, pred_regression)\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_test, pred_regression))\n",
    "\n",
    "# 1.5x threshold accuracy\n",
    "y_threshold_true = (y_reg_test >= 1.5).astype(int)\n",
    "y_threshold_pred = (pred_regression >= 1.5).astype(int)\n",
    "threshold_accuracy = accuracy_score(y_threshold_true, y_threshold_pred)\n",
    "\n",
    "# Classification accuracy\n",
    "y_class_true = np.argmax(y_class_test, axis=1)\n",
    "y_class_pred = np.argmax(pred_classification, axis=1)\n",
    "class_accuracy = accuracy_score(y_class_true, y_class_pred)\n",
    "\n",
    "print('\\n' + '='*50)\n",
    "print('TEST SETİ SONUÇLARI')\n",
    "print('='*50)\n",
    "print(f'\\n📊 REGRESSION METRİKLERİ:')\n",
    "print(f'  MAE: {mae:.4f}')\n",
    "print(f'  RMSE: {rmse:.4f}')\n",
    "print(f'\\n🎯 1.5x EŞİK DOĞRULUĞU (EN ÖNEMLİ):')\n",
    "print(f'  Accuracy: {threshold_accuracy*100:.2f}%')\n",
    "print(f'  Hedef: >75%')\n",
    "if threshold_accuracy >= 0.75:\n",
    "    print('  ✅ HEDEF BAŞARILI!')\n",
    "else:\n",
    "    print('  ⚠️ Hedefin altında')\n",
    "print(f'\\n📁 KATEGORİ DOĞRULUĞU:')\n",
    "print(f'  Accuracy: {class_accuracy*100:.2f}%')\n",
    "print(f'  Hedef: >60%')\n",
    "if class_accuracy >= 0.60:\n",
    "    print('  ✅ HEDEF BAŞARILI!')\n",
    "else:\n",
    "    print('  ⚠️ Hedefin altında')\n",
    "\n",
    "print(f'\\n📋 DETAYLI KATEGORİ RAPORU:')\n",
    "print(classification_report(\n",
    "    y_class_true,\n",
    "    y_class_pred,\n",
    "    target_names=['Kayıp (<1.5x)', 'Güvenli (1.5-10x)', 'Yüksek (>10x)']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## 💾 Adım 13: Model Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Model kaydet\n",
    "model.save('jetx_model.h5')\n",
    "print('✅ Model kaydedildi: jetx_model.h5')\n",
    "\n",
    "# Scaler kaydet\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print('✅ Scaler kaydedildi: scaler.pkl')\n",
    "\n",
    "# Model bilgileri\n",
    "import json\n",
    "model_info = {\n",
    "    'n_features': int(n_features),\n",
    "    'threshold_accuracy': float(threshold_accuracy),\n",
    "    'class_accuracy': float(class_accuracy),\n",
    "    'mae': float(mae),\n",
    "    'rmse': float(rmse),\n",
    "    'train_samples': len(X_features_train),\n",
    "    'test_samples': len(X_features_test),\n",
    "    'total_epochs': len(history.history['loss'])\n",
    "}\n",
    "\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print('✅ Model bilgileri kaydedildi: model_info.json')\n",
    "print(json.dumps(model_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 📥 Adım 14: Dosyaları İndirin!\n",
    "\n",
    "**Aşağıdaki hücreyi çalıştırarak model dosyalarını bilgisayarınıza indirin.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print('Model dosyaları indiriliyor...')\n",
    "print('\\n⏬ Tarayıcınız indirme isteyecek, lütfen kabul edin.\\n')\n",
    "\n",
    "print('1/3 - jetx_model.h5 indiriliyor...')\n",
    "files.download('jetx_model.h5')\n",
    "\n",
    "print('2/3 - scaler.pkl indiriliyor...')\n",
    "files.download('scaler.pkl')\n",
    "\n",
    "print('3/3 - model_info.json indiriliyor...')\n",
    "files.download('model_info.json')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('✅ TÜM DOSYALAR İNDİRİLDİ!')\n",
    "print('='*70)\n",
    "print('\\nSONRAKİ ADIMLAR:')\n",
    "print('1. İndirilen dosyaları lokal projenizin models/ klasörüne kopyalayın')\n",
    "print('2. Streamlit uygulamanızı çalıştırın: streamlit run app.py')\n",
    "print('3. Tahmin yapmaya başlayın!')\n",
    "print('\\n📊 Model Performansı:')\n",
    "print(f'  - 1.5x Eşik Doğruluğu: {threshold_accuracy*100:.2f}%')\n",
    "print(f'  - Kategori Doğruluğu: {class_accuracy*100:.2f}%')\n",
    "print(f'  - MAE: {mae:.4f}')\n",
    "print('\\n🎉 Başarılar!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
