{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸ° JetX Predictor - Full Model Training (N-BEATS + TCN)\n",
    "\n",
    "Bu notebook Google Colab'da JetX tahmin modelini eÄŸitir.\n",
    "\n",
    "**EÄŸitim SÃ¼reci:**\n",
    "1. KÃ¼tÃ¼phaneleri yÃ¼kle\n",
    "2. GitHub'dan projeyi klonla  \n",
    "3. SQLite veritabanÄ±ndan veri yÃ¼kle (7000 deÄŸer)\n",
    "4. Ã–zellik Ã§Ä±karma (Feature Engineering - 100+ Ã¶zellik)\n",
    "5. N-BEATS + TCN modelini oluÅŸtur\n",
    "6. Modeli eÄŸit (~45-60 dakika GPU ile)\n",
    "7. Model dosyalarÄ±nÄ± otomatik indir\n",
    "\n",
    "**Tahmini SÃ¼re:** ~1 saat (GPU ile)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ğŸ“¦ AdÄ±m 1: KÃ¼tÃ¼phaneleri YÃ¼kle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libs"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Gerekli kÃ¼tÃ¼phaneleri sessizce yÃ¼kle\n",
    "!pip install -q tensorflow scikit-learn pandas numpy scipy joblib matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"âœ… TensorFlow versiyon: {tf.__version__}\")\n",
    "print(f\"âœ… GPU kullanÄ±labilir mi: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "## ğŸ”„ AdÄ±m 2: GitHub'dan Projeyi Klonla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# EÄŸer zaten klonlanmÄ±ÅŸsa tekrar klonlama\n",
    "if not os.path.exists('jetxpredictor'):\n",
    "    !git clone https://github.com/onndd/jetxpredictor.git\n",
    "    print(\"âœ… Repo klonlandÄ±\")\n",
    "else:\n",
    "    print(\"âœ… Repo zaten mevcut\")\n",
    "\n",
    "# Proje dizinine geÃ§\n",
    "%cd jetxpredictor\n",
    "\n",
    "# Python path'e ekle\n",
    "sys.path.append('/content/jetxpredictor')\n",
    "\n",
    "print(\"âœ… Proje yÃ¼klendi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_definitions"
   },
   "source": [
    "## ğŸ“š AdÄ±m 3: Kategori TanÄ±mlarÄ±nÄ± YÃ¼kle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_definitions"
   },
   "outputs": [],
   "source": [
    "from category_definitions import CategoryDefinitions, FeatureEngineering, SEQUENCE_LENGTHS\n",
    "\n",
    "print(\"âœ… CategoryDefinitions yÃ¼klendi\")\n",
    "print(f\"âœ… Kritik eÅŸik: {CategoryDefinitions.CRITICAL_THRESHOLD}x\")\n",
    "print(f\"âœ… Sequence uzunluklarÄ±: {SEQUENCE_LENGTHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_data"
   },
   "source": [
    "## ğŸ“Š AdÄ±m 4: VeritabanÄ±ndan Veri YÃ¼kle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_db"
   },
   "outputs": [],
   "source": [
    "# SQLite veritabanÄ±nÄ± kontrol et\n",
    "db_path = 'jetx_data.db'\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    print(f'âŒ HATA: {db_path} bulunamadÄ±!')\n",
    "    print('LÃ¼tfen jetx_data.db dosyasÄ±nÄ± Colab\\'a yÃ¼kleyin (Files sekmesinden).')\n",
    "    raise FileNotFoundError(f'{db_path} bulunamadÄ±')\n",
    "\n",
    "# VeritabanÄ±ndan veri oku\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Tablo yapÄ±sÄ±nÄ± kontrol et\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(f'Tablolar: {tables}')\n",
    "\n",
    "# Veri yÃ¼kle\n",
    "try:\n",
    "    data = pd.read_sql_query(\"SELECT value FROM jetx_data ORDER BY id\", conn)\n",
    "except:\n",
    "    data = pd.read_sql_query(\"SELECT actual_value as value FROM predictions WHERE actual_value IS NOT NULL ORDER BY id\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Veriyi numpy array'e Ã§evir\n",
    "all_values = data['value'].values\n",
    "\n",
    "print(f'\\nâœ… Toplam {len(all_values)} veri yÃ¼klendi')\n",
    "print(f'Veri aralÄ±ÄŸÄ±: {all_values.min():.2f}x - {all_values.max():.2f}x')\n",
    "print(f'Ortalama: {all_values.mean():.2f}x')\n",
    "print(f'Medyan: {np.median(all_values):.2f}x')\n",
    "print(f'1.5x Ã¼stÃ¼ oran: {(all_values >= 1.5).sum() / len(all_values) * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_engineering"
   },
   "source": [
    "## ğŸ”§ AdÄ±m 5: Feature Engineering - Ã–zellik Ã‡Ä±karma\n",
    "\n",
    "Her deÄŸer iÃ§in 100+ Ã¶zellik Ã§Ä±karÄ±yoruz (sliding window yÃ¶ntemi ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract_features"
   },
   "outputs": [],
   "source": [
    "window_size = 500  # Minimum geÃ§miÅŸ veri\n",
    "prediction_horizon = 1\n",
    "\n",
    "X_features_list = []\n",
    "X_seq_50_list = []\n",
    "X_seq_200_list = []\n",
    "X_seq_500_list = []\n",
    "y_regression_list = []\n",
    "y_classification_list = []\n",
    "\n",
    "print('Ã–zellikler Ã§Ä±karÄ±lÄ±yor...')\n",
    "for i in range(window_size, len(all_values) - prediction_horizon):\n",
    "    history = all_values[:i].tolist()\n",
    "    target_value = all_values[i]\n",
    "    \n",
    "    # Ã–zellikleri Ã§Ä±kar\n",
    "    features = FeatureEngineering.extract_all_features(history)\n",
    "    feature_values = np.array(list(features.values()))\n",
    "    \n",
    "    # Sequence'leri oluÅŸtur\n",
    "    seq_50 = all_values[i-50:i]\n",
    "    seq_200 = all_values[i-200:i]\n",
    "    seq_500 = all_values[i-500:i]\n",
    "    \n",
    "    # Hedef deÄŸerler\n",
    "    y_reg = target_value\n",
    "    y_class = CategoryDefinitions.get_category_numeric(target_value)\n",
    "    y_class_onehot = np.zeros(3)\n",
    "    y_class_onehot[y_class] = 1\n",
    "    \n",
    "    X_features_list.append(feature_values)\n",
    "    X_seq_50_list.append(seq_50)\n",
    "    X_seq_200_list.append(seq_200)\n",
    "    X_seq_500_list.append(seq_500)\n",
    "    y_regression_list.append(y_reg)\n",
    "    y_classification_list.append(y_class_onehot)\n",
    "    \n",
    "    if (i - window_size + 1) % 500 == 0:\n",
    "        print(f'Ä°ÅŸlenen: {i - window_size + 1} / {len(all_values) - window_size - prediction_horizon}')\n",
    "\n",
    "# Numpy array'lere Ã§evir\n",
    "X_features = np.array(X_features_list)\n",
    "X_seq_50 = np.array(X_seq_50_list).reshape(-1, 50, 1)\n",
    "X_seq_200 = np.array(X_seq_200_list).reshape(-1, 200, 1)\n",
    "X_seq_500 = np.array(X_seq_500_list).reshape(-1, 500, 1)\n",
    "y_regression = np.array(y_regression_list)\n",
    "y_classification = np.array(y_classification_list)\n",
    "\n",
    "print(f'\\nâœ… Ã–zellik Ã§Ä±karma tamamlandÄ±')\n",
    "print(f'X_features shape: {X_features.shape}')\n",
    "print(f'X_seq_50 shape: {X_seq_50.shape}')\n",
    "print(f'X_seq_200 shape: {X_seq_200.shape}')\n",
    "print(f'X_seq_500 shape: {X_seq_500.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "normalize"
   },
   "source": [
    "## ğŸ“ AdÄ±m 6: Normalizasyon ve Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "normalize_split"
   },
   "outputs": [],
   "source": [
    "# Features iÃ§in StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_features_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# Sequence'ler iÃ§in log transform\n",
    "def normalize_sequence(seq):\n",
    "    return np.log10(seq + 1e-8)\n",
    "\n",
    "X_seq_50_norm = normalize_sequence(X_seq_50)\n",
    "X_seq_200_norm = normalize_sequence(X_seq_200)\n",
    "X_seq_500_norm = normalize_sequence(X_seq_500)\n",
    "\n",
    "# Train/Test split (%80/%20)\n",
    "test_size = 0.2\n",
    "indices = np.arange(len(X_features_scaled))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=test_size, shuffle=False)\n",
    "\n",
    "# Train data\n",
    "X_features_train = X_features_scaled[train_idx]\n",
    "X_seq_50_train = X_seq_50_norm[train_idx]\n",
    "X_seq_200_train = X_seq_200_norm[train_idx]\n",
    "X_seq_500_train = X_seq_500_norm[train_idx]\n",
    "y_reg_train = y_regression[train_idx]\n",
    "y_class_train = y_classification[train_idx]\n",
    "\n",
    "# Test data\n",
    "X_features_test = X_features_scaled[test_idx]\n",
    "X_seq_50_test = X_seq_50_norm[test_idx]\n",
    "X_seq_200_test = X_seq_200_norm[test_idx]\n",
    "X_seq_500_test = X_seq_500_norm[test_idx]\n",
    "y_reg_test = y_regression[test_idx]\n",
    "y_class_test = y_classification[test_idx]\n",
    "\n",
    "print(f'âœ… Veri normalize edildi ve bÃ¶lÃ¼ndÃ¼')\n",
    "print(f'Train samples: {len(X_features_train)}')\n",
    "print(f'Test samples: {len(X_features_test)}')\n",
    "print(f'Feature boyutu: {X_features_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_architecture"
   },
   "source": [
    "## ğŸ—ï¸ AdÄ±m 7: N-BEATS + TCN Model Mimarisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "build_model"
   },
   "outputs": [],
   "source": [
    "n_features = X_features_train.shape[1]\n",
    "\n",
    "# INPUT LAYERS\n",
    "input_features = layers.Input(shape=(n_features,), name='input_features')\n",
    "input_seq_50 = layers.Input(shape=(50, 1), name='input_seq_50')\n",
    "input_seq_200 = layers.Input(shape=(200, 1), name='input_seq_200')\n",
    "input_seq_500 = layers.Input(shape=(500, 1), name='input_seq_500')\n",
    "\n",
    "# N-BEATS BLOKLARI\n",
    "def create_nbeats_block(input_layer, units, num_blocks, name_prefix):\n",
    "    x = input_layer\n",
    "    for i in range(num_blocks):\n",
    "        x = layers.Dense(units, activation='relu', name=f'{name_prefix}_block{i}_dense1')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Dense(units, activation='relu', name=f'{name_prefix}_block{i}_dense2')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "    return x\n",
    "\n",
    "nbeats_short = layers.Flatten()(input_seq_50)\n",
    "nbeats_short = create_nbeats_block(nbeats_short, 64, 3, 'nbeats_short')\n",
    "nbeats_short = layers.Dense(64, activation='relu', name='nbeats_short_output')(nbeats_short)\n",
    "\n",
    "nbeats_medium = layers.Flatten()(input_seq_200)\n",
    "nbeats_medium = create_nbeats_block(nbeats_medium, 128, 4, 'nbeats_medium')\n",
    "nbeats_medium = layers.Dense(128, activation='relu', name='nbeats_medium_output')(nbeats_medium)\n",
    "\n",
    "nbeats_long = layers.Flatten()(input_seq_500)\n",
    "nbeats_long = create_nbeats_block(nbeats_long, 256, 5, 'nbeats_long')\n",
    "nbeats_long = layers.Dense(256, activation='relu', name='nbeats_long_output')(nbeats_long)\n",
    "\n",
    "nbeats_combined = layers.Concatenate(name='nbeats_combined')([nbeats_short, nbeats_medium, nbeats_long])\n",
    "\n",
    "# TCN MODÃœLÃœ\n",
    "def create_tcn_block(input_layer, filters, kernel_size, dilation_rate, name_prefix):\n",
    "    conv = layers.Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding='causal',\n",
    "        activation='relu',\n",
    "        name=f'{name_prefix}_conv'\n",
    "    )(input_layer)\n",
    "    conv = layers.Dropout(0.2)(conv)\n",
    "    \n",
    "    if input_layer.shape[-1] != filters:\n",
    "        residual = layers.Conv1D(filters, 1, padding='same', name=f'{name_prefix}_residual')(input_layer)\n",
    "    else:\n",
    "        residual = input_layer\n",
    "    \n",
    "    return layers.Add(name=f'{name_prefix}_add')([conv, residual])\n",
    "\n",
    "tcn = input_seq_500\n",
    "tcn = create_tcn_block(tcn, 64, 3, 1, 'tcn_block1')\n",
    "tcn = create_tcn_block(tcn, 64, 3, 2, 'tcn_block2')\n",
    "tcn = create_tcn_block(tcn, 128, 3, 4, 'tcn_block3')\n",
    "tcn = create_tcn_block(tcn, 128, 3, 8, 'tcn_block4')\n",
    "tcn = create_tcn_block(tcn, 256, 3, 16, 'tcn_block5')\n",
    "tcn = create_tcn_block(tcn, 256, 3, 32, 'tcn_block6')\n",
    "\n",
    "tcn_output = layers.GlobalAveragePooling1D(name='tcn_global_pool')(tcn)\n",
    "tcn_output = layers.Dense(512, activation='relu', name='tcn_output')(tcn_output)\n",
    "\n",
    "# ENSEMBLE FUSION\n",
    "combined = layers.Concatenate(name='ensemble_fusion')([input_features, nbeats_combined, tcn_output])\n",
    "\n",
    "fusion = layers.Dense(512, activation='relu', name='fusion_dense1')(combined)\n",
    "fusion = layers.Dropout(0.3)(fusion)\n",
    "fusion = layers.Dense(256, activation='relu', name='fusion_dense2')(fusion)\n",
    "fusion = layers.Dropout(0.3)(fusion)\n",
    "fusion = layers.Dense(128, activation='relu', name='fusion_dense3')(fusion)\n",
    "\n",
    "# MULTI-OUTPUT HEADS\n",
    "regression_head = layers.Dense(64, activation='relu', name='regression_head')(fusion)\n",
    "output_regression = layers.Dense(1, activation='linear', name='output_regression')(regression_head)\n",
    "\n",
    "classification_head = layers.Dense(64, activation='relu', name='classification_head')(fusion)\n",
    "output_classification = layers.Dense(3, activation='softmax', name='output_classification')(classification_head)\n",
    "\n",
    "confidence_head = layers.Dense(32, activation='relu', name='confidence_head')(fusion)\n",
    "output_confidence = layers.Dense(1, activation='sigmoid', name='output_confidence')(confidence_head)\n",
    "\n",
    "risk_head = layers.Dense(32, activation='relu', name='risk_head')(fusion)\n",
    "output_pattern_risk = layers.Dense(1, activation='sigmoid', name='output_pattern_risk')(risk_head)\n",
    "\n",
    "# MODEL\n",
    "model = models.Model(\n",
    "    inputs=[input_features, input_seq_50, input_seq_200, input_seq_500],\n",
    "    outputs=[output_regression, output_classification, output_confidence, output_pattern_risk],\n",
    "    name='JetX_NBEATS_TCN_Model'\n",
    ")\n",
    "\n",
    "print('âœ… Model oluÅŸturuldu')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile"
   },
   "source": [
    "## âš™ï¸ AdÄ±m 8: Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile_model"
   },
   "outputs": [],
   "source": [
    "# Custom loss: 1.5x threshold iÃ§in Ã¶zel aÄŸÄ±rlÄ±k\n",
    "def threshold_weighted_mae(y_true, y_pred):\n",
    "    mae = tf.abs(y_true - y_pred)\n",
    "    threshold_weight = tf.where(\n",
    "        tf.logical_and(y_true >= 1.4, y_true <= 1.6),\n",
    "        2.0,\n",
    "        1.0\n",
    "    )\n",
    "    weighted_mae = mae * threshold_weight\n",
    "    return tf.reduce_mean(weighted_mae)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'output_regression': threshold_weighted_mae,\n",
    "        'output_classification': 'categorical_crossentropy',\n",
    "        'output_confidence': 'binary_crossentropy',\n",
    "        'output_pattern_risk': 'binary_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'output_regression': 0.4,\n",
    "        'output_classification': 0.3,\n",
    "        'output_confidence': 0.2,\n",
    "        'output_pattern_risk': 0.1\n",
    "    },\n",
    "    metrics={\n",
    "        'output_regression': ['mae', 'mse'],\n",
    "        'output_classification': ['accuracy'],\n",
    "        'output_confidence': ['accuracy'],\n",
    "        'output_pattern_risk': ['accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "print('âœ… Model compile edildi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "callbacks"
   },
   "source": [
    "## ğŸ›ï¸ AdÄ±m 9: Callbacks HazÄ±rlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_callbacks"
   },
   "outputs": [],
   "source": [
    "# Hedef deÄŸerleri hazÄ±rla\n",
    "y_confidence_train = np.where(\n",
    "    (y_reg_train >= 1.45) & (y_reg_train <= 1.55),\n",
    "    0.3,\n",
    "    0.7\n",
    ")\n",
    "\n",
    "y_risk_train = np.where(y_reg_train < 1.5, 0.8, 0.2)\n",
    "\n",
    "y_confidence_test = np.where(\n",
    "    (y_reg_test >= 1.45) & (y_reg_test <= 1.55),\n",
    "    0.3,\n",
    "    0.7\n",
    ")\n",
    "\n",
    "y_risk_test = np.where(y_reg_test < 1.5, 0.8, 0.2)\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    'jetx_model_best.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=7,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('âœ… Callbacks hazÄ±rlandÄ±')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## ğŸš€ AdÄ±m 10: Model EÄŸitimi - BAÅLAT!\n",
    "\n",
    "âš ï¸ **Bu adÄ±m ~45-60 dakika sÃ¼rebilir (GPU ile)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "print('ğŸš€ Model eÄŸitimi baÅŸlÄ±yor...')\n",
    "print(f'Epochs: 100 (early stopping ile)')\n",
    "print(f'Batch size: 32')\n",
    "print(f'Validation split: 20%')\n",
    "print('\\n' + '='*50 + '\\n')\n",
    "\n",
    "history = model.fit(\n",
    "    [X_features_train, X_seq_50_train, X_seq_200_train, X_seq_500_train],\n",
    "    {\n",
    "        'output_regression': y_reg_train,\n",
    "        'output_classification': y_class_train,\n",
    "        'output_confidence': y_confidence_train,\n",
    "        'output_pattern_risk': y_risk_train\n",
    "    },\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('\\n' + '='*50)\n",
    "print('âœ… EÄŸitim tamamlandÄ±!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "## ğŸ“ˆ AdÄ±m 11: Training History GÃ¶rselleÅŸtirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_history"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['output_regression_mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_output_regression_mae'], label='Val MAE')\n",
    "plt.title('Regression MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'En iyi validation loss: {min(history.history[\"val_loss\"]):.4f}')\n",
    "print(f'En iyi MAE: {min(history.history[\"val_output_regression_mae\"]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate"
   },
   "source": [
    "## ğŸ“Š AdÄ±m 12: Test Seti DeÄŸerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "print('Test seti deÄŸerlendiriliyor...')\n",
    "\n",
    "predictions = model.predict(\n",
    "    [X_features_test, X_seq_50_test, X_seq_200_test, X_seq_500_test],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "pred_regression = predictions[0].flatten()\n",
    "pred_classification = predictions[1]\n",
    "\n",
    "# Metrikleri hesapla\n",
    "mae = mean_absolute_error(y_reg_test, pred_regression)\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_test, pred_regression))\n",
    "\n",
    "# 1.5x threshold accuracy\n",
    "y_threshold_true = (y_reg_test >= 1.5).astype(int)\n",
    "y_threshold_pred = (pred_regression >= 1.5).astype(int)\n",
    "threshold_accuracy = accuracy_score(y_threshold_true, y_threshold_pred)\n",
    "\n",
    "# Classification accuracy\n",
    "y_class_true = np.argmax(y_class_test, axis=1)\n",
    "y_class_pred = np.argmax(pred_classification, axis=1)\n",
    "class_accuracy = accuracy_score(y_class_true, y_class_pred)\n",
    "\n",
    "print('\\n' + '='*50)\n",
    "print('TEST SETÄ° SONUÃ‡LARI')\n",
    "print('='*50)\n",
    "print(f'\\nğŸ“Š REGRESSION METRÄ°KLERÄ°:')\n",
    "print(f'  MAE: {mae:.4f}')\n",
    "print(f'  RMSE: {rmse:.4f}')\n",
    "print(f'\\nğŸ¯ 1.5x EÅÄ°K DOÄRULUÄU (EN Ã–NEMLÄ°):')\n",
    "print(f'  Accuracy: {threshold_accuracy*100:.2f}%')\n",
    "print(f'  Hedef: >75%')\n",
    "if threshold_accuracy >= 0.75:\n",
    "    print('  âœ… HEDEF BAÅARILI!')\n",
    "else:\n",
    "    print('  âš ï¸ Hedefin altÄ±nda')\n",
    "print(f'\\nğŸ“ KATEGORÄ° DOÄRULUÄU:')\n",
    "print(f'  Accuracy: {class_accuracy*100:.2f}%')\n",
    "print(f'  Hedef: >60%')\n",
    "if class_accuracy >= 0.60:\n",
    "    print('  âœ… HEDEF BAÅARILI!')\n",
    "else:\n",
    "    print('  âš ï¸ Hedefin altÄ±nda')\n",
    "\n",
    "print(f'\\nğŸ“‹ DETAYLI KATEGORÄ° RAPORU:')\n",
    "print(classification_report(\n",
    "    y_class_true,\n",
    "    y_class_pred,\n",
    "    target_names=['KayÄ±p (<1.5x)', 'GÃ¼venli (1.5-10x)', 'YÃ¼ksek (>10x)']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## ğŸ’¾ AdÄ±m 13: Model Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Model kaydet\n",
    "model.save('jetx_model.h5')\n",
    "print('âœ… Model kaydedildi: jetx_model.h5')\n",
    "\n",
    "# Scaler kaydet\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print('âœ… Scaler kaydedildi: scaler.pkl')\n",
    "\n",
    "# Model bilgileri\n",
    "import json\n",
    "model_info = {\n",
    "    'n_features': int(n_features),\n",
    "    'threshold_accuracy': float(threshold_accuracy),\n",
    "    'class_accuracy': float(class_accuracy),\n",
    "    'mae': float(mae),\n",
    "    'rmse': float(rmse),\n",
    "    'train_samples': len(X_features_train),\n",
    "    'test_samples': len(X_features_test),\n",
    "    'total_epochs': len(history.history['loss'])\n",
    "}\n",
    "\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print('âœ… Model bilgileri kaydedildi: model_info.json')\n",
    "print(json.dumps(model_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## ğŸ“¥ AdÄ±m 14: DosyalarÄ± Ä°ndirin!\n",
    "\n",
    "**AÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rarak model dosyalarÄ±nÄ± bilgisayarÄ±nÄ±za indirin.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print('Model dosyalarÄ± indiriliyor...')\n",
    "print('\\nâ¬ TarayÄ±cÄ±nÄ±z indirme isteyecek, lÃ¼tfen kabul edin.\\n')\n",
    "\n",
    "print('1/3 - jetx_model.h5 indiriliyor...')\n",
    "files.download('jetx_model.h5')\n",
    "\n",
    "print('2/3 - scaler.pkl indiriliyor...')\n",
    "files.download('scaler.pkl')\n",
    "\n",
    "print('3/3 - model_info.json indiriliyor...')\n",
    "files.download('model_info.json')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('âœ… TÃœM DOSYALAR Ä°NDÄ°RÄ°LDÄ°!')\n",
    "print('='*70)\n",
    "print('\\nSONRAKÄ° ADIMLAR:')\n",
    "print('1. Ä°ndirilen dosyalarÄ± lokal projenizin models/ klasÃ¶rÃ¼ne kopyalayÄ±n')\n",
    "print('2. Streamlit uygulamanÄ±zÄ± Ã§alÄ±ÅŸtÄ±rÄ±n: streamlit run app.py')\n",
    "print('3. Tahmin yapmaya baÅŸlayÄ±n!')\n",
    "print('\\nğŸ“Š Model PerformansÄ±:')\n",
    "print(f'  - 1.5x EÅŸik DoÄŸruluÄŸu: {threshold_accuracy*100:.2f}%')\n",
    "print(f'  - Kategori DoÄŸruluÄŸu: {class_accuracy*100:.2f}%')\n",
    "print(f'  - MAE: {mae:.4f}')\n",
    "print('\\nğŸ‰ BaÅŸarÄ±lar!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
