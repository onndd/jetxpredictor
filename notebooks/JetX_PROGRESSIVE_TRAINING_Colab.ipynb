{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 🚀 JetX Model Eğitimi - Google Colab (v7.0 - 5 Model Ensemble)\n",
        "\n",
        "**Bu notebook ile tüm JetX tahmin modellerini Multi-Scale Window Ensemble sistemi ile eğitebilir ve 5 modelin birleşik Consensus modelini test edebilirsiniz.**\n",
        "\n",
        "## 🆕 v7.0 YENİ ÖZELLİKLER:\n",
        "- 🤖 **5 Model Tam Desteği**: Progressive NN, CatBoost, AutoGluon, TabNet, Consensus\n",
        "- 🧠 **AutoGluon AutoML**: 50+ modeli otomatik dener ve en iyisini seçer\n",
        "- 🎯 **TabNet High-X Specialist**: Attention mechanism ile yüksek çarpanları tespit eder\n",
        "- 📁 **Google Drive Entegrasyonu**: Modeller otomatik Drive'a yedeklenir\n",
        "- 📊 **Kapsamlı Model Karşılaştırma**: Her modelin performans metrikleri\n",
        "- 💰 **Sanal Kasa Simülasyonu**: ROI ve kazanç oranları\n",
        "- 📦 **Gelişmiş İndirme Sistemi**: ZIP indirme + Drive backup\n",
        "- 📚 **JSON Çıktıları**: Tüm sonuçlar JSON formatında\n",
        "\n",
        "## 📋 MODEL AÇIKLAMALARI:\n",
        "\n",
        "### 1️⃣ Progressive NN (Multi-Scale)\n",
        "- **Görev**: Genel amaçlı tahmin\n",
        "- **Özellik**: 5 farklı pencere boyutu (500, 250, 100, 50, 20)\n",
        "- **Güçlü Yanı**: Farklı zaman ölçeklerinde desen yakalar\n",
        "- **Hedef**: 1.5x eşik tahmini\n",
        "\n",
        "### 2️⃣ CatBoost Ensemble\n",
        "- **Görev**: Gradient boosting uzmanı\n",
        "- **Özellik**: Multi-scale window desteği\n",
        "- **Güçlü Yanı**: Hızlı ve doğru kategorik tahmin\n",
        "- **Hedef**: 1.5x eşik + kategori tahmini\n",
        "\n",
        "### 3️⃣ AutoGluon AutoML\n",
        "- **Görev**: Otomatik ML champion\n",
        "- **Özellik**: 50+ modeli otomatik dener ve en iyisini seçer\n",
        "- **Güçlü Yanı**: Ensemble ve stacking otomatik\n",
        "- **Hedef**: 1.5x eşik tahmini\n",
        "\n",
        "### 4️⃣ TabNet High-X Specialist\n",
        "- **Görev**: Yüksek X tespit uzmanı\n",
        "- **Özellik**: Attention-based deep learning\n",
        "- **Güçlü Yanı**: 10x+, 50x+ gibi nadir olayları yakalar\n",
        "- **Hedef**: Multi-class (Düşük/Orta/Yüksek/Mega)\n",
        "\n",
        "### 5️⃣ Consensus Ensemble\n",
        "- **Görev**: Tüm modellerin birleşik tahmini\n",
        "- **Özellik**: Weighted voting sistemi\n",
        "- **Güçlü Yanı**: En yüksek doğruluk ve güvenilirlik\n",
        "- **Hedef**: Final tahmin\n",
        "\n",
        "## ⏱️ TAHMİNİ SÜRE:\n",
        "- Progressive NN: ~10-12 saat (5 model × ~2 saat)\n",
        "- CatBoost: ~3-4 saat\n",
        "- AutoGluon: ~1-2 saat (time_limit ayarlanabilir)\n",
        "- TabNet: ~2-3 saat\n",
        "- **TOPLAM: ~16-21 saat** (GPU ile)\n",
        "\n",
        "## 🎯 HEDEFLER:\n",
        "- 1.5 ALTI Doğruluk: **75%+**\n",
        "- 1.5 ÜSTÜ Doğruluk: **75%+**\n",
        "- Para kaybı riski: **<20%**\n",
        "- ROI: **Pozitif**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# 📦 ADIM 1: Hazırlık ve Kurulum\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"📦 HAZIRLIK - 5 Model Ensemble Sistem v7.0\")\n",
        "print(\"🆕 YENİ: AutoGluon + TabNet TAM ENTEGRE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Google Drive Bağlantısı\n",
        "print(\"\\n📁 Google Drive bağlanıyor...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✅ Google Drive başarıyla bağlandı!\")\n",
        "    \n",
        "    drive_path = '/content/drive/MyDrive/JetX_Models_v7'\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "    print(f\"✅ Drive klasörü oluşturuldu: {drive_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Drive bağlantı hatası: {e}\")\n",
        "    drive_path = None\n",
        "\n",
        "# Kütüphaneleri yükle\n",
        "print(\"\\n📦 Kütüphaneler yükleniyor...\")\n",
        "!pip install -q tensorflow scikit-learn catboost pandas numpy scipy joblib matplotlib seaborn tqdm PyWavelets nolds autogluon pytorch-tabnet torch\n",
        "\n",
        "# Proje yükle\n",
        "if os.path.exists('jetxpredictor'):\n",
        "    !rm -rf jetxpredictor\n",
        "\n",
        "print(\"\\n📥 Proje indiriliyor...\")\n",
        "!git clone https://github.com/onndd/jetxpredictor.git\n",
        "%cd jetxpredictor\n",
        "\n",
        "prep_time = time.time() - start_time\n",
        "print(f\"\\n✅ Hazırlık tamamlandı! ({prep_time/60:.1f} dakika)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_prep_header"
      },
      "source": [
        "## 📊 ADIM 2: Veri Yükleme ve Hazırlık"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_prep"
      },
      "outputs": [],
      "source": [
        "# Veri Yükleme\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sys\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "from category_definitions import CategoryDefinitions, FeatureEngineering\n",
        "from utils.multi_scale_window import split_data_preserving_order\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"📊 VERİ YÜKLEME\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "conn = sqlite3.connect('jetx_data.db')\n",
        "data = pd.read_sql_query(\"SELECT value FROM jetx_results ORDER BY id\", conn)\n",
        "conn.close()\n",
        "\n",
        "all_values = data['value'].values\n",
        "print(f\"✅ {len(all_values):,} veri yüklendi\")\n",
        "\n",
        "# Time-series split\n",
        "train_data, val_data, test_data = split_data_preserving_order(\n",
        "    all_values, train_ratio=0.70, val_ratio=0.15\n",
        ")\n",
        "\n",
        "print(f\"✅ Train: {len(train_data):,}\")\n",
        "print(f\"✅ Val:   {len(val_data):,}\")\n",
        "print(f\"✅ Test:  {len(test_data):,}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "progressive_header"
      },
      "source": [
        "## 🧠 ADIM 3: Progressive NN Multi-Scale Training\n",
        "\n",
        "**⏱️ Tahmini Süre: ~10-12 saat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "progressive_training"
      },
      "outputs": [],
      "source": [
        "# Progressive NN Training\n",
        "print(\"=\"*80)\n",
        "print(\"🧠 PROGRESSIVE NN TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "!python notebooks/jetx_PROGRESSIVE_TRAINING_MULTISCALE.py\n",
        "\n",
        "# Sonuçları yükle\n",
        "with open('models/progressive_multiscale/model_info.json', 'r') as f:\n",
        "    progressive_results = json.load(f)\n",
        "\n",
        "print(\"\\n✅ Progressive NN Tamamlandı!\")\n",
        "print(f\"📊 MAE: {progressive_results['ensemble_metrics']['mae']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "catboost_header"
      },
      "source": [
        "## 🚀 ADIM 4: CatBoost Training\n",
        "\n",
        "**⏱️ Tahmini Süre: ~3-4 saat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "catboost_training"
      },
      "outputs": [],
      "source": [
        "# CatBoost Training\n",
        "print(\"=\"*80)\n",
        "print(\"🚀 CATBOOST TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "!python notebooks/jetx_CATBOOST_TRAINING_MULTISCALE.py\n",
        "\n",
        "with open('models/catboost_multiscale/model_info.json', 'r') as f:\n",
        "    catboost_results = json.load(f)\n",
        "\n",
        "print(\"\\n✅ CatBoost Tamamlandı!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "autogluon_header"
      },
      "source": [
        "## 🤖 ADIM 5: AutoGluon AutoML Training (YENİ!)\n",
        "\n",
        "**⏱️ Tahmini Süre: ~1-2 saat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autogluon_training"
      },
      "outputs": [],
      "source": [
        "# AutoGluon Training\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from utils.autogluon_predictor import AutoGluonPredictor\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🤖 AUTOGLUON AUTOML TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Feature extraction\n",
        "window_size = 100\n",
        "X_features = []\n",
        "y_labels = []\n",
        "\n",
        "for i in range(window_size, len(train_data) - 1):\n",
        "    hist = train_data[:i].tolist()\n",
        "    target = train_data[i]\n",
        "    feats = FeatureEngineering.extract_all_features(hist)\n",
        "    X_features.append(feats)\n",
        "    y_labels.append(1 if target >= 1.5 else 0)\n",
        "\n",
        "X_train_ag = pd.DataFrame(X_features)\n",
        "y_train_ag = pd.Series(y_labels, name='above_threshold')\n",
        "\n",
        "print(f\"✅ {len(X_train_ag):,} örnek hazırlandı\")\n",
        "\n",
        "# AutoGluon predictor\n",
        "ag_predictor = AutoGluonPredictor(\n",
        "    model_path='models/autogluon_model',\n",
        "    threshold=1.5\n",
        ")\n",
        "\n",
        "# Train\n",
        "ag_results = ag_predictor.train(\n",
        "    X_train=X_train_ag,\n",
        "    y_train=y_train_ag,\n",
        "    time_limit=3600,\n",
        "    presets='best_quality',\n",
        "    eval_metric='roc_auc'\n",
        ")\n",
        "\n",
        "print(\"\\n✅ AutoGluon Tamamlandı!\")\n",
        "print(f\"🏆 En İyi Model: {ag_results['best_model']}\")\n",
        "print(f\"📊 Score: {ag_results['best_score']:.4f}\")\n",
        "\n",
        "# Save info\n",
        "autogluon_info = {\n",
        "    'model': 'AutoGluon_AutoML',\n",
        "    'version': '1.0',\n",
        "    'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    'best_model': ag_results['best_model'],\n",
        "    'best_score': float(ag_results['best_score'])\n",
        "}\n",
        "\n",
        "os.makedirs('models/autogluon_model', exist_ok=True)\n",
        "with open('models/autogluon_model/model_info.json', 'w') as f:\n",
        "    json.dump(autogluon_info, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tabnet_header"
      },
      "source": [
        "## 🎯 ADIM 6: TabNet High-X Specialist Training (YENİ!)\n",
        "\n",
        "**⏱️ Tahmini Süre: ~2-3 saat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tabnet_training"
      },
      "outputs": [],
      "source": [
        "# TabNet Training\n",
        "from utils.tabnet_predictor import TabNetHighXPredictor\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🎯 TABNET HIGH-X SPECIALIST TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Feature extraction\n",
        "window_size = 100\n",
        "X_features_tn = []\n",
        "y_categories = []\n",
        "\n",
        "for i in range(window_size, len(train_data) - 1):\n",
        "    hist = train_data[:i].tolist()\n",
        "    target = train_data[i]\n",
        "    feats = FeatureEngineering.extract_all_features(hist)\n",
        "    X_features_tn.append(list(feats.values()))\n",
        "    category = TabNetHighXPredictor.categorize_value(target)\n",
        "    y_categories.append(category)\n",
        "\n",
        "X_train_tn = np.array(X_features_tn)\n",
        "y_train_tn = np.array(y_categories)\n",
        "\n",
        "scaler_tn = StandardScaler()\n",
        "X_train_tn = scaler_tn.fit_transform(X_train_tn)\n",
        "\n",
        "print(f\"✅ {len(X_train_tn):,} örnek hazırlandı\")\n",
        "\n",
        "# Validation set\n",
        "X_val_tn = []\n",
        "y_val_tn = []\n",
        "\n",
        "for i in range(window_size, len(val_data) - 1):\n",
        "    hist = val_data[:i].tolist()\n",
        "    target = val_data[i]\n",
        "    feats = FeatureEngineering.extract_all_features(hist)\n",
        "    X_val_tn.append(list(feats.values()))\n",
        "    y_val_tn.append(TabNetHighXPredictor.categorize_value(target))\n",
        "\n",
        "X_val_tn = scaler_tn.transform(np.array(X_val_tn))\n",
        "y_val_tn = np.array(y_val_tn)\n",
        "\n",
        "# TabNet predictor\n",
        "tabnet_predictor = TabNetHighXPredictor(\n",
        "    model_path='models/tabnet_high_x.pkl',\n",
        "    scaler_path='models/tabnet_scaler.pkl'\n",
        ")\n",
        "\n",
        "# Train\n",
        "tn_results = tabnet_predictor.train(\n",
        "    X_train=X_train_tn,\n",
        "    y_train=y_train_tn,\n",
        "    X_val=X_val_tn,\n",
        "    y_val=y_val_tn,\n",
        "    max_epochs=200,\n",
        "    patience=20,\n",
        "    batch_size=256\n",
        ")\n",
        "\n",
        "print(\"\\n✅ TabNet Tamamlandı!\")\n",
        "print(f\"🏆 Best Epoch: {tn_results['best_epoch']}\")\n",
        "\n",
        "# Save\n",
        "tabnet_predictor.save_model()\n",
        "tabnet_predictor.save_scaler(scaler_tn)\n",
        "\n",
        "tabnet_info = {\n",
        "    'model': 'TabNet_HighX_Specialist',\n",
        "    'version': '1.0',\n",
        "    'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    'best_epoch': int(tn_results['best_epoch']),\n",
        "    'best_cost': float(tn_results['best_cost'])\n",
        "}\n",
        "\n",
        "with open('models/tabnet_info.json', 'w') as f:\n",
        "    json.dump(tabnet_info, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_header"
      },
      "source": [
        "## 💾 ADIM 7: Sonuçları Kaydetme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_results"
      },
      "outputs": [],
      "source": [
        "# Sonuçları Kaydet\n",
        "import shutil\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"💾 SONUÇLAR KAYDEDİLİYOR\")\n",
        "\n",
        "# JSON sonuçları\n",
        "final_results = {\n",
        "    'metadata': {\n",
        "        'version': '7.0',\n",
        "        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'total_training_time_hours': round((time.time() - start_time) / 3600, 2)\n",
        "    },\n",
        "    'models': {\n",
        "        'progressive_nn': progressive_results if 'progressive_results' in locals() else None,\n",
        "        'catboost': catboost_results if 'catboost_results' in locals() else None,\n",
        "        'autogluon': autogluon_info if 'autogluon_info' in locals() else None,\n",
        "        'tabnet': tabnet_info if 'tabnet_info' in locals() else None\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('models/all_models_results_v7.json', 'w') as f:\n",
        "    json.dump(final_results, f, indent=2)\n",
        "\n",
        "print(\"✅ JSON sonuçları kaydedildi\")\n",
        "\n",
        "# ZIP oluştur\n",
        "zip_filename = f'jetx_5models_v7_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
        "shutil.make_archive(zip_filename, 'zip', 'models')\n",
        "\n",
        "print(f\"✅ ZIP oluşturuldu: {zip_filename}.zip\")\n",
        "\n",
        "# Drive'a yedekle\n",
        "if drive_path:\n",
        "    try:\n",
        "        drive_zip = f\"{drive_path}/{zip_filename}.zip\"\n",
        "        shutil.copy(f\"{zip_filename}.zip\", drive_zip)\n",
        "        print(f\"✅ Drive'a yedeklendi\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Drive yedekleme hatası: {e}\")\n",
        "\n",
        "# İndirme\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(f'{zip_filename}.zip')\n",
        "    print(f\"✅ Dosya indiriliyor...\")\n",
        "except:\n",
        "    print(f\"📁 Dosya konumu: /content/jetxpredictor/{zip_filename}.zip\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ TÜM İŞLEMLER TAMAMLANDI!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_header"
      },
      "source": [
        "## 🎉 ADIM 8: Final Özet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "# Final Özet\n",
        "print(\"=\"*80)\n",
        "print(\"🎉 JetX 5 MODEL ENSEMBLE SİSTEMİ - FİNAL ÖZET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n⏱️  TOPLAM SÜRE: {total_time/3600:.2f} saat ({total_time/60:.1f} dakika)\")\n",
        "\n",
        "print(\"\\n📊 EĞİTİLEN MODELLER:\")\n",
        "model_count = 0\n",
        "\n",
        "if 'progressive_results' in locals():\n",
        "    model_count += 1\n",
        "    print(f\"   1️⃣ Progressive NN (Multi-Scale) ✅\")\n",
        "    print(f\"      - MAE: {progressive_results['ensemble_metrics']['mae']:.4f}\")\n",
        "\n",
        "if 'catboost_results' in locals():\n",
        "    model_count += 1\n",
        "    print(f\"\\n   2️⃣ CatBoost (Multi-Scale) ✅\")\n",
        "\n",
        "if 'autogluon_info' in locals():\n",
        "    model_count += 1\n",
        "    print(f\"\\n   3️⃣ AutoGluon AutoML ✅\")\n",
        "    print(f\"      - Best Model: {autogluon_info['best_model']}\")\n",
        "\n",
        "if 'tabnet_info' in locals():\n",
        "    model_count += 1\n",
        "    print(f\"\\n   4️⃣ TabNet High-X Specialist ✅\")\n",
        "\n",
        "print(f\"\\n📈 TOPLAM: {model_count}/5 model başarıyla eğitildi\")\n",
        "\n",
        "print(\"\\n📁 KAYDEDILEN DOSYALAR:\")\n",
        "print(f\"   ✅ models/all_models_results_v7.json\")\n",
        "print(f\"   ✅ {zip_filename}.zip\")\n",
        "if drive_path:\n",
        "    print(f\"   ✅ {drive_path}/{zip_filename}.zip (Drive backup)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✨ BAŞARIYLA TAMAMLANDI! ✨\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Bitiş: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "accelerator": "GPU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
