{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ğŸš€ JetX Model EÄŸitimi - Google Colab (v7.0 - 5 Model Ensemble)\n",
        "\n",
        "**Bu notebook ile tÃ¼m JetX tahmin modellerini Multi-Scale Window Ensemble sistemi ile eÄŸitebilir ve 5 modelin birleÅŸik Consensus modelini test edebilirsiniz.**\n",
        "\n",
        "## ğŸ†• v7.0 YENÄ° Ã–ZELLÄ°KLER:\n",
        "- ğŸ¤– **5 Model Tam DesteÄŸi**: Progressive NN, CatBoost, AutoGluon, TabNet, Consensus\n",
        "- ğŸ§  **AutoGluon AutoML**: 50+ modeli otomatik dener ve en iyisini seÃ§er\n",
        "- ğŸ¯ **TabNet High-X Specialist**: Attention mechanism ile yÃ¼ksek Ã§arpanlarÄ± tespit eder\n",
        "- ğŸ“ **Google Drive Entegrasyonu**: Modeller otomatik Drive'a yedeklenir\n",
        "- ğŸ“Š **KapsamlÄ± Model KarÅŸÄ±laÅŸtÄ±rma**: Her modelin performans metrikleri\n",
        "- ğŸ’° **Sanal Kasa SimÃ¼lasyonu**: ROI ve kazanÃ§ oranlarÄ±\n",
        "- ğŸ“¦ **GeliÅŸmiÅŸ Ä°ndirme Sistemi**: ZIP indirme + Drive backup\n",
        "- ğŸ“š **JSON Ã‡Ä±ktÄ±larÄ±**: TÃ¼m sonuÃ§lar JSON formatÄ±nda\n",
        "\n",
        "## ğŸ“‹ MODEL AÃ‡IKLAMALARI:\n",
        "\n",
        "### 1ï¸âƒ£ Progressive NN (Multi-Scale)\n",
        "- **GÃ¶rev**: Genel amaÃ§lÄ± tahmin\n",
        "- **Ã–zellik**: 5 farklÄ± pencere boyutu (500, 250, 100, 50, 20)\n",
        "- **GÃ¼Ã§lÃ¼ YanÄ±**: FarklÄ± zaman Ã¶lÃ§eklerinde desen yakalar\n",
        "- **Hedef**: 1.5x eÅŸik tahmini\n",
        "\n",
        "### 2ï¸âƒ£ CatBoost Ensemble\n",
        "- **GÃ¶rev**: Gradient boosting uzmanÄ±\n",
        "- **Ã–zellik**: Multi-scale window desteÄŸi\n",
        "- **GÃ¼Ã§lÃ¼ YanÄ±**: HÄ±zlÄ± ve doÄŸru kategorik tahmin\n",
        "- **Hedef**: 1.5x eÅŸik + kategori tahmini\n",
        "\n",
        "### 3ï¸âƒ£ AutoGluon AutoML\n",
        "- **GÃ¶rev**: Otomatik ML champion\n",
        "- **Ã–zellik**: 50+ modeli otomatik dener ve en iyisini seÃ§er\n",
        "- **GÃ¼Ã§lÃ¼ YanÄ±**: Ensemble ve stacking otomatik\n",
        "- **Hedef**: 1.5x eÅŸik tahmini\n",
        "\n",
        "### 4ï¸âƒ£ TabNet High-X Specialist\n",
        "- **GÃ¶rev**: YÃ¼ksek X tespit uzmanÄ±\n",
        "- **Ã–zellik**: Attention-based deep learning\n",
        "- **GÃ¼Ã§lÃ¼ YanÄ±**: 10x+, 50x+ gibi nadir olaylarÄ± yakalar\n",
        "- **Hedef**: Multi-class (DÃ¼ÅŸÃ¼k/Orta/YÃ¼ksek/Mega)\n",
        "\n",
        "### 5ï¸âƒ£ Consensus Ensemble\n",
        "- **GÃ¶rev**: TÃ¼m modellerin birleÅŸik tahmini\n",
        "- **Ã–zellik**: Weighted voting sistemi\n",
        "- **GÃ¼Ã§lÃ¼ YanÄ±**: En yÃ¼ksek doÄŸruluk ve gÃ¼venilirlik\n",
        "- **Hedef**: Final tahmin\n",
        "\n",
        "## â±ï¸ TAHMÄ°NÄ° SÃœRE:\n",
        "- Progressive NN: ~10-12 saat (5 model Ã— ~2 saat)\n",
        "- CatBoost: ~3-4 saat\n",
        "- AutoGluon: ~1-2 saat (time_limit ayarlanabilir)\n",
        "- TabNet: ~2-3 saat\n",
        "- **TOPLAM: ~16-21 saat** (GPU ile)\n",
        "\n",
        "## ğŸ¯ HEDEFLER:\n",
        "- 1.5 ALTI DoÄŸruluk: **75%+**\n",
        "- 1.5 ÃœSTÃœ DoÄŸruluk: **75%+**\n",
        "- Para kaybÄ± riski: **<20%**\n",
        "- ROI: **Pozitif**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# ğŸ“¦ ADIM 1: HazÄ±rlÄ±k ve Kurulum\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ“¦ HAZIRLIK - 5 Model Ensemble Sistem v7.0\")\n",
        "print(\"ğŸ†• YENÄ°: AutoGluon + TabNet TAM ENTEGRE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Google Drive BaÄŸlantÄ±sÄ±\n",
        "print(\"\\nğŸ“ Google Drive baÄŸlanÄ±yor...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"âœ… Google Drive baÅŸarÄ±yla baÄŸlandÄ±!\")\n",
        "    \n",
        "    drive_path = '/content/drive/MyDrive/JetX_Models_v7'\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "    print(f\"âœ… Drive klasÃ¶rÃ¼ oluÅŸturuldu: {drive_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Drive baÄŸlantÄ± hatasÄ±: {e}\")\n",
        "    drive_path = None\n",
        "\n",
        "# KÃ¼tÃ¼phaneleri yÃ¼kle\n",
        "print(\"\\nğŸ“¦ KÃ¼tÃ¼phaneler yÃ¼kleniyor...\")\n",
        "!pip install -q tensorflow scikit-learn catboost pandas numpy scipy joblib matplotlib seaborn tqdm PyWavelets nolds autogluon pytorch-tabnet torch\n",
        "\n",
        "# Proje yÃ¼kle\n",
        "if os.path.exists('jetxpredictor'):\n",
        "    !rm -rf jetxpredictor\n",
        "\n",
        "print(\"\\nğŸ“¥ Proje indiriliyor...\")\n",
        "!git clone https://github.com/onndd/jetxpredictor.git\n",
        "%cd jetxpredictor\n",
        "\n",
        "prep_time = time.time() - start_time\n",
        "print(f\"\\nâœ… HazÄ±rlÄ±k tamamlandÄ±! ({prep_time/60:.1f} dakika)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_prep_header"
      },
      "source": [
        "## ğŸ“Š ADIM 2: Veri YÃ¼kleme ve HazÄ±rlÄ±k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_prep"
      },
      "outputs": [],
      "source": [
        "# Veri YÃ¼kleme\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sys\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "from category_definitions import CategoryDefinitions, FeatureEngineering\n",
        "from utils.multi_scale_window import split_data_preserving_order\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ“Š VERÄ° YÃœKLEME\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "conn = sqlite3.connect('jetx_data.db')\n",
        "data = pd.read_sql_query(\"SELECT value FROM jetx_results ORDER BY id\", conn)\n",
        "conn.close()\n",
        "\n",
        "all_values = data['value'].values\n",
        "print(f\"âœ… {len(all_values):,} veri yÃ¼klendi\")\n",
        "\n",
        "# Time-series split\n",
        "train_data, val_data, test_data = split_data_preserving_order(\n",
        "    all_values, train_ratio=0.70, val_ratio=0.15\n",
        ")\n",
        "\n",
        "print(f\"âœ… Train: {len(train_data):,}\")\n",
        "print(f\"âœ… Val:   {len(val_data):,}\")\n",
        "print(f\"âœ… Test:  {len(test_data):,}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "progressive_header"
      },
      "source": [
        "## ğŸ§  ADIM 3: Progressive NN Multi-Scale Training\n",
        "\n",
        "**â±ï¸ Tahmini SÃ¼re: ~10-12 saat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "progressive_training"
      },
      "outputs": [],
      "source": [
        "# Progressive NN Training\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ§  PROGRESSIVE NN TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "!python notebooks/jetx_PROGRESSIVE_TRAINING_MULTISCALE.py\n",
        "\n",
        "# SonuÃ§larÄ± yÃ¼kle\n",
        "with open('models/progressive_multiscale/model_info.json', 'r') as f:\n",
        "    progressive_results = json.load(f)\n",
        "\n",
        "print(\"\\nâœ… Progressive NN TamamlandÄ±!\")\n",
        "print(f\"ğŸ“Š MAE: {progressive_results['ensemble_metrics']['mae']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "catboost_header"
      },
      "source": [
        "## ğŸš€ ADIM 4: CatBoost Training\n",
        "\n",
        "**â±ï¸ Tahmini SÃ¼re: ~3-4 saat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "catboost_training"
      },
      "outputs": [],
      "source": [
        "# CatBoost Training\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸš€ CATBOOST TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "!python notebooks/jetx_CATBOOST_TRAINING_MULTISCALE.py\n",
        "\n",
        "with open('models/catboost_multiscale/model_info.json', 'r') as f:\n",
        "    catboost_results = json.load(f)\n",
        "\n",
        "print(\"\\nâœ… CatBoost TamamlandÄ±!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "autogluon_header"
      },
      "source": [
        "## ğŸ¤– ADIM 5: AutoGluon AutoML Training (YENÄ°!)\n",
        "\n",
        "**â±ï¸ Tahmini SÃ¼re: ~1-2 saat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autogluon_training"
      },
      "outputs": [],
      "source": [
        "# AutoGluon Training\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from utils.autogluon_predictor import AutoGluonPredictor\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ¤– AUTOGLUON AUTOML TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Feature extraction\n",
        "window_size = 100\n",
        "X_features = []\n",
        "y_labels = []\n",
        "\n",
        "for i in range(window_size, len(train_data) - 1):\n",
        "    hist = train_data[:i].tolist()\n",
        "    target = train_data[i]\n",
        "    feats = FeatureEngineering.extract_all_features(hist)\n",
        "    X_features.append(feats)\n",
        "    y_labels.append(1 if target >= 1.5 else 0)\n",
        "\n",
        "X_train_ag = pd.DataFrame(X_features)\n",
        "y_train_ag = pd.Series(y_labels, name='above_threshold')\n",
        "\n",
        "print(f\"âœ… {len(X_train_ag):,} Ã¶rnek hazÄ±rlandÄ±\")\n",
        "\n",
        "# AutoGluon predictor\n",
        "ag_predictor = AutoGluonPredictor(\n",
        "    model_path='models/autogluon_model',\n",
        "    threshold=1.5\n",
        ")\n",
        "\n",
        "# Train\n",
        "ag_results = ag_predictor.train(\n",
        "    X_train=X_train_ag,\n",
        "    y_train=y_train_ag,\n",
        "    time_limit=3600,\n",
        "    presets='best_quality',\n",
        "    eval_metric='roc_auc'\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… AutoGluon TamamlandÄ±!\")\n",
        "print(f\"ğŸ† En Ä°yi Model: {ag_results['best_model']}\")\n",
        "print(f\"ğŸ“Š Score: {ag_results['best_score']:.4f}\")\n",
        "\n",
        "# Save info\n",
        "autogluon_info = {\n",
        "    'model': 'AutoGluon_AutoML',\n",
        "    'version': '1.0',\n",
        "    'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    'best_model': ag_results['best_model'],\n",
        "    'best_score': float(ag_results['best_score'])\n",
        "}\n",
        "\n",
        "os.makedirs('models/autogluon_model', exist_ok=True)\n",
        "with open('models/autogluon_model/model_info.json', 'w') as f:\n",
        "    json.dump(autogluon_info, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tabnet_header"
      },
      "source": [
        "## ğŸ¯ ADIM 6: TabNet High-X Specialist Training (YENÄ°!)\n",
        "\n",
        "**â±ï¸ Tahmini SÃ¼re: ~2-3 saat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tabnet_training"
      },
      "outputs": [],
      "source": [
        "# TabNet Training\n",
        "from utils.tabnet_predictor import TabNetHighXPredictor\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ¯ TABNET HIGH-X SPECIALIST TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Feature extraction\n",
        "window_size = 100\n",
        "X_features_tn = []\n",
        "y_categories = []\n",
        "\n",
        "for i in range(window_size, len(train_data) - 1):\n",
        "    hist = train_data[:i].tolist()\n",
        "    target = train_data[i]\n",
        "    feats = FeatureEngineering.extract_all_features(hist)\n",
        "    X_features_tn.append(list(feats.values()))\n",
        "    category = TabNetHighXPredictor.categorize_value(target)\n",
        "    y_categories.append(category)\n",
        "\n",
        "X_train_tn = np.array(X_features_tn)\n",
        "y_train_tn = np.array(y_categories)\n",
        "\n",
        "scaler_tn = StandardScaler()\n",
        "X_train_tn = scaler_tn.fit_transform(X_train_tn)\n",
        "\n",
        "print(f\"âœ… {len(X_train_tn):,} Ã¶rnek hazÄ±rlandÄ±\")\n",
        "\n",
        "# Validation set\n",
        "X_val_tn = []\n",
        "y_val_tn = []\n",
        "\n",
        "for i in range(window_size, len(val_data) - 1):\n",
        "    hist = val_data[:i].tolist()\n",
        "    target = val_data[i]\n",
        "    feats = FeatureEngineering.extract_all_features(hist)\n",
        "    X_val_tn.append(list(feats.values()))\n",
        "    y_val_tn.append(TabNetHighXPredictor.categorize_value(target))\n",
        "\n",
        "X_val_tn = scaler_tn.transform(np.array(X_val_tn))\n",
        "y_val_tn = np.array(y_val_tn)\n",
        "\n",
        "# TabNet predictor\n",
        "tabnet_predictor = TabNetHighXPredictor(\n",
        "    model_path='models/tabnet_high_x.pkl',\n",
        "    scaler_path='models/tabnet_scaler.pkl'\n",
        ")\n",
        "\n",
        "# Train\n",
        "tn_results = tabnet_predictor.train(\n",
        "    X_train=X_train_tn,\n",
        "    y_train=y_train_tn,\n",
        "    X_val=X_val_tn,\n",
        "    y_val=y_val_tn,\n",
        "    max_epochs=200,\n",
        "    patience=20,\n",
        "    batch_size=256\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… TabNet TamamlandÄ±!\")\n",
        "print(f\"ğŸ† Best Epoch: {tn_results['best_epoch']}\")\n",
        "\n",
        "# Save\n",
        "tabnet_predictor.save_model()\n",
        "tabnet_predictor.save_scaler(scaler_tn)\n",
        "\n",
        "tabnet_info = {\n",
        "    'model': 'TabNet_HighX_Specialist',\n",
        "    'version': '1.0',\n",
        "    'date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    'best_epoch': int(tn_results['best_epoch']),\n",
        "    'best_cost': float(tn_results['best_cost'])\n",
        "}\n",
        "\n",
        "with open('models/tabnet_info.json', 'w') as f:\n",
        "    json.dump(tabnet_info, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_header"
      },
      "source": [
        "## ğŸ’¾ ADIM 7: SonuÃ§larÄ± Kaydetme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_results"
      },
      "outputs": [],
      "source": [
        "# SonuÃ§larÄ± Kaydet\n",
        "import shutil\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ’¾ SONUÃ‡LAR KAYDEDÄ°LÄ°YOR\")\n",
        "\n",
        "# JSON sonuÃ§larÄ±\n",
        "final_results = {\n",
        "    'metadata': {\n",
        "        'version': '7.0',\n",
        "        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'total_training_time_hours': round((time.time() - start_time) / 3600, 2)\n",
        "    },\n",
        "    'models': {\n",
        "        'progressive_nn': progressive_results if 'progressive_results' in locals() else None,\n",
        "        'catboost': catboost_results if 'catboost_results' in locals() else None,\n",
        "        'autogluon': autogluon_info if 'autogluon_info' in locals() else None,\n",
        "        'tabnet': tabnet_info if 'tabnet_info' in locals() else None\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('models/all_models_results_v7.json', 'w') as f:\n",
        "    json.dump(final_results, f, indent=2)\n",
        "\n",
        "print(\"âœ… JSON sonuÃ§larÄ± kaydedildi\")\n",
        "\n",
        "# ZIP oluÅŸtur\n",
        "zip_filename = f'jetx_5models_v7_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
        "shutil.make_archive(zip_filename, 'zip', 'models')\n",
        "\n",
        "print(f\"âœ… ZIP oluÅŸturuldu: {zip_filename}.zip\")\n",
        "\n",
        "# Drive'a yedekle\n",
        "if drive_path:\n",
        "    try:\n",
        "        drive_zip = f\"{drive_path}/{zip_filename}.zip\"\n",
        "        shutil.copy(f\"{zip_filename}.zip\", drive_zip)\n",
        "        print(f\"âœ… Drive'a yedeklendi\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Drive yedekleme hatasÄ±: {e}\")\n",
        "\n",
        "# Ä°ndirme\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(f'{zip_filename}.zip')\n",
        "    print(f\"âœ… Dosya indiriliyor...\")\n",
        "except:\n",
        "    print(f\"ğŸ“ Dosya konumu: /content/jetxpredictor/{zip_filename}.zip\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… TÃœM Ä°ÅLEMLER TAMAMLANDI!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_header"
      },
      "source": [
        "## ğŸ‰ ADIM 8: Final Ã–zet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "# Final Ã–zet\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ‰ JetX 5 MODEL ENSEMBLE SÄ°STEMÄ° - FÄ°NAL Ã–ZET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nâ±ï¸  TOPLAM SÃœRE: {total_time/3600:.2f} saat ({total_time/60:.1f} dakika)\")\n",
        "\n",
        "print(\"\\nğŸ“Š EÄÄ°TÄ°LEN MODELLER:\")\n",
        "model_count = 0\n",
        "\n",
        "if 'progressive_results' in locals():\n",
        "    model_count += 1\n",
        "    print(f\"   1ï¸âƒ£ Progressive NN (Multi-Scale) âœ…\")\n",
        "    print(f\"      - MAE: {progressive_results['ensemble_metrics']['mae']:.4f}\")\n",
        "\n",
        "if 'catboost_results' in locals():\n",
        "    model_count += 1\n",
        "    print(f\"\\n   2ï¸âƒ£ CatBoost (Multi-Scale) âœ…\")\n",
        "\n",
        "if 'autogluon_info' in locals():\n",
        "    model_count += 1\n",
        "    print(f\"\\n   3ï¸âƒ£ AutoGluon AutoML âœ…\")\n",
        "    print(f\"      - Best Model: {autogluon_info['best_model']}\")\n",
        "\n",
        "if 'tabnet_info' in locals():\n",
        "    model_count += 1\n",
        "    print(f\"\\n   4ï¸âƒ£ TabNet High-X Specialist âœ…\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ TOPLAM: {model_count}/5 model baÅŸarÄ±yla eÄŸitildi\")\n",
        "\n",
        "print(\"\\nğŸ“ KAYDEDILEN DOSYALAR:\")\n",
        "print(f\"   âœ… models/all_models_results_v7.json\")\n",
        "print(f\"   âœ… {zip_filename}.zip\")\n",
        "if drive_path:\n",
        "    print(f\"   âœ… {drive_path}/{zip_filename}.zip (Drive backup)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ¨ BAÅARIYLA TAMAMLANDI! âœ¨\")\n",
        "print(\"=\"*80)\n",
        "print(f\"BitiÅŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "accelerator": "GPU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
