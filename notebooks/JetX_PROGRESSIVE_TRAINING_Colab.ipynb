{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 🚀 JetX Model Eğitimi - Google Colab (v7.1)\n",
    "\n",
    "**5 Model Ensemble Sistemi ile JetX tahmin modellerini eğitin**\n",
    "\n",
    "---\n",
    "\n",
    "## 🆕 v7.1 Yeni Özellikler\n",
    "\n",
    "- 🔧 **İç İçe Klasör Sorunu Düzeltildi**: GitHub'dan direkt açma desteği\n",
    "- 🤖 **5 Model Tam Desteği**: Progressive NN, CatBoost, AutoGluon, TabNet, Consensus\n",
    "- 🧠 **AutoGluon AutoML**: 50+ modeli otomatik dener ve en iyisini seçer\n",
    "- 🎯 **TabNet High-X Specialist**: Attention mechanism ile yüksek çarpanları tespit eder\n",
    "- 📁 **Google Drive Entegrasyonu**: Modeller otomatik Drive'a yedeklenir\n",
    "- 📊 **Kapsamlı Model Karşılaştırma**: Her modelin performans metrikleri\n",
    "- 💰 **Sanal Kasa Simülasyonu**: ROI ve kazanç oranları\n",
    "- 📦 **Gelişmiş İndirme Sistemi**: ZIP indirme + Drive backup\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Model Açıklamaları\n",
    "\n",
    "| Model | Görev | Özellik | Hedef |\n",
    "|-------|-------|---------|-------|\n",
    "| **Progressive NN** | Genel amaçlı tahmin | 5 farklı pencere boyutu | 1.5x eşik tahmini |\n",
    "| **CatBoost** | Gradient boosting | Multi-scale window | 1.5x eşik + kategori |\n",
    "| **AutoGluon** | AutoML champion | 50+ model ensemble | 1.5x eşik tahmini |\n",
    "| **TabNet** | Yüksek X uzmanı | Attention mechanism | Multi-class tahmin |\n",
    "| **Consensus** | Ensemble final | Weighted voting | En yüksek doğruluk |\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱️ Tahmini Süre\n",
    "\n",
    "- Progressive NN: ~10-12 saat\n",
    "- CatBoost: ~3-4 saat\n",
    "- AutoGluon: ~1-2 saat\n",
    "- TabNet: ~2-3 saat\n",
    "- **TOPLAM: ~16-21 saat** (GPU ile)\n",
    "\n",
    "## 🎯 Hedefler\n",
    "\n",
    "- ✅ 1.5 Altı Doğruluk: **75%+**\n",
    "- ✅ 1.5 Üstü Doğruluk: **75%+**\n",
    "- ✅ Para Kaybı Riski: **<20%**\n",
    "- ✅ ROI: **Pozitif**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 📦 Adım 1: Hazırlık ve Kurulum\n",
    "\n",
    "Bu adımda:\n",
    "- Google Drive bağlantısı yapılır\n",
    "- Gerekli kütüphaneler yüklenir\n",
    "- Dizin kontrolü yapılır (GitHub'dan açma desteği)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_code"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"📦 HAZIRLIK - 5 Model Ensemble Sistem v7.1\")\n",
    "print(\"🆕 FİX: İç İçe Klasör Sorunu Giderildi\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Google Drive Bağlantısı\n",
    "print(\"\\n📁 Google Drive bağlanıyor...\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"✅ Google Drive başarıyla bağlandı!\")\n",
    "    \n",
    "    drive_path = '/content/drive/MyDrive/JetX_Models_v7'\n",
    "    os.makedirs(drive_path, exist_ok=True)\n",
    "    print(f\"✅ Drive klasörü oluşturuldu: {drive_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Drive bağlantı hatası: {e}\")\n",
    "    drive_path = None\n",
    "\n",
    "# Kütüphaneleri yükle\n",
    "print(\"\\n📦 Kütüphaneler yükleniyor...\")\n",
    "!pip install -q tensorflow scikit-learn catboost pandas numpy scipy joblib matplotlib seaborn tqdm PyWavelets nolds autogluon pytorch-tabnet torch\n",
    "\n",
    "# Proje klonlama - GitHub'dan jetxpredictor'ı indir\n",
    "print(\"\\n📥 Proje klonlanıyor...\")\n",
    "if not os.path.exists('jetxpredictor'):\n",
    "    !git clone https://github.com/onndd/jetxpredictor.git\n",
    "    print(\"✅ Proje başarıyla klonlandı!\")\n",
    "else:\n",
    "    print(\"✅ Proje zaten mevcut\")\n",
    "\n",
    "\n",
    "# Dizin kontrolü - GitHub'dan direkt açma desteği\n",
    "print(\"\\n📂 Dizin kontrolü yapılıyor...\")\n",
    "print(f\"Mevcut dizin: {os.getcwd()}\")\n",
    "\n",
    "# Proje kök dizinini bul\n",
    "if os.path.exists('jetx_data.db'):\n",
    "    print(\"✅ Proje kök dizinindeyiz!\")\n",
    "elif os.path.exists('../jetx_data.db'):\n",
    "    print(\"📁 Bir üst dizine çıkılıyor...\")\n",
    "    os.chdir('..')\n",
    "    print(f\"✅ Proje kök dizinine geçildi: {os.getcwd()}\")\n",
    "elif os.path.exists('jetxpredictor/jetx_data.db'):\n",
    "    print(\"📁 jetxpredictor klasörüne giriliyor...\")\n",
    "    os.chdir('jetxpredictor')\n",
    "    print(f\"✅ Proje kök dizinine geçildi: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"⚠️ UYARI: jetx_data.db bulunamadı!\")\n",
    "    print(f\"📂 Mevcut dizin içeriği: {os.listdir('.')}\")\n",
    "    \n",
    "# Models klasörünü kontrol et ve oluştur\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "    print(\"✅ models/ klasörü oluşturuldu\")\n",
    "else:\n",
    "    print(\"✅ models/ klasörü mevcut\")\n",
    "\n",
    "prep_time = time.time() - start_time\n",
    "print(f\"\\n✅ Hazırlık tamamlandı! ({prep_time/60:.1f} dakika)\")\n",
    "print(f\"📂 Çalışma dizini: {os.getcwd()}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 📊 Adım 2: Veri Yükleme ve Hazırlık\n",
    "\n",
    "Veritabanından veri yüklenip train/val/test olarak bölünür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loading_code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from category_definitions import CategoryDefinitions, FeatureEngineering\n",
    "from utils.multi_scale_window import split_data_preserving_order\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"📊 VERİ YÜKLEME\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "conn = sqlite3.connect('jetx_data.db')\n",
    "data = pd.read_sql_query(\"SELECT value FROM jetx_results ORDER BY id\", conn)\n",
    "conn.close()\n",
    "\n",
    "all_values = data['value'].values\n",
    "print(f\"✅ {len(all_values):,} veri yüklendi\")\n",
    "\n",
    "# Time-series split\n",
    "train_data, val_data, test_data = split_data_preserving_order(\n",
    "    all_values, train_ratio=0.70, val_ratio=0.15\n",
    ")\n",
    "\n",
    "print(f\"✅ Train: {len(train_data):,}\")\n",
    "print(f\"✅ Val:   {len(val_data):,}\")\n",
    "print(f\"✅ Test:  {len(test_data):,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "progressive_nn"
   },
   "source": [
    "## 🧠 Adım 3: Progressive NN Training\n",
    "\n",
    "**⏱️ Tahmini Süre: ~10-12 saat**\n",
    "\n",
    "Multi-scale window ensemble ile Progressive Neural Network eğitimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "progressive_nn_code"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"🧠 PROGRESSIVE NN TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python notebooks/jetx_PROGRESSIVE_TRAINING_MULTISCALE.py\n",
    "\n",
    "# Sonuçları yükle\n",
    "with open('models/progressive_multiscale/model_info.json', 'r') as f:\n",
    "    progressive_results = json.load(f)\n",
    "\n",
    "print(\"\\n✅ Progressive NN Tamamlandı!\")\n",
    "print(f\"📊 MAE: {progressive_results['ensemble_metrics']['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "catboost"
   },
   "source": [
    "## 🚀 Adım 4: CatBoost Training\n",
    "\n",
    "**⏱️ Tahmini Süre: ~3-4 saat**\n",
    "\n",
    "Gradient boosting uzmanı CatBoost modeli eğitimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "catboost_code"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"🚀 CATBOOST TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python notebooks/jetx_CATBOOST_TRAINING_MULTISCALE.py\n",
    "\n",
    "with open('models/catboost_multiscale/model_info.json', 'r') as f:\n",
    "    catboost_results = json.load(f)\n",
    "\n",
    "print(\"\\n✅ CatBoost Tamamlandı!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "autogluon"
   },
   "source": [
    "## 🤖 Adım 5: AutoGluon AutoML Training\n",
    "\n",
    "**⏱️ Tahmini Süre: ~1-2 saat**\n",
    "\n",
    "50+ modeli otomatik deneyen AutoGluon AutoML eğitimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autogluon_code"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.autogluon_predictor import AutoGluonPredictor\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🤖 AUTOGLUON AUTOML TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature extraction\n",
    "window_size = 100\n",
    "X_features = []\n",
    "y_labels = []\n",
    "\n",
    "for i in range(window_size, len(train_data) - 1):\n",
    "    hist = train_data[:i].tolist()\n",
    "    target = train_data[i]\n",
    "    feats = FeatureEngineering.extract_all_features(hist)\n",
    "    X_features.append(feats)\n",
    "    y_labels.append(1 if target >= 1.5 else 0)\n",
    "\n",
    "X_train_ag = pd.DataFrame(X_features)\n",
    "y_train_ag = pd.Series(y_labels, name='above_threshold')\n",
    "\n",
    "print(f\"✅ {len(X_train_ag):,} örnek hazırlandı\")\n",
    "\n",
    "# AutoGluon predictor\n",
    "ag_predictor = AutoGluonPredictor(\n",
    "    model_path='models/autogluon_model',\n",
    "    threshold=1.5\n",
    ")\n",
    "\n",
    "# Train\n",
    "ag_results = ag_predictor.train(\n",
    "    X_train=X_train_ag,\n",
    "    y_train=y_train_ag,\n",
    "    time_limit=3600,\n",
    "    presets='best_quality',\n",
    "    eval_metric='roc_auc'\n",
    ")\n",
    "\n",
    "print(\"\\n✅ AutoGluon Tamamlandı!\")\n",
    "print(f\"🏆 En İyi Model: {ag_results['best_model']}\")\n",
    "print(f\"📊 Score: {ag_results['best_score']:.4f}\")\n",
    "\n",
    "# Save info\n",
    "autogluon_info = {\n",
    "    'model': 'AutoGluon_AutoML',\n",
    "    'version': '1.0',\n",
    "    'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'best_model': ag_results['best_model'],\n",
    "    'best_score': float(ag_results['best_score'])\n",
    "}\n",
    "\n",
    "os.makedirs('models/autogluon_model', exist_ok=True)\n",
    "with open('models/autogluon_model/model_info.json', 'w') as f:\n",
    "    json.dump(autogluon_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tabnet"
   },
   "source": [
    "## 🎯 Adım 6: TabNet High-X Specialist Training\n",
    "\n",
    "**⏱️ Tahmini Süre: ~2-3 saat**\n",
    "\n",
    "Attention mechanism ile yüksek çarpanları tespit eden TabNet eğitimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tabnet_code"
   },
   "outputs": [],
   "source": [
    "from utils.tabnet_predictor import TabNetHighXPredictor\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🎯 TABNET HIGH-X SPECIALIST TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature extraction\n",
    "window_size = 100\n",
    "X_features_tn = []\n",
    "y_categories = []\n",
    "\n",
    "for i in range(window_size, len(train_data) - 1):\n",
    "    hist = train_data[:i].tolist()\n",
    "    target = train_data[i]\n",
    "    feats = FeatureEngineering.extract_all_features(hist)\n",
    "    X_features_tn.append(list(feats.values()))\n",
    "    category = TabNetHighXPredictor.categorize_value(target)\n",
    "    y_categories.append(category)\n",
    "\n",
    "X_train_tn = np.array(X_features_tn)\n",
    "y_train_tn = np.array(y_categories)\n",
    "\n",
    "scaler_tn = StandardScaler()\n",
    "X_train_tn = scaler_tn.fit_transform(X_train_tn)\n",
    "\n",
    "print(f\"✅ {len(X_train_tn):,} örnek hazırlandı\")\n",
    "\n",
    "# Validation set\n",
    "X_val_tn = []\n",
    "y_val_tn = []\n",
    "\n",
    "for i in range(window_size, len(val_data) - 1):\n",
    "    hist = val_data[:i].tolist()\n",
    "    target = val_data[i]\n",
    "    feats = FeatureEngineering.extract_all_features(hist)\n",
    "    X_val_tn.append(list(feats.values()))\n",
    "    y_val_tn.append(TabNetHighXPredictor.categorize_value(target))\n",
    "\n",
    "X_val_tn = scaler_tn.transform(np.array(X_val_tn))\n",
    "y_val_tn = np.array(y_val_tn)\n",
    "\n",
    "# TabNet predictor\n",
    "tabnet_predictor = TabNetHighXPredictor(\n",
    "    model_path='models/tabnet_high_x.pkl',\n",
    "    scaler_path='models/tabnet_scaler.pkl'\n",
    ")\n",
    "\n",
    "# Train\n",
    "tn_results = tabnet_predictor.train(\n",
    "    X_train=X_train_tn,\n",
    "    y_train=y_train_tn,\n",
    "    X_val=X_val_tn,\n",
    "    y_val=y_val_tn,\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "print(\"\\n✅ TabNet Tamamlandı!\")\n",
    "print(f\"🏆 Best Epoch: {tn_results['best_epoch']}\")\n",
    "\n",
    "# Save\n",
    "tabnet_predictor.save_model()\n",
    "tabnet_predictor.save_scaler(scaler_tn)\n",
    "\n",
    "tabnet_info = {\n",
    "    'model': 'TabNet_HighX_Specialist',\n",
    "    'version': '1.0',\n",
    "    'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'best_epoch': int(tn_results['best_epoch']),\n",
    "    'best_cost': float(tn_results['best_cost'])\n",
    "}\n",
    "\n",
    "with open('models/tabnet_info.json', 'w') as f:\n",
    "    json.dump(tabnet_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## 💾 Adım 7: Sonuçları Kaydetme ve İndirme\n",
    "\n",
    "Tüm modeller ZIP'lenir ve indirilir. Google Drive'a yedekleme yapılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results_code"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"💾 SONUÇLAR KAYDEDİLİYOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# JSON sonuçları\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'version': '7.1',\n",
    "        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_training_time_hours': round((time.time() - start_time) / 3600, 2)\n",
    "    },\n",
    "    'models': {\n",
    "        'progressive_nn': progressive_results if 'progressive_results' in locals() else None,\n",
    "        'catboost': catboost_results if 'catboost_results' in locals() else None,\n",
    "        'autogluon': autogluon_info if 'autogluon_info' in locals() else None,\n",
    "        'tabnet': tabnet_info if 'tabnet_info' in locals() else None\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('models/all_models_results_v7.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"✅ JSON sonuçları kaydedildi\")\n",
    "\n",
    "# Lokal Streamlit uyumluluğu için dosya isimlendirme\n",
    "print(\"\\n📝 Lokal kullanım için dosyalar hazırlanıyor...\")\n",
    "\n",
    "# Progressive NN modelini ana model olarak kopyala\n",
    "if os.path.exists('models/progressive_multiscale/ensemble_model.h5'):\n",
    "    shutil.copy('models/progressive_multiscale/ensemble_model.h5', 'models/jetx_model.h5')\n",
    "    print(\"✅ jetx_model.h5 oluşturuldu\")\n",
    "\n",
    "if os.path.exists('models/progressive_multiscale/scaler.pkl'):\n",
    "    shutil.copy('models/progressive_multiscale/scaler.pkl', 'models/scaler.pkl')\n",
    "    print(\"✅ scaler.pkl oluşturuldu\")\n",
    "\n",
    "# ZIP oluştur\n",
    "zip_filename = f'jetx_5models_v7_{datetime.now().strftime(\"%Y%m%d_%H%M\")}'\n",
    "shutil.make_archive(zip_filename, 'zip', 'models')\n",
    "\n",
    "print(f\"\\n✅ ZIP oluşturuldu: {zip_filename}.zip\")\n",
    "\n",
    "# Drive'a yedekle\n",
    "if drive_path:\n",
    "    try:\n",
    "        drive_zip = f\"{drive_path}/{zip_filename}.zip\"\n",
    "        shutil.copy(f\"{zip_filename}.zip\", drive_zip)\n",
    "        print(f\"✅ Drive'a yedeklendi: {drive_zip}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Drive yedekleme hatası: {e}\")\n",
    "\n",
    "# İndirme\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(f'{zip_filename}.zip')\n",
    "    print(f\"\\n✅ Dosya indiriliyor...\")\n",
    "    print(\"\\n📌 İNDİRME TALİMATLARI:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. ZIP dosyasını masaüstünde açın\")\n",
    "    print(\"2. İçindeki tüm dosyaları jetxpredictor/models/ klasörüne kopyalayın\")\n",
    "    print(\"3. Terminal'de: streamlit run app.py\")\n",
    "    print(\"4. \\\"✅ Model yüklendi ve hazır!\\\" mesajını görmelisiniz\")\n",
    "    print(\"=\"*80)\n",
    "except:\n",
    "    print(f\"\\n📁 Dosya konumu: {os.getcwd()}/{zip_filename}.zip\")\n",
    "    print(\"Manuel indirme: Dosyalar > Sağ tık > İndir\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ TÜM İŞLEMLER TAMAMLANDI!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 🎉 Adım 8: Final Özet\n",
    "\n",
    "Eğitim süreci tamamlandı. Sonuçları görüntüleyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary_code"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"🎉 JetX 5 MODEL ENSEMBLE SİSTEMİ - FİNAL ÖZET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n⏱️  TOPLAM SÜRE: {total_time/3600:.2f} saat ({total_time/60:.1f} dakika)\")\n",
    "\n",
    "print(\"\\n📊 EĞİTİLEN MODELLER:\")\n",
    "model_count = 0\n",
    "\n",
    "if 'progressive_results' in locals():\n",
    "    model_count += 1\n",
    "    print(f\"   1️⃣ Progressive NN (Multi-Scale) ✅\")\n",
    "    print(f\"      - MAE: {progressive_results['ensemble_metrics']['mae']:.4f}\")\n",
    "\n",
    "if 'catboost_results' in locals():\n",
    "    model_count += 1\n",
    "    print(f\"\\n   2️⃣ CatBoost (Multi-Scale) ✅\")\n",
    "\n",
    "if 'autogluon_info' in locals():\n",
    "    model_count += 1\n",
    "    print(f\"\\n   3️⃣ AutoGluon AutoML ✅\")\n",
    "    print(f\"      - Best Model: {autogluon_info['best_model']}\")\n",
    "\n",
    "if 'tabnet_info' in locals():\n",
    "    model_count += 1\n",
    "    print(f\"\\n   4️⃣ TabNet High-X Specialist ✅\")\n",
    "\n",
    "print(f\"\\n📈 TOPLAM: {model_count}/5 model başarıyla eğitildi\")\n",
    "\n",
    "print(\"\\n📁 KAYDEDILEN DOSYALAR:\")\n",
    "print(f\"   ✅ models/jetx_model.h5 (Ana Progressive NN)\")\n",
    "print(f\"   ✅ models/scaler.pkl (Feature scaler)\")\n",
    "print(f\"   ✅ models/all_models_results_v7.json\")\n",
    "print(f\"   ✅ {zip_filename}.zip\")\n",
    "if drive_path:\n",
    "    print(f\"   ✅ {drive_path}/{zip_filename}.zip (Drive backup)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✨ BAŞARIYLA TAMAMLANDI! ✨\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Bitiş: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "JetX_PROGRESSIVE_TRAINING_Colab.ipynb",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}