#!/usr/bin/env python3
"""
ğŸ¯ JetX PROGRESSIVE TRAINING - MULTI-SCALE WINDOW ENSEMBLE (v3.1)

YENÄ° YAKLAÅIM: Multi-Scale Window Ensemble
- Her pencere boyutu iÃ§in ayrÄ± model eÄŸitimi
- Window boyutlarÄ±: [500, 250, 100, 50, 20]
- Her model farklÄ± zaman Ã¶lÃ§eÄŸinde desen Ã¶ÄŸrenir
- Final: TÃ¼m modellerin ensemble'Ä±

GÃœNCELLEME (v3.1):
- 2 MODLU YAPI: Normal (0.85) ve Rolling (0.95)
- Sanal kasalar bu modlara gÃ¶re optimize edildi.

HEDEFLER:
- Normal Mod DoÄŸruluk: %80+
- Rolling Mod DoÄŸruluk: %90+
- MAE: < 2.0

âš ï¸  VERÄ° BÃœTÃœNLÄ°ÄÄ°:
- Shuffle: YASAK
- Augmentation: YASAK
- Kronolojik sÄ±ra: KORUNUYOR
"""

import subprocess
import sys
import os
import time
from datetime import datetime
import json
import shutil
from pathlib import Path
import pickle

# XLA optimizasyonu devre dÄ±ÅŸÄ± (stabilite iÃ§in)
os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

print("="*80)
print("ğŸ¯ JetX PROGRESSIVE TRAINING - MULTI-SCALE WINDOW ENSEMBLE (v3.1)")
print("="*80)
print(f"BaÅŸlangÄ±Ã§: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print()
print("ğŸ”§ SÄ°STEM KONFIGURASYONU:")
print("   Normal Mod EÅŸik: 0.85")
print("   Rolling Mod EÅŸik: 0.95")
print("   Window BoyutlarÄ±: [500, 250, 100, 50, 20]")
print()

# KÃ¼tÃ¼phaneleri yÃ¼kle
print("ğŸ“¦ KÃ¼tÃ¼phaneler yÃ¼kleniyor...")
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", 
                      "tensorflow", "scikit-learn", "pandas", "numpy", 
                      "scipy", "joblib", "matplotlib", "seaborn", "tqdm",
                      "PyWavelets", "nolds"])

import numpy as np
import pandas as pd
import joblib
import sqlite3
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, backend as K
from tensorflow.keras.optimizers import Adam
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# EÅÄ°KLER
THRESHOLD_NORMAL = 0.85
THRESHOLD_ROLLING = 0.95

# =============================================================================
# GPU OPTIMIZER ENTEGRASYONU
# =============================================================================
try:
    # GPU optimizer'Ä± import et
    from utils.gpu_optimizer import setup_colab_gpu_optimization, get_gpu_optimizer
    
    print("\nğŸš€ GPU OPTÄ°MÄ°ZASYONU BAÅLATILIYOR...")
    gpu_results = setup_colab_gpu_optimization()
    
    # GPU optimizer instance
    gpu_optimizer = get_gpu_optimizer()
    
    # GPU monitoring
    print("ğŸ“Š GPU performansÄ± izleniyor...")
    gpu_optimizer.monitor_gpu_usage(duration_seconds=3)
    
except ImportError as e:
    print(f"âš ï¸ GPU optimizer import edilemedi: {e}")
    gpu_optimizer = None
except Exception as e:
    print(f"âš ï¸ GPU optimizasyonu baÅŸarÄ±sÄ±z: {e}")
    gpu_optimizer = None

# GPU konfigÃ¼rasyonu
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        # Memory growth ayarla - GPU belleÄŸini dinamik kullan
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        
        # Mixed precision training - GPU performansÄ±nÄ± artÄ±rÄ±r
        from tensorflow.keras import mixed_precision
        mixed_precision.set_global_policy('mixed_float16')
        
        print(f"âœ… TensorFlow: {tf.__version__}")
        print(f"âœ… GPU: {len(gpus)} GPU bulundu ve yapÄ±landÄ±rÄ±ldÄ±")
        print(f"   - Memory growth: Aktif")
        print(f"   - Mixed precision: Aktif (float16)")
        print(f"   - GPU'lar: {[gpu.name for gpu in gpus]}")
        
        # GPU optimizer entegrasyonu
        if gpu_optimizer:
            try:
                gpu_optimizer.optimize_tensorflow()
            except Exception as e:
                print(f"âš ï¸ TensorFlow GPU optimizasyonu baÅŸarÄ±sÄ±z: {e}")
        
    except RuntimeError as e:
        print(f"âš ï¸ GPU konfigÃ¼rasyon hatasÄ±: {e}")
        print(f"âœ… TensorFlow: {tf.__version__}")
        print(f"âœ… GPU: Mevcut ama CPU modunda Ã§alÄ±ÅŸacak")
else:
    print(f"âœ… TensorFlow: {tf.__version__}")
    print(f"âš ï¸ GPU: BulunamadÄ± - CPU modunda Ã§alÄ±ÅŸacak")
    # CPU fallback iÃ§in gpu optimizer'Ä± hala Ã§aÄŸÄ±rabiliriz
    if gpu_optimizer:
        print("â„¹ï¸ GPU optimizer CPU fallback mekanizmalarÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±yor...")

# Proje yÃ¼kle ve kÃ¶k dizini tespit et
PROJECT_ROOT = None

# Ã–nce mevcut dizini kontrol et
if os.path.exists('jetx_data.db'):
    PROJECT_ROOT = os.getcwd()
    print("\nâœ… Proje kÃ¶k dizini tespit edildi (mevcut dizin)")
elif os.path.exists('jetxpredictor/jetx_data.db'):
    PROJECT_ROOT = os.path.join(os.getcwd(), 'jetxpredictor')
    print(f"\nâœ… Proje kÃ¶k dizini tespit edildi: {PROJECT_ROOT}")
else:
    # Yoksa klonla
    print("\nğŸ“¥ Proje klonlanÄ±yor...")
    subprocess.check_call(["git", "clone", "https://github.com/onndd/jetxpredictor.git"])
    PROJECT_ROOT = os.path.join(os.getcwd(), 'jetxpredictor')
    print(f"âœ… Proje klonlandÄ±: {PROJECT_ROOT}")

# sys.path'e ekle (chdir YAPMA!)
sys.path.insert(0, PROJECT_ROOT)
print(f"ğŸ“‚ Ã‡alÄ±ÅŸma dizini: {os.getcwd()}")
print(f"ğŸ“‚ Proje kÃ¶k dizini: {PROJECT_ROOT}")

from category_definitions import CategoryDefinitions, FeatureEngineering
from utils.multi_scale_window import MultiScaleWindowExtractor, MultiScaleEnsemble, split_data_preserving_order
from utils.custom_losses import percentage_aware_regression_loss, balanced_focal_loss, create_weighted_binary_crossentropy
from utils.adaptive_lr_scheduler import AdaptiveLearningRateScheduler, CosineAnnealingSchedule, LearningRateSchedulerFactory
print(f"âœ… Proje yÃ¼klendi - Kritik eÅŸik: {CategoryDefinitions.CRITICAL_THRESHOLD}x\n")

# =============================================================================
# VERÄ° YÃœKLEME (SIRA KORUNARAK)
# =============================================================================
print("ğŸ“Š Veri yÃ¼kleniyor...")
db_path = os.path.join(PROJECT_ROOT, 'jetx_data.db')
conn = sqlite3.connect(db_path)
data = pd.read_sql_query("SELECT value FROM jetx_results ORDER BY id", conn)
conn.close()

# String verileri float'a Ã§evir - Unicode karakterleri temizle (DÃœZELTME: Index kaymasÄ± Ã¶nlendi)
all_values = data['value'].values

# Unicode karakterlerini ve bozuk verileri temizle - DÃœZELTME: Index korunuyor
cleaned_values = []
skipped_indices = []  # Atlanan indexleri takip et
for i, val in enumerate(all_values):
    try:
        # String'i temizle - Unicode satÄ±r ayÄ±rÄ±cÄ±larÄ±nÄ± ve diÄŸer bozuk karakterleri kaldÄ±r
        val_str = str(val).replace('\u2028', '').replace('\u2029', '').strip()
        # Birden fazla sayÄ± varsa (Ã¶rn: "2.29 1.29") ilkini al
        if ' ' in val_str:
            val_str = val_str.split()[0]
        # Float'a Ã§evir
        cleaned_values.append(float(val_str))
    except (ValueError, TypeError) as e:
        skipped_indices.append(i)  # Index'i kaydet
        print(f"âš ï¸ SatÄ±r {i} atlandÄ± - bozuk veri: '{val}' - Hata: {e}")
        continue

all_values = np.array(cleaned_values)
print(f"âœ… {len(all_values):,} veri yÃ¼klendi", end="")
if len(skipped_indices) > 0:
    print(f" ({len(skipped_indices)} bozuk satÄ±r atlandÄ± - indexler: {skipped_indices[:5]}{'...' if len(skipped_indices) > 5 else ''})")
else:
    print()
print(f"AralÄ±k: {all_values.min():.2f}x - {all_values.max():.2f}x")

below = (all_values < 1.5).sum()
above = (all_values >= 1.5).sum()
print(f"\nğŸ“Š CLASS DAÄILIMI:")
print(f"  1.5 altÄ±: {below:,} ({below/len(all_values)*100:.1f}%)")
print(f"  1.5 Ã¼stÃ¼: {above:,} ({above/len(all_values)*100:.1f}%)")
print(f"  Dengesizlik: 1:{above/below:.2f}")

# =============================================================================
# TIME-SERIES SPLIT (SHUFFLE YOK!)
# =============================================================================
print("\nğŸ“Š TIME-SERIES SPLIT (Kronolojik)...")
train_data, val_data, test_data = split_data_preserving_order(
    all_values,
    train_ratio=0.70,
    val_ratio=0.15
)

# =============================================================================
# MULTI-SCALE FEATURE ENGINEERING
# =============================================================================
print("\nğŸ”§ MULTI-SCALE FEATURE EXTRACTION...")
print("ğŸ”¹ Her pencere boyutu iÃ§in feature engineering")

window_sizes = [500, 250, 100, 50, 20]

def extract_features_for_window(data, window_size, start_idx=None, end_idx=None):
    """
    Belirli bir pencere boyutu iÃ§in feature extraction
    
    Args:
        data: Input veri
        window_size: Pencere boyutu
        start_idx: BaÅŸlangÄ±Ã§ indeksi (None ise window_size'den baÅŸlar)
        end_idx: BitiÅŸ indeksi (None ise veri sonuna kadar)
    """
    X_features = []
    X_sequences = []
    y_regression = []
    y_classification = []
    y_threshold = []
    
    # BaÅŸlangÄ±Ã§ ve bitiÅŸ indekslerini belirle
    if start_idx is None:
        start_idx = window_size
    if end_idx is None:
        end_idx = len(data) - 1
    
    for i in tqdm(range(start_idx, end_idx), desc=f'Window {window_size}'):
        hist = data[:i].tolist()
        target = data[i]
        
        # Feature engineering
        feats = FeatureEngineering.extract_all_features(hist)
        X_features.append(list(feats.values()))
        
        # Sequence (son window_size deÄŸer)
        sequence = data[i-window_size:i]
        X_sequences.append(sequence)
        
        # Targets
        y_regression.append(target)
        
        # Classification (3 sÄ±nÄ±f)
        cat = CategoryDefinitions.get_category_numeric(target)
        onehot = np.zeros(3)
        onehot[cat] = 1
        y_classification.append(onehot)
        
        # Threshold (1.5 altÄ±/Ã¼stÃ¼)
        y_threshold.append(1.0 if target >= 1.5 else 0.0)
    
    X_features = np.array(X_features)
    X_sequences = np.array(X_sequences).reshape(-1, window_size, 1)
    y_regression = np.array(y_regression)
    y_classification = np.array(y_classification)
    y_threshold = np.array(y_threshold).reshape(-1, 1)
    
    return X_features, X_sequences, y_regression, y_classification, y_threshold

# Her window boyutu iÃ§in feature extraction
all_data_by_window = {}

# En bÃ¼yÃ¼k pencere boyutu (500) iÃ§in test baÅŸlangÄ±Ã§ indeksini hesapla
max_window = max(window_sizes)
test_start_idx = max_window  # En bÃ¼yÃ¼k pencere boyutu kadar offset

for window_size in window_sizes:
    print(f"\nğŸ”§ Window {window_size} iÃ§in feature extraction...")
    
    # Train data
    X_f_train, X_seq_train, y_reg_train, y_cls_train, y_thr_train = extract_features_for_window(
        train_data, window_size
    )
    
    # Val data
    X_f_val, X_seq_val, y_reg_val, y_cls_val, y_thr_val = extract_features_for_window(
        val_data, window_size
    )
    
    # Test data - TÃœM MODELLER Ä°Ã‡Ä°N AYNI BAÅLANGIÃ‡ Ä°NDEKSÄ°
    # Bu, ensemble iÃ§in tutarlÄ± tahmin uzunluklarÄ± saÄŸlar
    X_f_test, X_seq_test, y_reg_test, y_cls_test, y_thr_test = extract_features_for_window(
        test_data, window_size, start_idx=test_start_idx
    )
    
    # Normalizasyon
    scaler = StandardScaler()
    X_f_train = scaler.fit_transform(X_f_train)
    X_f_val = scaler.transform(X_f_val)
    X_f_test = scaler.transform(X_f_test)
    
    # Log-scale sequences
    X_seq_train = np.log10(X_seq_train + 1e-8)
    X_seq_val = np.log10(X_seq_val + 1e-8)
    X_seq_test = np.log10(X_seq_test + 1e-8)
    
    all_data_by_window[window_size] = {
        'train': (X_f_train, X_seq_train, y_reg_train, y_cls_train, y_thr_train),
        'val': (X_f_val, X_seq_val, y_reg_val, y_cls_val, y_thr_val),
        'test': (X_f_test, X_seq_test, y_reg_test, y_cls_test, y_thr_test),
        'scaler': scaler
    }
    
    print(f"âœ… Window {window_size}: {len(X_f_train):,} train, {len(X_f_val):,} val, {len(X_f_test):,} test")

# =============================================================================
# MODEL MÄ°MARÄ°SÄ° (HER PENCERE Ä°Ã‡Ä°N AYRI)
# =============================================================================
def build_model_for_window(window_size, n_features):
    """
    Belirli bir pencere boyutu iÃ§in model oluÅŸtur
    Her pencere boyutu kendi modeline sahip
    """
    # Inputs
    inp_features = layers.Input((n_features,), name='features')
    inp_sequence = layers.Input((window_size, 1), name='sequence')
    
    # Feature branch
    x_feat = layers.Dense(256, activation='relu', kernel_regularizer='l2')(inp_features)
    x_feat = layers.BatchNormalization()(x_feat)
    x_feat = layers.Dropout(0.3)(x_feat)
    x_feat = layers.Dense(128, activation='relu')(x_feat)
    x_feat = layers.Dropout(0.2)(x_feat)
    
    # Sequence branch - pencere boyutuna gÃ¶re adapte
    if window_size <= 50:
        # KÃ¼Ã§Ã¼k pencere: Basit LSTM
        x_seq = layers.LSTM(64, return_sequences=False)(inp_sequence)
        x_seq = layers.Dropout(0.2)(x_seq)
    elif window_size <= 100:
        # Orta pencere: 2-layer LSTM
        x_seq = layers.LSTM(128, return_sequences=True)(inp_sequence)
        x_seq = layers.Dropout(0.2)(x_seq)
        x_seq = layers.LSTM(64, return_sequences=False)(x_seq)
        x_seq = layers.Dropout(0.2)(x_seq)
    else:
        # BÃ¼yÃ¼k pencere: 3-layer LSTM + Attention
        x_seq = layers.LSTM(256, return_sequences=True)(inp_sequence)
        x_seq = layers.Dropout(0.2)(x_seq)
        x_seq = layers.LSTM(128, return_sequences=True)(x_seq)
        x_seq = layers.Dropout(0.2)(x_seq)
        
        # Attention - Lambda yerine GlobalAveragePooling1D kullan
        attention = layers.Dense(1, activation='tanh')(x_seq)
        attention = layers.Flatten()(attention)
        attention = layers.Activation('softmax')(attention)
        attention = layers.RepeatVector(128)(attention)
        attention = layers.Permute([2, 1])(attention)
        
        x_seq_attended = layers.Multiply()([x_seq, attention])
        x_seq = layers.GlobalAveragePooling1D()(x_seq_attended)
        x_seq = layers.Dense(128, activation='linear', use_bias=False)(x_seq)
        x_seq = layers.Dropout(0.2)(x_seq)
    
    # Fusion
    fusion = layers.Concatenate()([x_feat, x_seq])
    fusion = layers.Dense(256, activation='relu', kernel_regularizer='l2')(fusion)
    fusion = layers.BatchNormalization()(fusion)
    fusion = layers.Dropout(0.3)(fusion)
    fusion = layers.Dense(128, activation='relu')(fusion)
    fusion = layers.Dropout(0.2)(fusion)
    
    # Outputs
    # Regression
    reg_branch = layers.Dense(64, activation='relu')(fusion)
    reg_branch = layers.Dropout(0.2)(reg_branch)
    out_reg = layers.Dense(1, activation='linear', name='regression')(reg_branch)
    
    # Classification (3 sÄ±nÄ±f)
    cls_branch = layers.Dense(64, activation='relu')(fusion)
    cls_branch = layers.Dropout(0.2)(cls_branch)
    out_cls = layers.Dense(3, activation='softmax', name='classification')(cls_branch)
    
    # Threshold (1.5 altÄ±/Ã¼stÃ¼)
    thr_branch = layers.Dense(32, activation='relu')(fusion)
    thr_branch = layers.Dropout(0.2)(thr_branch)
    out_thr = layers.Dense(1, activation='sigmoid', name='threshold')(thr_branch)
    
    model = models.Model([inp_features, inp_sequence], [out_reg, out_cls, out_thr])
    
    return model

# =============================================================================
# DETAYLI EPOCH CALLBACK (2 MODLU)
# =============================================================================
class DetailedMetricsCallback(callbacks.Callback):
    """
    Her epoch sonunda detaylÄ± metrikler gÃ¶sterir (Normal ve Rolling Mod iÃ§in)
    """
    def __init__(self, X_val, y_val):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
    
    def simulate_bankroll(self, predictions, actuals, threshold):
        """Basit kasa simÃ¼lasyonu"""
        initial = 10000
        wallet = initial
        wins = 0
        total_bets = 0
        for pred, actual in zip(predictions, actuals):
            if pred >= threshold:
                wallet -= 10
                total_bets += 1
                if actual >= 1.5:
                    wallet += 15
                    wins += 1
        roi = ((wallet - initial) / initial) * 100 if total_bets > 0 else 0
        win_rate = (wins / total_bets * 100) if total_bets > 0 else 0
        return roi, win_rate, wins, total_bets
    
    def on_epoch_end(self, epoch, logs=None):
        # Tahminler yap
        preds = self.model.predict(self.X_val, verbose=0)
        threshold_preds = preds[2].flatten()
        
        # GerÃ§ek DeÄŸerler
        y_true = (self.y_val >= 1.5).astype(int)
        
        # NORMAL MOD (0.85) Analizi
        y_pred_normal = (threshold_preds >= THRESHOLD_NORMAL).astype(int)
        acc_normal = accuracy_score(y_true, y_pred_normal) * 100
        
        # ROLLING MOD (0.95) Analizi
        y_pred_rolling = (threshold_preds >= THRESHOLD_ROLLING).astype(int)
        acc_rolling = accuracy_score(y_true, y_pred_rolling) * 100
        
        # ROI Hesapla (Normal Mod Ã¼zerinden)
        roi, win_rate, wins, total_bets = self.simulate_bankroll(threshold_preds, self.y_val, THRESHOLD_NORMAL)
        
        # DetaylÄ± Ã§Ä±ktÄ±
        print(f"\n{'='*80}")
        print(f"ğŸ“Š EPOCH {epoch+1} - PERFORMANS RAPORU")
        print(f"âš–ï¸  Normal Mod ({THRESHOLD_NORMAL}):   {acc_normal:6.2f}%")
        print(f"ğŸš€ Rolling Mod ({THRESHOLD_ROLLING}):   {acc_rolling:6.2f}%")
        print(f"ğŸ’µ ROI (Normal):         {roi:+7.2f}%")
        print(f"ğŸ“ˆ Win Rate (Normal):    {win_rate:6.2f}%  ({wins}/{total_bets})")
        print(f"ğŸ“‰ Loss:                 val_loss={logs.get('val_loss', 0):.4f}")
        print(f"{'='*80}\n")

# =============================================================================
# WEIGHTED MODEL CHECKPOINT CALLBACK (NORMAL MOD ODAKLI)
# =============================================================================
class WeightedModelCheckpoint(callbacks.Callback):
    """
    Modeli kaydederken Normal Mod (0.85) performansÄ±na odaklanÄ±r.
    """
    def __init__(self, filepath, X_val, y_val):
        super().__init__()
        self.filepath = filepath
        self.X_val = X_val
        self.y_val = y_val
        self.best_score = -float('inf')
    
    def normalize_roi(self, roi):
        if roi < 0:
            return max(0, 40 + roi * 0.4) 
        else:
            return min(100, 50 + roi * 0.5)
    
    def on_epoch_end(self, epoch, logs=None):
        preds = self.model.predict(self.X_val, verbose=0)
        threshold_preds = preds[2].flatten()
        
        y_true = (self.y_val >= 1.5).astype(int)
        
        # Normal Mod (0.85) Metrikleri
        y_pred = (threshold_preds >= THRESHOLD_NORMAL).astype(int)
        
        TN = np.sum((y_true == 0) & (y_pred == 0))
        FP = np.sum((y_true == 0) & (y_pred == 1))
        TP = np.sum((y_true == 1) & (y_pred == 1))
        
        precision = (TP / (TP + FP) * 100) if (TP + FP) > 0 else 0
        
        # ROI Hesapla
        initial = 10000
        wallet = initial
        total_bets = 0
        wins = 0
        
        for pred, actual in zip(threshold_preds, self.y_val):
            if pred >= THRESHOLD_NORMAL:
                total_bets += 1
                wallet -= 10
                if actual >= 1.5:
                    wallet += 15
                    wins += 1
        
        roi = ((wallet - initial) / initial) * 100 if total_bets > 0 else 0
        win_rate = (wins / total_bets) * 100 if total_bets > 0 else 0
        
        normalized_roi = self.normalize_roi(roi)
        
        # Skorlama (Normal Mod PerformansÄ±)
        weighted_score = (
            0.50 * normalized_roi +
            0.30 * precision +
            0.20 * win_rate
        )
        
        if weighted_score > self.best_score:
            self.best_score = weighted_score
            self.model.save(self.filepath)
            print(f"\nâœ¨ YENÄ° EN Ä°YÄ° MODEL! (Score: {weighted_score:.2f})")
            print(f"   ROI: {roi:.2f}% | Precision: {precision:.2f}%")

# =============================================================================
# HER PENCERE Ä°Ã‡Ä°N MODEL EÄÄ°TÄ°MÄ°
# =============================================================================
print("\n" + "="*80)
print("ğŸ”¥ MULTI-SCALE MODEL EÄÄ°TÄ°MÄ° BAÅLIYOR")
print("="*80)

trained_models = {}
training_times = {}

for window_size in window_sizes:
    print("\n" + "="*80)
    print(f"ğŸ¯ WINDOW {window_size} - MODEL EÄÄ°TÄ°MÄ°")
    print("="*80)
    
    window_start_time = time.time()
    
    # Veriyi al
    data_dict = all_data_by_window[window_size]
    X_f_tr, X_seq_tr, y_reg_tr, y_cls_tr, y_thr_tr = data_dict['train']
    X_f_val, X_seq_val, y_reg_val, y_cls_val, y_thr_val = data_dict['val']
    
    # Model oluÅŸtur
    model = build_model_for_window(window_size, X_f_tr.shape[1])
    print(f"âœ… Model oluÅŸturuldu: {model.count_params():,} parametre")
    
    # Class weights - DENGELI
    w0, w1 = 1.5, 1.0
    
    # Adaptive Learning Rate Scheduler oluÅŸtur
    adaptive_scheduler = AdaptiveLearningRateScheduler(
        initial_lr=0.001,
        max_lr=0.01,
        min_lr=0.0001,
        patience=5
    )
    
    # Compile
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss={
            'regression': percentage_aware_regression_loss,
            'classification': 'categorical_crossentropy',
            'threshold': create_weighted_binary_crossentropy(w0, w1)
        },
        loss_weights={
            'regression': 0.50,
            'classification': 0.25,
            'threshold': 0.25
        },
        metrics={
            'regression': ['mae'],
            'classification': ['accuracy'],
            'threshold': ['accuracy']
        }
    )
    
    # Callbacks
    checkpoint_path = os.path.join(PROJECT_ROOT, f'models/progressive_window_{window_size}_best.h5')
    os.makedirs(os.path.join(PROJECT_ROOT, 'models'), exist_ok=True)
    
    # DetaylÄ± metrikler callback'i
    detailed_metrics = DetailedMetricsCallback(
        X_val=[X_f_val, X_seq_val],
        y_val=y_reg_val
    )
    
    # Weighted model checkpoint
    weighted_checkpoint = WeightedModelCheckpoint(
        filepath=checkpoint_path,
        X_val=[X_f_val, X_seq_val],
        y_val=y_reg_val
    )
    
    # Custom Learning Rate Callback
    class AdaptiveLRCallback(callbacks.Callback):
        def __init__(self, scheduler):
            super().__init__()
            self.scheduler = scheduler
            
        def on_epoch_end(self, epoch, logs=None):
            if logs is None: logs = {}
            current_lr = self.scheduler(epoch, logs)
            K.set_value(self.model.optimizer.learning_rate, current_lr)
    
    adaptive_lr_callback = AdaptiveLRCallback(adaptive_scheduler)
    
    cbs = [
        detailed_metrics,
        weighted_checkpoint,
        adaptive_lr_callback,
        callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False, verbose=1),
        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7, verbose=1)
    ]
    
    # EÄŸitim
    hist = model.fit(
        [X_f_tr, X_seq_tr],
        {
            'regression': y_reg_tr,
            'classification': y_cls_tr,
            'threshold': y_thr_tr
        },
        epochs=150,
        batch_size=32,
        validation_data=(
            [X_f_val, X_seq_val],
            {
                'regression': y_reg_val,
                'classification': y_cls_val,
                'threshold': y_thr_val
            }
        ),
        shuffle=False,  # KRITIK: Shuffle devre dÄ±ÅŸÄ±!
        callbacks=cbs,
        verbose=1
    )
    
    window_time = time.time() - window_start_time
    training_times[window_size] = window_time
    
    # En iyi modeli yÃ¼kle
    model.load_weights(checkpoint_path)
    
    # Test performansÄ±
    X_f_test, X_seq_test, y_reg_test, y_cls_test, y_thr_test = data_dict['test']
    pred = model.predict([X_f_test, X_seq_test], verbose=0)
    p_thr = pred[2].flatten()
    
    # 2 Modlu Analiz
    y_true = (y_reg_test >= 1.5).astype(int)
    y_pred_normal = (p_thr >= THRESHOLD_NORMAL).astype(int)
    y_pred_rolling = (p_thr >= THRESHOLD_ROLLING).astype(int)
    
    acc_normal = accuracy_score(y_true, y_pred_normal)
    acc_rolling = accuracy_score(y_true, y_pred_rolling)
    
    print(f"\nğŸ“Š WINDOW {window_size} SONUÃ‡LARI:")
    print(f"  Normal Mod Acc: {acc_normal*100:.2f}%")
    print(f"  Rolling Mod Acc: {acc_rolling*100:.2f}%")
    
    trained_models[window_size] = {
        'model': model,
        'scaler': data_dict['scaler'],
        'acc_normal': float(acc_normal),
        'acc_rolling': float(acc_rolling),
        'training_time': window_time
    }

total_training_time = sum(training_times.values())
print(f"\nâœ… TÃœM MODELLER EÄÄ°TÄ°LDÄ°! (Toplam: {total_training_time/60:.1f} dk)")

# =============================================================================
# ENSEMBLE PERFORMANS DEÄERLENDÄ°RMESÄ°
# =============================================================================
print("\n" + "="*80)
print("ğŸ¯ ENSEMBLE PERFORMANS DEÄERLENDÄ°RMESÄ°")
print("="*80)

X_f_test_500, X_seq_test_500, y_reg_test, _, y_thr_test = all_data_by_window[500]['test']

ensemble_predictions_reg = []
ensemble_predictions_thr = []

for window_size in window_sizes:
    model_dict = trained_models[window_size]
    model = model_dict['model']
    X_f_test_w, X_seq_test_w, _, _, _ = all_data_by_window[window_size]['test']
    pred = model.predict([X_f_test_w, X_seq_test_w], verbose=0)
    ensemble_predictions_reg.append(pred[0].flatten())
    ensemble_predictions_thr.append(pred[2].flatten())

# AÄŸÄ±rlÄ±klÄ± Ortalama
weights = [0.10, 0.15, 0.30, 0.25, 0.20] # 20, 50, 100, 250, 500
ensemble_reg = np.average(ensemble_predictions_reg, axis=0, weights=weights)
ensemble_thr = np.average(ensemble_predictions_thr, axis=0, weights=weights)

# Ensemble Metrics
y_true = (y_reg_test >= 1.5).astype(int)
y_pred_normal = (ensemble_thr >= THRESHOLD_NORMAL).astype(int)
y_pred_rolling = (ensemble_thr >= THRESHOLD_ROLLING).astype(int)

acc_ensemble_normal = accuracy_score(y_true, y_pred_normal)
acc_ensemble_rolling = accuracy_score(y_true, y_pred_rolling)

print(f"\nğŸ“Š ENSEMBLE PERFORMANSI:")
print(f"  Normal Mod ({THRESHOLD_NORMAL}): {acc_ensemble_normal*100:.2f}%")
print(f"  Rolling Mod ({THRESHOLD_ROLLING}): {acc_ensemble_rolling*100:.2f}%")

# =============================================================================
# 2 MODLU SANAL KASA SÄ°MÃœLASYONU (ENSEMBLE)
# =============================================================================
print("\n" + "="*80)
print("ğŸ’° SANAL KASA SÄ°MÃœLASYONU (ENSEMBLE)")
print("="*80)

initial_bankroll = len(y_reg_test) * 10
bet_amount = 10.0

# KASA 1: NORMAL MOD (0.85)
wallet1 = initial_bankroll
bets1 = 0
wins1 = 0

for i in range(len(y_reg_test)):
    if ensemble_thr[i] >= THRESHOLD_NORMAL:
        wallet1 -= bet_amount
        bets1 += 1
        # Dinamik Ã‡Ä±kÄ±ÅŸ
        exit_point = min(max(1.5, ensemble_reg[i] * 0.8), 2.5)
        if y_reg_test[i] >= exit_point:
            wallet1 += exit_point * bet_amount
            wins1 += 1

roi1 = (wallet1 - initial_bankroll) / initial_bankroll * 100
win_rate1 = (wins1 / bets1 * 100) if bets1 > 0 else 0

print(f"ğŸ’° KASA 1 (NORMAL - {THRESHOLD_NORMAL}):")
print(f"  ROI: {roi1:+.2f}% | Win Rate: {win_rate1:.1f}% | Bets: {bets1}")

# KASA 2: ROLLING MOD (0.95)
wallet2 = initial_bankroll
bets2 = 0
wins2 = 0

for i in range(len(y_reg_test)):
    if ensemble_thr[i] >= THRESHOLD_ROLLING:
        wallet2 -= bet_amount
        bets2 += 1
        # Sabit GÃ¼venli Ã‡Ä±kÄ±ÅŸ
        if y_reg_test[i] >= 1.5:
            wallet2 += 1.5 * bet_amount
            wins2 += 1

roi2 = (wallet2 - initial_bankroll) / initial_bankroll * 100
win_rate2 = (wins2 / bets2 * 100) if bets2 > 0 else 0

print(f"ğŸ’° KASA 2 (ROLLING - {THRESHOLD_ROLLING}):")
print(f"  ROI: {roi2:+.2f}% | Win Rate: {win_rate2:.1f}% | Bets: {bets2}")

# =============================================================================
# MODEL KAYDETME & ZIP
# =============================================================================
print("\n" + "="*80)
print("ğŸ’¾ MODELLER KAYDEDÄ°LÄ°YOR")
print("="*80)

try:
    models_dir = os.path.join(PROJECT_ROOT, 'models/progressive_multiscale')
    os.makedirs(models_dir, exist_ok=True)
    
    for window_size in window_sizes:
        model_dict = trained_models[window_size]
        model_path = os.path.join(models_dir, f'model_window_{window_size}.h5')
        model_dict['model'].save(model_path)
        scaler_path = os.path.join(models_dir, f'scaler_window_{window_size}.pkl')
        joblib.dump(model_dict['scaler'], scaler_path)
        
    # Info JSON
    info = {
        'model': 'Progressive_NN_MultiScale_Ensemble',
        'version': '3.1',
        'thresholds': {'normal': THRESHOLD_NORMAL, 'rolling': THRESHOLD_ROLLING},
        'metrics': {
            'normal_acc': float(acc_ensemble_normal),
            'rolling_acc': float(acc_ensemble_rolling)
        },
        'simulation': {
            'normal_roi': float(roi1),
            'rolling_roi': float(roi2)
        }
    }
    with open(os.path.join(models_dir, 'model_info.json'), 'w') as f:
        json.dump(info, f, indent=2)
        
    print("âœ… Modeller kaydedildi.")
    
except Exception as e:
    print(f"âŒ Kaydetme hatasÄ±: {e}")

# ZIP
zip_filename = 'jetx_models_progressive_multiscale_v3.1'
shutil.make_archive(zip_filename, 'zip', models_dir)
print(f"âœ… ZIP oluÅŸturuldu: {zip_filename}.zip")

# Colab Download
try:
    import google.colab
    from google.colab import files
    files.download(f'{zip_filename}.zip')
except:
    pass

print(f"\n{'='*80}")
print(f"BitiÅŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"{'='*80}")
