#!/usr/bin/env python3
"""
ğŸ¤– JetX XGBOOST TRAINING - Feature Engineering BazlÄ± Model

AMAÃ‡: XGBoost ile hÄ±zlÄ± ve etkili tahmin modeli eÄŸitmek

AVANTAJLAR:
- Ã‡ok daha hÄ±zlÄ± eÄŸitim (~30-60 dakika vs 2-3 saat)
- Feature importance analizi yapÄ±labilir
- Daha az bellek kullanÄ±mÄ±
- Overfitting'e daha direnÃ§li

STRATEJI:
- XGBRegressor: DeÄŸer tahmini iÃ§in
- XGBClassifier: 1.5 eÅŸik tahmini iÃ§in (scale_pos_weight ile dengeleme)

HEDEFLER:
- 1.5 ALTI DoÄŸruluk: %70-75%+
- 1.5 ÃœSTÃœ DoÄŸruluk: %70-75%+
- MAE: < 5.0

SÃœRE: ~30-60 dakika (GPU ile)
"""

import subprocess
import sys
import os
import time
from datetime import datetime

print("="*80)
print("ğŸ¤– JetX XGBOOST TRAINING")
print("="*80)
print(f"BaÅŸlangÄ±Ã§: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print()

# KÃ¼tÃ¼phaneleri yÃ¼kle
print("ğŸ“¦ KÃ¼tÃ¼phaneler yÃ¼kleniyor...")
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", 
                      "xgboost", "scikit-learn", "pandas", "numpy", 
                      "scipy", "joblib", "matplotlib", "seaborn", "tqdm",
                      "PyWavelets", "nolds"])

import numpy as np
import pandas as pd
import joblib
import sqlite3
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix, classification_report
import xgboost as xgb
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

print(f"âœ… XGBoost: {xgb.__version__}")

# Proje yÃ¼kle
if not os.path.exists('jetxpredictor'):
    print("\nğŸ“¥ Proje klonlanÄ±yor...")
    subprocess.check_call(["git", "clone", "https://github.com/onndd/jetxpredictor.git"])

os.chdir('jetxpredictor')
sys.path.append(os.getcwd())

from category_definitions import CategoryDefinitions, FeatureEngineering
from utils.advanced_bankroll import AdvancedBankrollManager
print(f"âœ… Proje yÃ¼klendi - Kritik eÅŸik: {CategoryDefinitions.CRITICAL_THRESHOLD}x\n")

# =============================================================================
# VERÄ° YÃœKLEME
# =============================================================================
print("ğŸ“Š Veri yÃ¼kleniyor...")
conn = sqlite3.connect('jetx_data.db')
data = pd.read_sql_query("SELECT value FROM jetx_results ORDER BY id", conn)
conn.close()

all_values = data['value'].values
print(f"âœ… {len(all_values):,} veri yÃ¼klendi")
print(f"AralÄ±k: {all_values.min():.2f}x - {all_values.max():.2f}x")

below = (all_values < 1.5).sum()
above = (all_values >= 1.5).sum()
print(f"\nğŸ“Š CLASS DAÄILIMI:")
print(f"  1.5 altÄ±: {below:,} ({below/len(all_values)*100:.1f}%)")
print(f"  1.5 Ã¼stÃ¼: {above:,} ({above/len(all_values)*100:.1f}%)")
print(f"  Dengesizlik: 1:{above/below:.2f}")

# =============================================================================
# FEATURE ENGINEERING
# =============================================================================
print("\nğŸ”§ Feature extraction...")
window_size = 1000  # Neural network ile aynÄ±
X_features = []
y_regression = []
y_classification = []

for i in tqdm(range(window_size, len(all_values)-1), desc='Features'):
    hist = all_values[:i].tolist()
    target = all_values[i]
    
    # TÃ¼m Ã¶zellikleri Ã§Ä±kar
    feats = FeatureEngineering.extract_all_features(hist)
    X_features.append(list(feats.values()))
    
    # Regression target
    y_regression.append(target)
    
    # Classification target (1.5 altÄ±/Ã¼stÃ¼)
    y_classification.append(1 if target >= 1.5 else 0)

X = np.array(X_features)
y_reg = np.array(y_regression)
y_cls = np.array(y_classification)

print(f"âœ… {len(X):,} Ã¶rnek hazÄ±rlandÄ±")
print(f"âœ… Feature sayÄ±sÄ±: {X.shape[1]}")

# =============================================================================
# NORMALIZASYON
# =============================================================================
print("\nğŸ“Š Normalizasyon...")
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train/Test split - STRATIFIED SAMPLING EKLENDI
X_train, X_test, y_reg_train, y_reg_test, y_cls_train, y_cls_test = train_test_split(
    X, y_reg, y_cls, test_size=0.2, shuffle=True, stratify=y_cls, random_state=42
)

print(f"âœ… Train: {len(X_train):,}, Test: {len(X_test):,}")

# =============================================================================
# XGBOOST REGRESSOR (DeÄŸer Tahmini)
# =============================================================================
print("\n" + "="*80)
print("ğŸ¯ XGBOOST REGRESSOR EÄÄ°TÄ°MÄ° (DeÄŸer Tahmini)")
print("="*80)

reg_start = time.time()

# XGBoost parametreleri
regressor = xgb.XGBRegressor(
    n_estimators=500,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=3,
    gamma=0.1,
    random_state=42,
    tree_method='hist',  # CPU iÃ§in optimize
    objective='reg:squarederror',
    eval_metric='mae'
)

print("ğŸ“Š Model Parametreleri:")
print(f"  n_estimators: 500")
print(f"  max_depth: 8")
print(f"  learning_rate: 0.05")
print(f"  subsample: 0.8")
print(f"  colsample_bytree: 0.8\n")

# EÄŸitim
print("ğŸ”¥ EÄŸitim baÅŸlÄ±yor...")
regressor.fit(
    X_train, y_reg_train,
    eval_set=[(X_test, y_reg_test)],
    verbose=50
)

reg_time = time.time() - reg_start
print(f"\nâœ… Regressor eÄŸitimi tamamlandÄ±! SÃ¼re: {reg_time/60:.1f} dakika")

# DeÄŸerlendirme
y_reg_pred = regressor.predict(X_test)
mae_reg = mean_absolute_error(y_reg_test, y_reg_pred)
rmse_reg = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred))

print(f"\nğŸ“Š REGRESSOR PERFORMANSI:")
print(f"  MAE: {mae_reg:.4f}")
print(f"  RMSE: {rmse_reg:.4f}")

# Feature importance (Top 15)
feature_names = list(FeatureEngineering.extract_all_features(all_values[:1000].tolist()).keys())
importances = regressor.feature_importances_
top_features = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)[:15]

print(f"\nğŸ“Š TOP 15 Ã–NEMLÄ° Ã–ZELLIKLER:")
for i, (feat, imp) in enumerate(top_features, 1):
    print(f"  {i:2d}. {feat:30s}: {imp:.4f}")

# =============================================================================
# XGBOOST CLASSIFIER (EÅŸik Tahmini)
# =============================================================================
print("\n" + "="*80)
print("ğŸ¯ XGBOOST CLASSIFIER EÄÄ°TÄ°MÄ° (1.5 EÅŸik Tahmini)")
print("="*80)

cls_start = time.time()

# sample_weight ile class dengeleme (HATA DÃœZELTÄ°LDÄ°!)
# Eski hatalÄ± kod: scale_pos_weight = above_count / below_count (YANLIÅ!)
# Yeni Ã§Ã¶zÃ¼m: sample_weight kullanarak doÄŸrudan 1.5 altÄ± Ã¶rneklere aÄŸÄ±rlÄ±k ver
below_count = (y_cls_train == 0).sum()
above_count = (y_cls_train == 1).sum()

# Her Ã¶rnek iÃ§in sample weight hesapla
# 1.5 altÄ± iÃ§in 5.0x, 1.5 Ã¼stÃ¼ iÃ§in 1.0x
WEIGHT_MULTIPLIER = 5.0  # 1.5 altÄ± iÃ§in aÄŸÄ±rlÄ±k Ã§arpanÄ±
sample_weights_train = np.where(y_cls_train == 0, WEIGHT_MULTIPLIER, 1.0)

print(f"ğŸ“Š SAMPLE WEIGHT (HATA DÃœZELTÄ°LDÄ°!):")
print(f"  1.5 altÄ±: {below_count:,} Ã¶rnek â†’ Her biri {WEIGHT_MULTIPLIER}x aÄŸÄ±rlÄ±k")
print(f"  1.5 Ã¼stÃ¼: {above_count:,} Ã¶rnek â†’ Her biri 1.0x aÄŸÄ±rlÄ±k")
print(f"  âœ… scale_pos_weight HATASI dÃ¼zeltildi â†’ sample_weight kullanÄ±lÄ±yor\n")

# XGBoost parametreleri - scale_pos_weight KALDIRILDI
classifier = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=7,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=3,
    gamma=0.1,
    # scale_pos_weight KALDIRILDI - sample_weight kullanÄ±lacak
    random_state=42,
    tree_method='hist',
    objective='binary:logistic',
    eval_metric='logloss'
)

print("ğŸ“Š Model Parametreleri:")
print(f"  n_estimators: 500")
print(f"  max_depth: 7")
print(f"  learning_rate: 0.05")
print(f"  sample_weight kullanÄ±lÄ±yor: {WEIGHT_MULTIPLIER}x (1.5 altÄ± iÃ§in)\n")

# EÄŸitim - sample_weight ile
print("ğŸ”¥ EÄŸitim baÅŸlÄ±yor (sample_weight ile)...")
classifier.fit(
    X_train, y_cls_train,
    sample_weight=sample_weights_train,  # SAMPLE WEIGHT EKLENDI
    eval_set=[(X_test, y_cls_test)],
    verbose=50
)

cls_time = time.time() - cls_start
print(f"\nâœ… Classifier eÄŸitimi tamamlandÄ±! SÃ¼re: {cls_time/60:.1f} dakika")

# DeÄŸerlendirme
y_cls_pred = classifier.predict(X_test)
y_cls_proba = classifier.predict_proba(X_test)[:, 1]  # 1.5 Ã¼stÃ¼ olma olasÄ±lÄ±ÄŸÄ±

cls_acc = accuracy_score(y_cls_test, y_cls_pred)

# SÄ±nÄ±f bazÄ±nda accuracy
below_mask = y_cls_test == 0
above_mask = y_cls_test == 1

below_acc = accuracy_score(y_cls_test[below_mask], y_cls_pred[below_mask]) if below_mask.sum() > 0 else 0
above_acc = accuracy_score(y_cls_test[above_mask], y_cls_pred[above_mask]) if above_mask.sum() > 0 else 0

print(f"\nğŸ“Š CLASSIFIER PERFORMANSI:")
print(f"  Genel Accuracy: {cls_acc*100:.2f}%")
print(f"  ğŸ”´ 1.5 AltÄ± DoÄŸruluk: {below_acc*100:.2f}%")
print(f"  ğŸŸ¢ 1.5 ÃœstÃ¼ DoÄŸruluk: {above_acc*100:.2f}%")

# Confusion Matrix
cm = confusion_matrix(y_cls_test, y_cls_pred)
print(f"\nğŸ“‹ CONFUSION MATRIX:")
print(f"                Tahmin")
print(f"GerÃ§ek   1.5 AltÄ± | 1.5 ÃœstÃ¼")
print(f"1.5 AltÄ± {cm[0,0]:6d}   | {cm[0,1]:6d}  âš ï¸ PARA KAYBI")
print(f"1.5 ÃœstÃ¼ {cm[1,0]:6d}   | {cm[1,1]:6d}")

if cm[0,0] + cm[0,1] > 0:
    fpr = cm[0,1] / (cm[0,0] + cm[0,1])
    print(f"\nğŸ’° PARA KAYBI RÄ°SKÄ°: {fpr*100:.1f}%", end="")
    if fpr < 0.20:
        print(" âœ… HEDEF AÅILDI!")
    else:
        print(f" (Hedef: <20%)")
    
    # Classification Report
    print(f"\nğŸ“Š DETAYLI RAPOR:")
    print(classification_report(y_cls_test, y_cls_pred, target_names=['1.5 AltÄ±', '1.5 ÃœstÃ¼']))
    
    # =============================================================================
    # GELÄ°ÅMÄ°Å SANAL KASA SÄ°MÃœLASYONU
    # =============================================================================
    print("\n" + "="*70)
    print("ğŸ’° GELÄ°ÅMÄ°Å SANAL KASA SÄ°MÃœLASYONU (Kelly Criterion)")
    print("="*70)
    print("3 farklÄ± risk tolerance stratejisi ile test ediliyor...")
    
    # 3 farklÄ± risk tolerance ile test
    for risk_tolerance in ['conservative', 'moderate', 'aggressive']:
        print(f"\n{'='*70}")
        print(f"ğŸ“Š {risk_tolerance.upper()} STRATEJÄ°")
        print(f"{'='*70}")
        
        # Advanced Bankroll Manager oluÅŸtur
        bankroll = AdvancedBankrollManager(
            initial_bankroll=1000.0,
            risk_tolerance=risk_tolerance,
            win_multiplier=1.5,
            min_bet=10.0
        )
        
        # Test seti Ã¼zerinde simÃ¼lasyon
        for i in range(len(y_reg_test)):
            # Model tahmini
            model_pred_cls = y_cls_pred[i]  # 0 veya 1
            model_confidence = y_cls_proba[i]  # Confidence score
            actual_value = y_reg_test[i]
            
            # Model "1.5 Ã¼stÃ¼" (1) tahmin ediyorsa bahis yap
            if model_pred_cls == 1:
                # Optimal bahis miktarÄ±nÄ± hesapla (Kelly Criterion)
                bet_size = bankroll.calculate_bet_size(
                    confidence=model_confidence,
                    predicted_value=1.5
                )
                
                # Bet size > 0 ise bahis yap
                if bet_size > 0:
                    result = bankroll.place_bet(
                        bet_size=bet_size,
                        predicted_value=1.5,
                        actual_value=actual_value,
                        confidence=model_confidence
                    )
                
                # Stop-loss veya take-profit kontrolÃ¼
                should_stop, reason = bankroll.should_stop()
                if should_stop:
                    print(f"\nâš ï¸ {reason}")
                    print(f"SimÃ¼lasyon durduruluyor (Test Ã¶rneÄŸi {i+1}/{len(y_reg_test)})")
                    break
        
        # DetaylÄ± rapor
        bankroll.print_report()
    
    print("\n" + "="*70)
    print("âœ… GeliÅŸmiÅŸ sanal kasa simÃ¼lasyonu tamamlandÄ±!")
    print("="*70)

# =============================================================================
# MODEL KAYDETME
# =============================================================================
print("\n" + "="*80)
print("ğŸ’¾ Modeller kaydediliyor...")
print("="*80)

# models/ klasÃ¶rÃ¼nÃ¼ oluÅŸtur
os.makedirs('models', exist_ok=True)

# Modelleri kaydet
regressor.save_model('models/xgboost_regressor.json')
classifier.save_model('models/xgboost_classifier.json')
joblib.dump(scaler, 'models/xgboost_scaler.pkl')

print("âœ… Dosyalar kaydedildi:")
print("  - models/xgboost_regressor.json")
print("  - models/xgboost_classifier.json")
print("  - models/xgboost_scaler.pkl")

# Model bilgilerini kaydet
import json
total_time = reg_time + cls_time
info = {
    'model': 'XGBOOST_DUAL_MODEL',
    'version': '1.0_XGBOOST',
    'training_time_minutes': round(total_time/60, 1),
    'model_times': {
        'regressor': round(reg_time/60, 1),
        'classifier': round(cls_time/60, 1)
    },
    'feature_count': X.shape[1],
    'metrics': {
        'regression': {
            'mae': float(mae_reg),
            'rmse': float(rmse_reg)
        },
        'classification': {
            'accuracy': float(cls_acc),
            'below_15_accuracy': float(below_acc),
            'above_15_accuracy': float(above_acc),
            'money_loss_risk': float(fpr) if cm[0,0] + cm[0,1] > 0 else 0.0
        }
    },
    'hyperparameters': {
        'regressor': {
            'n_estimators': 500,
            'max_depth': 8,
            'learning_rate': 0.05
        },
        'classifier': {
            'n_estimators': 500,
            'max_depth': 7,
            'learning_rate': 0.05,
            'scale_pos_weight': float(scale_pos_weight)
        }
    },
    'top_features': [{'name': feat, 'importance': float(imp)} for feat, imp in top_features]
}

with open('models/xgboost_model_info.json', 'w') as f:
    json.dump(info, f, indent=2)

print("  - models/xgboost_model_info.json")

print(f"\nğŸ“Š Model Bilgisi:")
print(json.dumps(info, indent=2))

# Google Colab'da indir - Ä°yileÅŸtirilmiÅŸ kontrol
try:
    import google.colab
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

if IN_COLAB:
    try:
        from google.colab import files
        print("\nğŸ“¥ Dosyalar indiriliyor...")
        files.download('models/xgboost_regressor.json')
        print("âœ… xgboost_regressor.json indirildi")
        files.download('models/xgboost_classifier.json')
        print("âœ… xgboost_classifier.json indirildi")
        files.download('models/xgboost_scaler.pkl')
        print("âœ… xgboost_scaler.pkl indirildi")
        files.download('models/xgboost_model_info.json')
        print("âœ… xgboost_model_info.json indirildi")
        print("\nâœ… TÃ¼m dosyalar baÅŸarÄ±yla indirildi!")
        print("ğŸ“Œ Bu dosyalarÄ± lokal projenizin models/ klasÃ¶rÃ¼ne kopyalayÄ±n")
    except Exception as e:
        print(f"\nâš ï¸ Ä°ndirme hatasÄ±: {e}")
        print("ğŸ“ Dosyalar models/ klasÃ¶rÃ¼nde kaydedildi.")
else:
    print("\nâš ï¸ Google Colab ortamÄ± algÄ±lanamadÄ± - dosyalar sadece kaydedildi")
    print("ğŸ“ Dosyalar models/ klasÃ¶rÃ¼nde mevcut:")
    print("   â€¢ xgboost_regressor.json")
    print("   â€¢ xgboost_classifier.json")
    print("   â€¢ xgboost_scaler.pkl")
    print("   â€¢ xgboost_model_info.json")
    print("\nğŸ’¡ Not: Bu script Google Colab'da Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda dosyalar otomatik indirilir.")

# Final rapor
print("\n" + "="*80)
print("ğŸ‰ XGBOOST TRAINING TAMAMLANDI!")
print("="*80)
print(f"Toplam SÃ¼re: {total_time/60:.1f} dakika ({total_time/3600:.1f} saat)")
print()

if below_acc >= 0.70 and fpr < 0.25:
    print("âœ… âœ… Ä°YÄ° PERFORMANS!")
    print(f"  ğŸ”´ 1.5 ALTI: {below_acc*100:.1f}%")
    print(f"  ğŸ’° Para kaybÄ±: {fpr*100:.1f}%")
    print("\nğŸš€ Model kullanÄ±ma hazÄ±r!")
else:
    print("âš ï¸ Orta performans")
    print(f"  ğŸ”´ 1.5 ALTI: {below_acc*100:.1f}%")
    print(f"  ğŸ’° Para kaybÄ±: {fpr*100:.1f}%")
    print("\nXGBoost Neural Network'e gÃ¶re daha basit ama hÄ±zlÄ± bir alternatiftir.")

print("\nğŸ“ Sonraki adÄ±m:")
print("  1. XGBoost modellerini lokal projeye kopyalayÄ±n")
print("  2. Predictor'da model_type='xgboost' ile kullanÄ±n")
print("  3. Neural Network ile karÅŸÄ±laÅŸtÄ±rÄ±n")
print("="*80)
print(f"BitiÅŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)
