{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JetX Predictor - Model Eğitim Notebook'u\
",
    "\
",
    "Bu notebook JetX tahmin modelini eğitmek için kullanılır.\
",
    "\
",
    "**Önemli:** Bu notebook Google Colab'da çalıştırılmalıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kurulum ve İçe Aktarımlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive'ı bağla\
",
    "from google.colab import drive\
",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneleri yükle\
",
    "!pip install tensorflow scikit-learn pandas numpy matplotlib seaborn plotly joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# İçe aktarımlar\
",
    "import numpy as np\
",
    "import pandas as pd\
",
    "import matplotlib.pyplot as plt\
",
    "import seaborn as sns\
",
    "import sqlite3\
",
    "from sklearn.model_selection import train_test_split\
",
    "from sklearn.preprocessing import StandardScaler\
",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\
",
    "import tensorflow as tf\
",
    "from tensorflow import keras\
",
    "from tensorflow.keras.models import Sequential\
",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU\
",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\
",
    "import joblib\
",
    "import warnings\
",
    "warnings.filterwarnings('ignore')\
",
    "\
",
    "print(\"TensorFlow version:\", tf.__version__)\
",
    "print(\"GPU kullanılabilir mi:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Veritabanını Yükle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veritabanı dosyasını Colab'a yükle\
",
    "# Yöntem 1: Google Drive'dan\
",
    "# db_path = '/content/drive/MyDrive/JetXPredictor/data/jetx_data.db'\
",
    "\
",
    "# Yöntem 2: Manuel yükleme\
",
    "from google.colab import files\
",
    "uploaded = files.upload()\
",
    "db_path = 'jetx_data.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veritabanından verileri oku\
",
    "conn = sqlite3.connect(db_path)\
",
    "df = pd.read_sql_query(\"SELECT * FROM jetx_results ORDER BY id\", conn)\
",
    "conn.close()\
",
    "\
",
    "print(f\"Toplam veri sayısı: {len(df)}\")\
",
    "print(f\"\\
İlk 5 kayıt:\")\
",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Veri Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temel istatistikler\
",
    "print(\"Temel İstatistikler:\")\
",
    "print(df['value'].describe())\
",
    "\
",
    "# 1.5x eşik analizi\
",
    "below_threshold = len(df[df['value'] < 1.5])\
",
    "above_threshold = len(df[df['value'] >= 1.5])\
",
    "print(f\"\\
1.5x Altı: {below_threshold} ({below_threshold/len(df)*100:.2f}%)\")\
",
    "print(f\"1.5x Üstü: {above_threshold} ({above_threshold/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Görselleştirme\
",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\
",
    "\
",
    "# Histogram\
",
    "axes[0, 0].hist(df['value'], bins=50, edgecolor='black')\
",
    "axes[0, 0].axvline(x=1.5, color='red', linestyle='--', label='1.5x Eşik')\
",
    "axes[0, 0].set_title('Değer Dağılımı')\
",
    "axes[0, 0].set_xlabel('Çarpan')\
",
    "axes[0, 0].set_ylabel('Frekans')\
",
    "axes[0, 0].legend()\
",
    "\
",
    "# Zaman serisi\
",
    "axes[0, 1].plot(df['value'], linewidth=0.5)\
",
    "axes[0, 1].axhline(y=1.5, color='red', linestyle='--', label='1.5x Eşik')\
",
    "axes[0, 1].set_title('Zaman Serisi')\
",
    "axes[0, 1].set_xlabel('Oyun #')\
",
    "axes[0, 1].set_ylabel('Çarpan')\
",
    "axes[0, 1].legend()\
",
    "\
",
    "# Box plot\
",
    "axes[1, 0].boxplot([df[df['value'] < 1.5]['value'], df[df['value'] >= 1.5]['value']], \
",
    "                    labels=['< 1.5x', '≥ 1.5x'])\
",
    "axes[1, 0].set_title('1.5x Eşiğine Göre Dağılım')\
",
    "axes[1, 0].set_ylabel('Çarpan')\
",
    "\
",
    "# Hareketli ortalama\
",
    "df['ma_50'] = df['value'].rolling(window=50).mean()\
",
    "axes[1, 1].plot(df['ma_50'])\
",
    "axes[1, 1].axhline(y=1.5, color='red', linestyle='--', label='1.5x Eşik')\
",
    "axes[1, 1].set_title('50 Periyot Hareketli Ortalama')\
",
    "axes[1, 1].set_xlabel('Oyun #')\
",
    "axes[1, 1].set_ylabel('Hareketli Ortalama')\
",
    "axes[1, 1].legend()\
",
    "\
",
    "plt.tight_layout()\
",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Özellik Mühendisliği\
",
    "\
",
    "category_definitions.py dosyasındaki fonksiyonları kullanacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_definitions.py dosyasını yükle\
",
    "# Manuel olarak kodu buraya kopyalayın veya dosyayı yükleyin\
",
    "\
",
    "# Özellik çıkarma fonksiyonu\
",
    "def extract_features(data, window_size=50):\
",
    "    \"\"\"\
",
    "    Zaman serisi verilerinden özellikler çıkarır\
",
    "    \"\"\"\
",
    "    features = []\
",
    "    targets = []\
",
    "    \
",
    "    for i in range(window_size, len(data)):\
",
    "        window = data[i-window_size:i]\
",
    "        \
",
    "        # Temel özellikler\
",
    "        feature_dict = {\
",
    "            # Hareketli ortalamalar\
",
    "            'mean_5': np.mean(window[-5:]),\
",
    "            'mean_10': np.mean(window[-10:]),\
",
    "            'mean_20': np.mean(window[-20:]),\
",
    "            'mean_50': np.mean(window),\
",
    "            \
",
    "            # Standart sapmalar\
",
    "            'std_10': np.std(window[-10:]),\
",
    "            'std_20': np.std(window[-20:]),\
",
    "            \
",
    "            # Min/Max\
",
    "            'min_10': np.min(window[-10:]),\
",
    "            'max_10': np.max(window[-10:]),\
",
    "            'min_50': np.min(window),\
",
    "            'max_50': np.max(window),\
",
    "            \
",
    "            # 1.5x eşik özellikleri\
",
    "            'below_15_count_10': sum(1 for v in window[-10:] if v < 1.5),\
",
    "            'above_15_count_10': sum(1 for v in window[-10:] if v >= 1.5),\
",
    "            'threshold_ratio_50': sum(1 for v in window if v >= 1.5) / len(window),\
",
    "            \
",
    "            # Volatilite\
",
    "            'range_10': np.max(window[-10:]) - np.min(window[-10:]),\
",
    "            'range_50': np.max(window) - np.min(window),\
",
    "            \
",
    "            # Son değer\
",
    "            'last_value': window[-1],\
",
    "        }\
",
    "        \
",
    "        # Büyük çarpan mesafeleri\
",
    "        for milestone in [10.0, 20.0, 50.0, 100.0]:\
",
    "            distance = 0\
",
    "            for j in range(len(window)-1, -1, -1):\
",
    "                if window[j] >= milestone:\
",
    "                    distance = len(window) - 1 - j\
",
    "                    break\
",
    "            else:\
",
    "                distance = len(window)\
",
    "            feature_dict[f'distance_{int(milestone)}x'] = distance\
",
    "        \
",
    "        features.append(list(feature_dict.values()))\
",
    "        \
",
    "        # Hedef: 1.5x üstü mü?\
",
    "        targets.append(1 if data[i] >= 1.5 else 0)\
",
    "    \
",
    "    return np.array(features), np.array(targets), list(feature_dict.keys())\
",
    "\
",
    "# Özellikleri çıkar\
",
    "X, y, feature_names = extract_features(df['value'].values, window_size=50)\
",
    "\
",
    "print(f\"Özellik sayısı: {X.shape[1]}\")\
",
    "print(f\"Örnek sayısı: {X.shape[0]}\")\
",
    "print(f\"Hedef dağılımı: {np.bincount(y)}\")\
",
    "print(f\"\\
Özellik isimleri: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Veri Hazırlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test split\
",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=False)\
",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, shuffle=False)\
",
    "\
",
    "print(f\"Train: {X_train.shape[0]} örneks ({X_train.shape[0]/len(X)*100:.1f}%)\")\
",
    "print(f\"Validation: {X_val.shape[0]} örneks ({X_val.shape[0]/len(X)*100:.1f}%)\")\
",
    "print(f\"Test: {X_test.shape[0]} örneks ({X_test.shape[0]/len(X)*100:.1f}%)\")\
",
    "\
",
    "# Normalizasyon\
",
    "scaler = StandardScaler()\
",
    "X_train_scaled = scaler.fit_transform(X_train)\
",
    "X_val_scaled = scaler.transform(X_val)\
",
    "X_test_scaled = scaler.transform(X_test)\
",
    "\
",
    "print(\"\\
Normalizasyon tamamlandı!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Oluşturma ve Eğitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model mimarisi (Dense Neural Network)\
",
    "model = Sequential([\
",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\
",
    "    Dropout(0.3),\
",
    "    Dense(64, activation='relu'),\
",
    "    Dropout(0.2),\
",
    "    Dense(32, activation='relu'),\
",
    "    Dropout(0.2),\
",
    "    Dense(16, activation='relu'),\
",
    "    Dense(1, activation='sigmoid')\
",
    "])\
",
    "\
",
    "# Derleme\
",
    "model.compile(\
",
    "    optimizer='adam',\
",
    "    loss='binary_crossentropy',\
",
    "    metrics=['accuracy']\
",
    ")\
",
    "\
",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback'ler\
",
    "early_stopping = EarlyStopping(\
",
    "    monitor='val_loss',\
",
    "    patience=10,\
",
    "    restore_best_weights=True\
",
    ")\
",
    "\
",
    "checkpoint = ModelCheckpoint(\
",
    "    'best_model.h5',\
",
    "    monitor='val_accuracy',\
",
    "    save_best_only=True,\
",
    "    mode='max'\
",
    ")\
",
    "\
",
    "# Eğitim\
",
    "history = model.fit(\
",
    "    X_train_scaled, y_train,\
",
    "    validation_data=(X_val_scaled, y_val),\
",
    "    epochs=100,\
",
    "    batch_size=32,\
",
    "    callbacks=[early_stopping, checkpoint],\
",
    "    verbose=1\
",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Değerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim grafiği\
",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\
",
    "\
",
    "# Loss\
",
    "axes[0].plot(history.history['loss'], label='Train Loss')\
",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\
",
    "axes[0].set_title('Model Loss')\
",
    "axes[0].set_xlabel('Epoch')\
",
    "axes[0].set_ylabel('Loss')\
",
    "axes[0].legend()\
",
    "\
",
    "# Accuracy\
",
    "axes[1].plot(history.history['accuracy'], label='Train Accuracy')\
",
    "axes[1].plot(history.history['val_accuracy'], label='Val Accuracy')\
",
    "axes[1].set_title('Model Accuracy')\
",
    "axes[1].set_xlabel('Epoch')\
",
    "axes[1].set_ylabel('Accuracy')\
",
    "axes[1].legend()\
",
    "\
",
    "plt.tight_layout()\
",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test seti değerlendirmesi\
",
    "y_pred_proba = model.predict(X_test_scaled)\
",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\
",
    "\
",
    "# Metrikler\
",
    "accuracy = accuracy_score(y_test, y_pred)\
",
    "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\
",
    "print(\"\\
Classification Report:\")\
",
    "print(classification_report(y_test, y_pred, target_names=['< 1.5x', '≥ 1.5x']))\
",
    "\
",
    "# Confusion Matrix\
",
    "cm = confusion_matrix(y_test, y_pred)\
",
    "plt.figure(figsize=(8, 6))\
",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \
",
    "            xticklabels=['< 1.5x', '≥ 1.5x'],\
",
    "            yticklabels=['< 1.5x', '≥ 1.5x'])\
",
    "plt.title('Confusion Matrix')\
",
    "plt.ylabel('Gerçek')\
",
    "plt.xlabel('Tahmin')\
",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ve scaler'ı kaydet\
",
    "model.save('jetx_model.h5')\
",
    "joblib.dump(scaler, 'scaler.pkl')\
",
    "\
",
    "print(\"✅ Model kaydedildi: jetx_model.h5\")\
",
    "print(\"✅ Scaler kaydedildi: scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive'a kaydet (opsiyonel)\
",
    "!cp jetx_model.h5 /content/drive/MyDrive/JetXPredictor/models/\
",
    "!cp scaler.pkl /content/drive/MyDrive/JetXPredictor/models/\
",
    "\
",
    "print(\"✅ Dosyalar Google Drive'a kopyalandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dosyaları indir\
",
    "from google.colab import files\
",
    "files.download('jetx_model.h5')\
",
    "files.download('scaler.pkl')\
",
    "\
",
    "print(\"✅ Dosyalar indiriliyor...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Tahminleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Son birkaç örnekle test\
",
    "num_samples = 10\
",
    "test_samples = X_test_scaled[-num_samples:]\
",
    "test_labels = y_test[-num_samples:]\
",
    "\
",
    "predictions = model.predict(test_samples)\
",
    "\
",
    "print(\"Son 10 Test Örneği:\")\
",
    "print(\"=\"*60)\
",
    "for i in range(num_samples):\
",
    "    pred_proba = predictions[i][0]\
",
    "    pred_class = 1 if pred_proba > 0.5 else 0\
",
    "    actual = test_labels[i]\
",
    "    correct = \"✅\" if pred_class == actual else \"❌\"\
",
    "    \
",
    "    print(f\"Örnek {i+1}: Tahmin={pred_class} (Güven: {pred_proba:.2%}), \"\
",
    "          f\"Gerçek={actual} {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sonraki Adımlar\
",
    "\
",
    "1. ✅ Modeli indirin (`jetx_model.h5` ve `scaler.pkl`)\
",
    "2. ✅ Lokal projenizin `models/` klasörüne koyun\
",
    "3. ✅ Streamlit uygulamasını çalıştırın: `streamlit run app.py`\
",
    "4. ✅ Tahminleri test edin!\
",
    "\
",
    "**Önemli Notlar:**\
",
    "- Model performansını sürekli izleyin\
",
    "- Yeni verilerle düzenli olarak yeniden eğitin\
",
    "- 1.5x eşik doğruluğuna odaklanın\
",
    "- Rolling modda %80+ güven kullanın"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
