{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ° JetX AGGRESSIVE Model - Para KaybÄ± Ã–nleyici\n",
    "\n",
    "**âš ï¸ 1.5x ALTI TAHMÄ°NLERÄ°NDE YÃœKSEK DOÄRULUK Ä°Ã‡Ä°N OPTÄ°MÄ°ZE EDÄ°LDÄ°**\n",
    "\n",
    "Bu notebook Google Colab'da Ã§alÄ±ÅŸÄ±r ve agresif eÄŸitim ile 1.5 altÄ± tahminlerinde maksimum performans saÄŸlar.\n",
    "\n",
    "## Ã–zellikler\n",
    "- âœ… Class Imbalance Ã§Ã¶zÃ¼mÃ¼ (1.5 altÄ± iÃ§in 2.5x aÄŸÄ±rlÄ±k)\n",
    "- âœ… Focal Loss (yanlÄ±ÅŸ tahminlere ekstra ceza)\n",
    "- âœ… GÃ¼Ã§lendirilmiÅŸ model (2x katman)\n",
    "- âœ… 300 epoch agresif eÄŸitim\n",
    "- âœ… DetaylÄ± metrik tracking\n",
    "- âœ… Colab optimizasyonlarÄ±\n",
    "\n",
    "## Hedefler\n",
    "- 1.5 ALTI DoÄŸruluk: **70%+** âœ…\n",
    "- 1.5 ÃœSTÃœ DoÄŸruluk: 75%+\n",
    "- Genel Accuracy: 80%+\n",
    "\n",
    "**SÃ¼re:** ~2-3 saat (GPU ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KÃ¼tÃ¼phaneleri yÃ¼kle\n",
    "!pip install -q tensorflow scikit-learn pandas numpy scipy joblib matplotlib seaborn tqdm\n",
    "\n",
    "import sys, os, sqlite3, numpy as np, pandas as pd, joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub'dan klonla\n",
    "if not os.path.exists('jetxpredictor'):\n",
    "    !git clone https://github.com/onndd/jetxpredictor.git\n",
    "%cd jetxpredictor\n",
    "sys.path.append('/content/jetxpredictor')\n",
    "\n",
    "from category_definitions import CategoryDefinitions, FeatureEngineering\n",
    "print(f\"âœ… Proje yÃ¼klendi - Kritik eÅŸik: {CategoryDefinitions.CRITICAL_THRESHOLD}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VeritabanÄ±ndan veri yÃ¼kle\n",
    "conn = sqlite3.connect('jetx_data.db')\n",
    "data = pd.read_sql_query(\"SELECT value FROM jetx_results ORDER BY id\", conn)\n",
    "conn.close()\n",
    "\n",
    "all_values = data['value'].values\n",
    "print(f\"âœ… {len(all_values)} veri yÃ¼klendi\")\n",
    "print(f\"AralÄ±k: {all_values.min():.2f}x - {all_values.max():.2f}x\")\n",
    "\n",
    "below = (all_values < 1.5).sum()\n",
    "above = (all_values >= 1.5).sum()\n",
    "print(f\"\\nğŸ”´ CLASS IMBALANCE:\")\n",
    "print(f\"1.5 altÄ±: {below} ({below/len(all_values)*100:.1f}%)\")\n",
    "print(f\"1.5 Ã¼stÃ¼: {above} ({above/len(all_values)*100:.1f}%)\")\n",
    "print(f\"Dengesizlik: 1:{above/below:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "window_size = 500\n",
    "X_f, X_50, X_200, X_500 = [], [], [], []\n",
    "y_reg, y_cls, y_thr = [], [], []\n",
    "\n",
    "for i in tqdm(range(window_size, len(all_values)-1), desc='Features'):\n",
    "    hist = all_values[:i].tolist()\n",
    "    target = all_values[i]\n",
    "    \n",
    "    feats = FeatureEngineering.extract_all_features(hist)\n",
    "    X_f.append(list(feats.values()))\n",
    "    X_50.append(all_values[i-50:i])\n",
    "    X_200.append(all_values[i-200:i])\n",
    "    X_500.append(all_values[i-500:i])\n",
    "    \n",
    "    y_reg.append(target)\n",
    "    cat = CategoryDefinitions.get_category_numeric(target)\n",
    "    onehot = np.zeros(3)\n",
    "    onehot[cat] = 1\n",
    "    y_cls.append(onehot)\n",
    "    y_thr.append(1.0 if target >= 1.5 else 0.0)\n",
    "\n",
    "X_f = np.array(X_f)\n",
    "X_50 = np.array(X_50).reshape(-1, 50, 1)\n",
    "X_200 = np.array(X_200).reshape(-1, 200, 1)\n",
    "X_500 = np.array(X_500).reshape(-1, 500, 1)\n",
    "y_reg = np.array(y_reg)\n",
    "y_cls = np.array(y_cls)\n",
    "y_thr = np.array(y_thr)\n",
    "\n",
    "print(f\"âœ… {len(X_f)} Ã¶rnek hazÄ±rlandÄ±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizasyon\n",
    "scaler = StandardScaler()\n",
    "X_f = scaler.fit_transform(X_f)\n",
    "X_50 = np.log10(X_50 + 1e-8)\n",
    "X_200 = np.log10(X_200 + 1e-8)\n",
    "X_500 = np.log10(X_500 + 1e-8)\n",
    "\n",
    "# Split\n",
    "idx = np.arange(len(X_f))\n",
    "tr_idx, te_idx = train_test_split(idx, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_f_tr, X_50_tr, X_200_tr, X_500_tr = X_f[tr_idx], X_50[tr_idx], X_200[tr_idx], X_500[tr_idx]\n",
    "y_reg_tr, y_cls_tr, y_thr_tr = y_reg[tr_idx], y_cls[tr_idx], y_thr[tr_idx]\n",
    "\n",
    "X_f_te, X_50_te, X_200_te, X_500_te = X_f[te_idx], X_50[te_idx], X_200[te_idx], X_500[te_idx]\n",
    "y_reg_te, y_cls_te, y_thr_te = y_reg[te_idx], y_cls[te_idx], y_thr[te_idx]\n",
    "\n",
    "print(f\"Train: {len(X_f_tr)}, Test: {len(X_f_te)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GÃœÃ‡LENDIRILMIÅ MODEL\n",
    "n_f = X_f_tr.shape[1]\n",
    "\n",
    "inp_f = layers.Input((n_f,), name='features')\n",
    "inp_50 = layers.Input((50, 1), name='seq50')\n",
    "inp_200 = layers.Input((200, 1), name='seq200')\n",
    "inp_500 = layers.Input((500, 1), name='seq500')\n",
    "\n",
    "# N-BEATS (gÃ¼Ã§lendirilmiÅŸ)\n",
    "def nbeats(x, units, blocks, name):\n",
    "    for i in range(blocks):\n",
    "        x = layers.Dense(units, activation='relu')(x)\n",
    "        x = layers.Dropout(0.15)(x)\n",
    "    return x\n",
    "\n",
    "nb_s = layers.Flatten()(inp_50)\n",
    "nb_s = nbeats(nb_s, 64, 5, 's')\n",
    "nb_s = layers.Dense(64, activation='relu')(nb_s)\n",
    "\n",
    "nb_m = layers.Flatten()(inp_200)\n",
    "nb_m = nbeats(nb_m, 128, 7, 'm')\n",
    "nb_m = layers.Dense(128, activation='relu')(nb_m)\n",
    "\n",
    "nb_l = layers.Flatten()(inp_500)\n",
    "nb_l = nbeats(nb_l, 256, 9, 'l')\n",
    "nb_l = layers.Dense(256, activation='relu')(nb_l)\n",
    "\n",
    "nb_all = layers.Concatenate()([nb_s, nb_m, nb_l])\n",
    "\n",
    "# TCN (gÃ¼Ã§lendirilmiÅŸ)\n",
    "def tcn_blk(x, f, d, n):\n",
    "    c = layers.Conv1D(f, 3, dilation_rate=d, padding='causal', activation='relu')(x)\n",
    "    c = layers.Dropout(0.15)(c)\n",
    "    r = layers.Conv1D(f, 1, padding='same')(x) if x.shape[-1] != f else x\n",
    "    return layers.Add()([c, r])\n",
    "\n",
    "tcn = inp_500\n",
    "tcn = tcn_blk(tcn, 64, 1, '1')\n",
    "tcn = tcn_blk(tcn, 64, 2, '2')\n",
    "tcn = tcn_blk(tcn, 128, 4, '3')\n",
    "tcn = tcn_blk(tcn, 128, 8, '4')\n",
    "tcn = tcn_blk(tcn, 256, 16, '5')\n",
    "tcn = tcn_blk(tcn, 256, 32, '6')\n",
    "tcn = tcn_blk(tcn, 256, 64, '7')\n",
    "tcn = tcn_blk(tcn, 512, 128, '8')\n",
    "tcn = layers.GlobalAveragePooling1D()(tcn)\n",
    "tcn = layers.Dense(512, activation='relu')(tcn)\n",
    "\n",
    "# Fusion\n",
    "fus = layers.Concatenate()([inp_f, nb_all, tcn])\n",
    "fus = layers.Dense(512, activation='relu')(fus)\n",
    "fus = layers.Dropout(0.25)(fus)\n",
    "fus = layers.Dense(256, activation='relu')(fus)\n",
    "fus = layers.Dropout(0.25)(fus)\n",
    "fus = layers.Dense(128, activation='relu')(fus)\n",
    "\n",
    "# Outputs\n",
    "out_reg = layers.Dense(1, activation='linear', name='regression')(layers.Dense(64, activation='relu')(fus))\n",
    "out_cls = layers.Dense(3, activation='softmax', name='classification')(layers.Dense(64, activation='relu')(fus))\n",
    "out_thr = layers.Dense(1, activation='sigmoid', name='threshold')(layers.Dense(32, activation='relu')(fus))\n",
    "\n",
    "model = models.Model([inp_f, inp_50, inp_200, inp_500], [out_reg, out_cls, out_thr])\n",
    "print(f\"âœ… Model oluÅŸturuldu: {model.count_params():,} parametre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOCAL LOSS\n",
    "def focal_loss(gamma=2.0, alpha=0.75):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        return -K.mean(alpha * K.pow(1 - pt, gamma) * K.log(pt))\n",
    "    return loss\n",
    "\n",
    "# ULTRA AGGRESSIVE LOSS\n",
    "def ultra_loss(y_true, y_pred):\n",
    "    mae = K.abs(y_true - y_pred)\n",
    "    crit = K.cast(K.logical_and(y_true >= 1.4, y_true <= 1.6), 'float32') * 20.0\n",
    "    wrong = K.cast(K.not_equal(K.cast(y_true >= 1.5, 'float32'), K.cast(y_pred >= 1.5, 'float32')), 'float32') * 10.0\n",
    "    w = K.maximum(K.maximum(crit, wrong), 1.0)\n",
    "    return K.mean(mae * w)\n",
    "\n",
    "# Class weights\n",
    "c0 = (y_thr_tr == 0).sum()\n",
    "c1 = (y_thr_tr == 1).sum()\n",
    "w0 = (len(y_thr_tr) / (2 * c0)) * 2.5\n",
    "w1 = len(y_thr_tr) / (2 * c1)\n",
    "print(f\"\\nClass weights: 1.5 altÄ±={w0:.2f}, 1.5 Ã¼stÃ¼={w1:.2f}\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(0.0005),\n",
    "    loss={'regression': ultra_loss, 'classification': 'categorical_crossentropy', 'threshold': focal_loss()},\n",
    "    loss_weights={'regression': 0.3, 'classification': 0.2, 'threshold': 0.5},\n",
    "    metrics={'regression': ['mae'], 'classification': ['accuracy'], 'threshold': ['accuracy']}\n",
    ")\n",
    "print(\"âœ… Model compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS\n",
    "class MetricsCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:\n",
    "            p = self.model.predict([X_f_tr[:500], X_50_tr[:500], X_200_tr[:500], X_500_tr[:500]], verbose=0)[2].flatten()\n",
    "            pb = (p >= 0.5).astype(int)\n",
    "            tb = y_thr_tr[:500].astype(int)\n",
    "            below = tb == 0\n",
    "            above = tb == 1\n",
    "            ba = (pb[below] == tb[below]).mean() if below.sum() > 0 else 0\n",
    "            aa = (pb[above] == tb[above]).mean() if above.sum() > 0 else 0\n",
    "            print(f\"\\nEpoch {epoch+1}: 1.5 ALTI={ba*100:.1f}%, ÃœSTÃœ={aa*100:.1f}%\")\n",
    "\n",
    "cb = [\n",
    "    callbacks.ModelCheckpoint('jetx_best.h5', monitor='val_threshold_accuracy', save_best_only=True, mode='max', verbose=1),\n",
    "    callbacks.EarlyStopping(monitor='val_threshold_accuracy', patience=40, mode='max', restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7),\n",
    "    MetricsCallback()\n",
    "]\n",
    "print(\"âœ… Callbacks hazÄ±r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "print(\"ğŸš€ EÄŸitim baÅŸlÄ±yor: 300 epoch, batch=16, patience=40\\n\")\n",
    "\n",
    "hist = model.fit(\n",
    "    [X_f_tr, X_50_tr, X_200_tr, X_500_tr],\n",
    "    {'regression': y_reg_tr, 'classification': y_cls_tr, 'threshold': y_thr_tr},\n",
    "    epochs=300,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=cb,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… EÄŸitim tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "pred = model.predict([X_f_te, X_50_te, X_200_te, X_500_te], verbose=0)\n",
    "p_reg = pred[0].flatten()\n",
    "p_cls = pred[1]\n",
    "p_thr = pred[2].flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_reg_te, p_reg)\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_te, p_reg))\n",
    "\n",
    "thr_true = (y_reg_te >= 1.5).astype(int)\n",
    "thr_pred = (p_thr >= 0.5).astype(int)\n",
    "thr_acc = accuracy_score(thr_true, thr_pred)\n",
    "\n",
    "below_mask = thr_true == 0\n",
    "above_mask = thr_true == 1\n",
    "below_acc = accuracy_score(thr_true[below_mask], thr_pred[below_mask])\n",
    "above_acc = accuracy_score(thr_true[above_mask], thr_pred[above_mask])\n",
    "\n",
    "cls_true = np.argmax(y_cls_te, axis=1)\n",
    "cls_pred = np.argmax(p_cls, axis=1)\n",
    "cls_acc = accuracy_score(cls_true, cls_pred)\n",
    "\n",
    "cm = confusion_matrix(thr_true, thr_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SETÄ° SONUÃ‡LARI\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n",
    "print(f\"\\nğŸ”´ 1.5 ALTI DoÄŸruluk: {below_acc*100:.2f}% (Hedef: 70%+)\")\n",
    "if below_acc >= 0.70:\n",
    "    print(\"âœ… âœ… âœ… HEDEF BAÅARILI! Para kaybÄ± riski minimize edildi!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Hedefin altÄ±nda - daha fazla eÄŸitim gerekebilir\")\n",
    "print(f\"\\nğŸŸ¢ 1.5 ÃœSTÃœ DoÄŸruluk: {above_acc*100:.2f}%\")\n",
    "print(f\"ğŸ“Š Genel Threshold Acc: {thr_acc*100:.2f}%\")\n",
    "print(f\"ğŸ“ Kategori Acc: {cls_acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ CONFUSION MATRIX:\")\n",
    "print(f\"DoÄŸru 1.5 altÄ±: {cm[0,0]} | YanlÄ±ÅŸ 1.5 Ã¼stÃ¼: {cm[0,1]} âš ï¸\")\n",
    "print(f\"YanlÄ±ÅŸ 1.5 altÄ±: {cm[1,0]} | DoÄŸru 1.5 Ã¼stÃ¼: {cm[1,1]}\")\n",
    "\n",
    "if cm[0,0] + cm[0,1] > 0:\n",
    "    fpr = cm[0,1] / (cm[0,0] + cm[0,1])\n",
    "    print(f\"\\nâš ï¸ PARA KAYBI RÄ°SKÄ°: {fpr*100:.1f}% (1.5 altÄ±yken yanlÄ±ÅŸ Ã¼stÃ¼ tahmini)\")\n",
    "    if fpr < 0.30:\n",
    "        print(\"âœ… Ä°yi! Para kaybÄ± riski dÃ¼ÅŸÃ¼k\")\n",
    "    else:\n",
    "        print(\"âŒ YÃ¼ksek risk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAYDET & Ä°NDÄ°R\n",
    "model.save('jetx_model.h5')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "import json\n",
    "info = {\n",
    "    'model': 'AGGRESSIVE_NBEATS_TCN',\n",
    "    'params': int(model.count_params()),\n",
    "    'epochs': len(hist.history['loss']),\n",
    "    'threshold_acc': float(thr_acc),\n",
    "    'below_15_acc': float(below_acc),\n",
    "    'above_15_acc': float(above_acc),\n",
    "    'class_acc': float(cls_acc),\n",
    "    'mae': float(mae)\n",
    "}\n",
    "\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(info, f, indent=2)\n",
    "\n",
    "print(\"âœ… Dosyalar kaydedildi\")\n",
    "print(json.dumps(info, indent=2))\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('jetx_model.h5')\n",
    "    files.download('scaler.pkl')\n",
    "    files.download('model_info.json')\n",
    "    print(\"\\nâœ… Dosyalar indirildi!\")\n",
    "except:\n",
    "    print(\"\\nâš ï¸ Colab dÄ±ÅŸÄ±nda - dosyalar sadece kaydedildi\")\n",
    "\n",
    "if below_acc >= 0.70:\n",
    "    print(\"\\nğŸ‰ Model hazÄ±r! 1.5 altÄ± tahminlerinde hedefi aÅŸtÄ±!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Model daha fazla eÄŸitim gerektirebilir\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
