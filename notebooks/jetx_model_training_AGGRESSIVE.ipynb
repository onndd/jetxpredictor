{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎰 JetX AGGRESSIVE Model - Para Kaybı Önleyici\n",
    "\n",
    "**⚠️ 1.5x ALTI TAHMİNLERİNDE YÜKSEK DOĞRULUK İÇİN OPTİMİZE EDİLDİ**\n",
    "\n",
    "Bu notebook Google Colab'da çalışır ve agresif eğitim ile 1.5 altı tahminlerinde maksimum performans sağlar.\n",
    "\n",
    "## Özellikler\n",
    "- ✅ Class Imbalance çözümü (1.5 altı için 2.5x ağırlık)\n",
    "- ✅ Focal Loss (yanlış tahminlere ekstra ceza)\n",
    "- ✅ Güçlendirilmiş model (2x katman)\n",
    "- ✅ 300 epoch agresif eğitim\n",
    "- ✅ Detaylı metrik tracking\n",
    "- ✅ Colab optimizasyonları\n",
    "\n",
    "## Hedefler\n",
    "- 1.5 ALTI Doğruluk: **70%+** ✅\n",
    "- 1.5 ÜSTÜ Doğruluk: 75%+\n",
    "- Genel Accuracy: 80%+\n",
    "\n",
    "**Süre:** ~2-3 saat (GPU ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kütüphaneleri yükle\n",
    "!pip install -q tensorflow scikit-learn pandas numpy scipy joblib matplotlib seaborn tqdm\n",
    "\n",
    "import sys, os, sqlite3, numpy as np, pandas as pd, joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub'dan klonla\n",
    "if not os.path.exists('jetxpredictor'):\n",
    "    !git clone https://github.com/onndd/jetxpredictor.git\n",
    "%cd jetxpredictor\n",
    "sys.path.append('/content/jetxpredictor')\n",
    "\n",
    "from category_definitions import CategoryDefinitions, FeatureEngineering\n",
    "print(f\"✅ Proje yüklendi - Kritik eşik: {CategoryDefinitions.CRITICAL_THRESHOLD}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veritabanından veri yükle\n",
    "conn = sqlite3.connect('jetx_data.db')\n",
    "data = pd.read_sql_query(\"SELECT value FROM jetx_results ORDER BY id\", conn)\n",
    "conn.close()\n",
    "\n",
    "all_values = data['value'].values\n",
    "print(f\"✅ {len(all_values)} veri yüklendi\")\n",
    "print(f\"Aralık: {all_values.min():.2f}x - {all_values.max():.2f}x\")\n",
    "\n",
    "below = (all_values < 1.5).sum()\n",
    "above = (all_values >= 1.5).sum()\n",
    "print(f\"\\n🔴 CLASS IMBALANCE:\")\n",
    "print(f\"1.5 altı: {below} ({below/len(all_values)*100:.1f}%)\")\n",
    "print(f\"1.5 üstü: {above} ({above/len(all_values)*100:.1f}%)\")\n",
    "print(f\"Dengesizlik: 1:{above/below:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "window_size = 500\n",
    "X_f, X_50, X_200, X_500 = [], [], [], []\n",
    "y_reg, y_cls, y_thr = [], [], []\n",
    "\n",
    "for i in tqdm(range(window_size, len(all_values)-1), desc='Features'):\n",
    "    hist = all_values[:i].tolist()\n",
    "    target = all_values[i]\n",
    "    \n",
    "    feats = FeatureEngineering.extract_all_features(hist)\n",
    "    X_f.append(list(feats.values()))\n",
    "    X_50.append(all_values[i-50:i])\n",
    "    X_200.append(all_values[i-200:i])\n",
    "    X_500.append(all_values[i-500:i])\n",
    "    \n",
    "    y_reg.append(target)\n",
    "    cat = CategoryDefinitions.get_category_numeric(target)\n",
    "    onehot = np.zeros(3)\n",
    "    onehot[cat] = 1\n",
    "    y_cls.append(onehot)\n",
    "    y_thr.append(1.0 if target >= 1.5 else 0.0)\n",
    "\n",
    "X_f = np.array(X_f)\n",
    "X_50 = np.array(X_50).reshape(-1, 50, 1)\n",
    "X_200 = np.array(X_200).reshape(-1, 200, 1)\n",
    "X_500 = np.array(X_500).reshape(-1, 500, 1)\n",
    "y_reg = np.array(y_reg)\n",
    "y_cls = np.array(y_cls)\n",
    "y_thr = np.array(y_thr)\n",
    "\n",
    "print(f\"✅ {len(X_f)} örnek hazırlandı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizasyon\n",
    "scaler = StandardScaler()\n",
    "X_f = scaler.fit_transform(X_f)\n",
    "X_50 = np.log10(X_50 + 1e-8)\n",
    "X_200 = np.log10(X_200 + 1e-8)\n",
    "X_500 = np.log10(X_500 + 1e-8)\n",
    "\n",
    "# Split\n",
    "idx = np.arange(len(X_f))\n",
    "tr_idx, te_idx = train_test_split(idx, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_f_tr, X_50_tr, X_200_tr, X_500_tr = X_f[tr_idx], X_50[tr_idx], X_200[tr_idx], X_500[tr_idx]\n",
    "y_reg_tr, y_cls_tr, y_thr_tr = y_reg[tr_idx], y_cls[tr_idx], y_thr[tr_idx]\n",
    "\n",
    "X_f_te, X_50_te, X_200_te, X_500_te = X_f[te_idx], X_50[te_idx], X_200[te_idx], X_500[te_idx]\n",
    "y_reg_te, y_cls_te, y_thr_te = y_reg[te_idx], y_cls[te_idx], y_thr[te_idx]\n",
    "\n",
    "print(f\"Train: {len(X_f_tr)}, Test: {len(X_f_te)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GÜÇLENDIRILMIŞ MODEL\n",
    "n_f = X_f_tr.shape[1]\n",
    "\n",
    "inp_f = layers.Input((n_f,), name='features')\n",
    "inp_50 = layers.Input((50, 1), name='seq50')\n",
    "inp_200 = layers.Input((200, 1), name='seq200')\n",
    "inp_500 = layers.Input((500, 1), name='seq500')\n",
    "\n",
    "# N-BEATS (güçlendirilmiş)\n",
    "def nbeats(x, units, blocks, name):\n",
    "    for i in range(blocks):\n",
    "        x = layers.Dense(units, activation='relu')(x)\n",
    "        x = layers.Dropout(0.15)(x)\n",
    "    return x\n",
    "\n",
    "nb_s = layers.Flatten()(inp_50)\n",
    "nb_s = nbeats(nb_s, 64, 5, 's')\n",
    "nb_s = layers.Dense(64, activation='relu')(nb_s)\n",
    "\n",
    "nb_m = layers.Flatten()(inp_200)\n",
    "nb_m = nbeats(nb_m, 128, 7, 'm')\n",
    "nb_m = layers.Dense(128, activation='relu')(nb_m)\n",
    "\n",
    "nb_l = layers.Flatten()(inp_500)\n",
    "nb_l = nbeats(nb_l, 256, 9, 'l')\n",
    "nb_l = layers.Dense(256, activation='relu')(nb_l)\n",
    "\n",
    "nb_all = layers.Concatenate()([nb_s, nb_m, nb_l])\n",
    "\n",
    "# TCN (güçlendirilmiş)\n",
    "def tcn_blk(x, f, d, n):\n",
    "    c = layers.Conv1D(f, 3, dilation_rate=d, padding='causal', activation='relu')(x)\n",
    "    c = layers.Dropout(0.15)(c)\n",
    "    r = layers.Conv1D(f, 1, padding='same')(x) if x.shape[-1] != f else x\n",
    "    return layers.Add()([c, r])\n",
    "\n",
    "tcn = inp_500\n",
    "tcn = tcn_blk(tcn, 64, 1, '1')\n",
    "tcn = tcn_blk(tcn, 64, 2, '2')\n",
    "tcn = tcn_blk(tcn, 128, 4, '3')\n",
    "tcn = tcn_blk(tcn, 128, 8, '4')\n",
    "tcn = tcn_blk(tcn, 256, 16, '5')\n",
    "tcn = tcn_blk(tcn, 256, 32, '6')\n",
    "tcn = tcn_blk(tcn, 256, 64, '7')\n",
    "tcn = tcn_blk(tcn, 512, 128, '8')\n",
    "tcn = layers.GlobalAveragePooling1D()(tcn)\n",
    "tcn = layers.Dense(512, activation='relu')(tcn)\n",
    "\n",
    "# Fusion\n",
    "fus = layers.Concatenate()([inp_f, nb_all, tcn])\n",
    "fus = layers.Dense(512, activation='relu')(fus)\n",
    "fus = layers.Dropout(0.25)(fus)\n",
    "fus = layers.Dense(256, activation='relu')(fus)\n",
    "fus = layers.Dropout(0.25)(fus)\n",
    "fus = layers.Dense(128, activation='relu')(fus)\n",
    "\n",
    "# Outputs\n",
    "out_reg = layers.Dense(1, activation='linear', name='regression')(layers.Dense(64, activation='relu')(fus))\n",
    "out_cls = layers.Dense(3, activation='softmax', name='classification')(layers.Dense(64, activation='relu')(fus))\n",
    "out_thr = layers.Dense(1, activation='sigmoid', name='threshold')(layers.Dense(32, activation='relu')(fus))\n",
    "\n",
    "model = models.Model([inp_f, inp_50, inp_200, inp_500], [out_reg, out_cls, out_thr])\n",
    "print(f\"✅ Model oluşturuldu: {model.count_params():,} parametre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOCAL LOSS\n",
    "def focal_loss(gamma=2.0, alpha=0.75):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        return -K.mean(alpha * K.pow(1 - pt, gamma) * K.log(pt))\n",
    "    return loss\n",
    "\n",
    "# ULTRA AGGRESSIVE LOSS\n",
    "def ultra_loss(y_true, y_pred):\n",
    "    mae = K.abs(y_true - y_pred)\n",
    "    crit = K.cast(K.logical_and(y_true >= 1.4, y_true <= 1.6), 'float32') * 20.0\n",
    "    wrong = K.cast(K.not_equal(K.cast(y_true >= 1.5, 'float32'), K.cast(y_pred >= 1.5, 'float32')), 'float32') * 10.0\n",
    "    w = K.maximum(K.maximum(crit, wrong), 1.0)\n",
    "    return K.mean(mae * w)\n",
    "\n",
    "# Class weights\n",
    "c0 = (y_thr_tr == 0).sum()\n",
    "c1 = (y_thr_tr == 1).sum()\n",
    "w0 = (len(y_thr_tr) / (2 * c0)) * 2.5\n",
    "w1 = len(y_thr_tr) / (2 * c1)\n",
    "print(f\"\\nClass weights: 1.5 altı={w0:.2f}, 1.5 üstü={w1:.2f}\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(0.0005),\n",
    "    loss={'regression': ultra_loss, 'classification': 'categorical_crossentropy', 'threshold': focal_loss()},\n",
    "    loss_weights={'regression': 0.3, 'classification': 0.2, 'threshold': 0.5},\n",
    "    metrics={'regression': ['mae'], 'classification': ['accuracy'], 'threshold': ['accuracy']}\n",
    ")\n",
    "print(\"✅ Model compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS\n",
    "class MetricsCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:\n",
    "            p = self.model.predict([X_f_tr[:500], X_50_tr[:500], X_200_tr[:500], X_500_tr[:500]], verbose=0)[2].flatten()\n",
    "            pb = (p >= 0.5).astype(int)\n",
    "            tb = y_thr_tr[:500].astype(int)\n",
    "            below = tb == 0\n",
    "            above = tb == 1\n",
    "            ba = (pb[below] == tb[below]).mean() if below.sum() > 0 else 0\n",
    "            aa = (pb[above] == tb[above]).mean() if above.sum() > 0 else 0\n",
    "            print(f\"\\nEpoch {epoch+1}: 1.5 ALTI={ba*100:.1f}%, ÜSTÜ={aa*100:.1f}%\")\n",
    "\n",
    "cb = [\n",
    "    callbacks.ModelCheckpoint('jetx_best.h5', monitor='val_threshold_accuracy', save_best_only=True, mode='max', verbose=1),\n",
    "    callbacks.EarlyStopping(monitor='val_threshold_accuracy', patience=40, mode='max', restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7),\n",
    "    MetricsCallback()\n",
    "]\n",
    "print(\"✅ Callbacks hazır\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "print(\"🚀 Eğitim başlıyor: 300 epoch, batch=16, patience=40\\n\")\n",
    "\n",
    "hist = model.fit(\n",
    "    [X_f_tr, X_50_tr, X_200_tr, X_500_tr],\n",
    "    {'regression': y_reg_tr, 'classification': y_cls_tr, 'threshold': y_thr_tr},\n",
    "    epochs=300,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=cb,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Eğitim tamamlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "pred = model.predict([X_f_te, X_50_te, X_200_te, X_500_te], verbose=0)\n",
    "p_reg = pred[0].flatten()\n",
    "p_cls = pred[1]\n",
    "p_thr = pred[2].flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_reg_te, p_reg)\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_te, p_reg))\n",
    "\n",
    "thr_true = (y_reg_te >= 1.5).astype(int)\n",
    "thr_pred = (p_thr >= 0.5).astype(int)\n",
    "thr_acc = accuracy_score(thr_true, thr_pred)\n",
    "\n",
    "below_mask = thr_true == 0\n",
    "above_mask = thr_true == 1\n",
    "below_acc = accuracy_score(thr_true[below_mask], thr_pred[below_mask])\n",
    "above_acc = accuracy_score(thr_true[above_mask], thr_pred[above_mask])\n",
    "\n",
    "cls_true = np.argmax(y_cls_te, axis=1)\n",
    "cls_pred = np.argmax(p_cls, axis=1)\n",
    "cls_acc = accuracy_score(cls_true, cls_pred)\n",
    "\n",
    "cm = confusion_matrix(thr_true, thr_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SETİ SONUÇLARI\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n",
    "print(f\"\\n🔴 1.5 ALTI Doğruluk: {below_acc*100:.2f}% (Hedef: 70%+)\")\n",
    "if below_acc >= 0.70:\n",
    "    print(\"✅ ✅ ✅ HEDEF BAŞARILI! Para kaybı riski minimize edildi!\")\n",
    "else:\n",
    "    print(\"⚠️ Hedefin altında - daha fazla eğitim gerekebilir\")\n",
    "print(f\"\\n🟢 1.5 ÜSTÜ Doğruluk: {above_acc*100:.2f}%\")\n",
    "print(f\"📊 Genel Threshold Acc: {thr_acc*100:.2f}%\")\n",
    "print(f\"📁 Kategori Acc: {cls_acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n📋 CONFUSION MATRIX:\")\n",
    "print(f\"Doğru 1.5 altı: {cm[0,0]} | Yanlış 1.5 üstü: {cm[0,1]} ⚠️\")\n",
    "print(f\"Yanlış 1.5 altı: {cm[1,0]} | Doğru 1.5 üstü: {cm[1,1]}\")\n",
    "\n",
    "if cm[0,0] + cm[0,1] > 0:\n",
    "    fpr = cm[0,1] / (cm[0,0] + cm[0,1])\n",
    "    print(f\"\\n⚠️ PARA KAYBI RİSKİ: {fpr*100:.1f}% (1.5 altıyken yanlış üstü tahmini)\")\n",
    "    if fpr < 0.30:\n",
    "        print(\"✅ İyi! Para kaybı riski düşük\")\n",
    "    else:\n",
    "        print(\"❌ Yüksek risk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAYDET & İNDİR\n",
    "model.save('jetx_model.h5')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "import json\n",
    "info = {\n",
    "    'model': 'AGGRESSIVE_NBEATS_TCN',\n",
    "    'params': int(model.count_params()),\n",
    "    'epochs': len(hist.history['loss']),\n",
    "    'threshold_acc': float(thr_acc),\n",
    "    'below_15_acc': float(below_acc),\n",
    "    'above_15_acc': float(above_acc),\n",
    "    'class_acc': float(cls_acc),\n",
    "    'mae': float(mae)\n",
    "}\n",
    "\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(info, f, indent=2)\n",
    "\n",
    "print(\"✅ Dosyalar kaydedildi\")\n",
    "print(json.dumps(info, indent=2))\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('jetx_model.h5')\n",
    "    files.download('scaler.pkl')\n",
    "    files.download('model_info.json')\n",
    "    print(\"\\n✅ Dosyalar indirildi!\")\n",
    "except:\n",
    "    print(\"\\n⚠️ Colab dışında - dosyalar sadece kaydedildi\")\n",
    "\n",
    "if below_acc >= 0.70:\n",
    "    print(\"\\n🎉 Model hazır! 1.5 altı tahminlerinde hedefi aştı!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Model daha fazla eğitim gerektirebilir\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
