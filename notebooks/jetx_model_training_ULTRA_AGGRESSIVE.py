#!/usr/bin/env python3
"""
ğŸ”¥ JetX ULTRA AGGRESSIVE Model - 1.5 ALTI TAHMÄ°N MAKÄ°NESÄ°

âš¡ PARA KAYBINI Ã–NLEMEK Ä°Ã‡Ä°N TASARLANDI

HEDEFLER:
- 1.5 ALTI DoÄŸruluk: %80+** (kritik!)
- 1.5 ÃœSTÃœ DoÄŸruluk: %75+
- Para KaybÄ± Riski: %15 altÄ±
- Genel Accuracy: %80+

ULTRA AGGRESSIVE Ä°YÄ°LEÅTÄ°RMELER:
- âœ… 1000 EPOCH (300'den 3.3x artÄ±ÅŸ)
- âœ… Batch size: 4 (16'dan 4x azaltma)
- âœ… Class weight: 10x (1.5 altÄ± iÃ§in 2.5x'ten 4x artÄ±ÅŸ)
- âœ… Focal loss gamma: 5.0 (2.0'dan 2.5x artÄ±ÅŸ)
- âœ… Threshold Killer Loss (100x ceza)
- âœ… Model derinliÄŸi: 2-3x artÄ±ÅŸ
- âœ… Learning rate schedule
- âœ… Patience: 100 (40'tan 2.5x artÄ±ÅŸ)

SÃ¼re: ~3-5 saat (GPU ile) - BU NORMAL VE GEREKLÄ°!
"""

# KÃ¼tÃ¼phaneleri yÃ¼kle
import subprocess
import sys

print("ğŸ“¦ KÃ¼tÃ¼phaneler yÃ¼kleniyor...")
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", 
                      "tensorflow", "scikit-learn", "pandas", "numpy", 
                      "scipy", "joblib", "matplotlib", "seaborn", "tqdm"])

import os
import sqlite3
import numpy as np
import pandas as pd
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, backend as K
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

print(f"TensorFlow: {tf.__version__}")
print(f"GPU: {'âœ… MEVCUT' if len(tf.config.list_physical_devices('GPU')) > 0 else 'âŒ YOK (CPU ile 10x daha uzun sÃ¼rer)'}")

# =============================================================================
# PROJE YÃœKLE
# =============================================================================
if not os.path.exists('jetxpredictor'):
    print("ğŸ“¥ Proje klonlanÄ±yor...")
    subprocess.check_call(["git", "clone", "https://github.com/onndd/jetxpredictor.git"])

os.chdir('jetxpredictor')
sys.path.append(os.getcwd())

from category_definitions import CategoryDefinitions, FeatureEngineering
print(f"âœ… Proje yÃ¼klendi - Kritik eÅŸik: {CategoryDefinitions.CRITICAL_THRESHOLD}x")

# =============================================================================
# VERÄ° YÃœKLE
# =============================================================================
print("\nğŸ“Š Veri yÃ¼kleniyor...")
conn = sqlite3.connect('jetx_data.db')
data = pd.read_sql_query("SELECT value FROM jetx_results ORDER BY id", conn)
conn.close()

all_values = data['value'].values
print(f"âœ… {len(all_values)} veri yÃ¼klendi")
print(f"AralÄ±k: {all_values.min():.2f}x - {all_values.max():.2f}x")

below = (all_values < 1.5).sum()
above = (all_values >= 1.5).sum()
print(f"\nğŸ”´ CLASS IMBALANCE:")
print(f"1.5 altÄ±: {below} ({below/len(all_values)*100:.1f}%)")
print(f"1.5 Ã¼stÃ¼: {above} ({above/len(all_values)*100:.1f}%)")
print(f"Dengesizlik oranÄ±: 1:{above/below:.2f}")
print(f"\nâš¡ Bu dengesizlik 10x class weight ile Ã§Ã¶zÃ¼lecek!")

# =============================================================================
# FEATURE ENGINEERING
# =============================================================================
print("\nğŸ”§ Feature extraction baÅŸlÄ±yor...")
window_size = 500
X_f, X_50, X_200, X_500 = [], [], [], []
y_reg, y_cls, y_thr = [], [], []

for i in tqdm(range(window_size, len(all_values)-1), desc='Features'):
    hist = all_values[:i].tolist()
    target = all_values[i]
    
    feats = FeatureEngineering.extract_all_features(hist)
    X_f.append(list(feats.values()))
    X_50.append(all_values[i-50:i])
    X_200.append(all_values[i-200:i])
    X_500.append(all_values[i-500:i])
    
    y_reg.append(target)
    cat = CategoryDefinitions.get_category_numeric(target)
    onehot = np.zeros(3)
    onehot[cat] = 1
    y_cls.append(onehot)
    y_thr.append(1.0 if target >= 1.5 else 0.0)

X_f = np.array(X_f)
X_50 = np.array(X_50).reshape(-1, 50, 1)
X_200 = np.array(X_200).reshape(-1, 200, 1)
X_500 = np.array(X_500).reshape(-1, 500, 1)
y_reg = np.array(y_reg)
y_cls = np.array(y_cls)
y_thr = np.array(y_thr)

print(f"âœ… {len(X_f)} Ã¶rnek hazÄ±rlandÄ±")
print(f"Feature sayÄ±sÄ±: {X_f.shape[1]}")

# =============================================================================
# NORMALÄ°ZASYON VE SPLIT
# =============================================================================
print("\nğŸ“Š Normalizasyon ve split...")
scaler = StandardScaler()
X_f = scaler.fit_transform(X_f)
X_50 = np.log10(X_50 + 1e-8)
X_200 = np.log10(X_200 + 1e-8)
X_500 = np.log10(X_500 + 1e-8)

idx = np.arange(len(X_f))
tr_idx, te_idx = train_test_split(idx, test_size=0.2, shuffle=False)

X_f_tr, X_50_tr, X_200_tr, X_500_tr = X_f[tr_idx], X_50[tr_idx], X_200[tr_idx], X_500[tr_idx]
y_reg_tr, y_cls_tr, y_thr_tr = y_reg[tr_idx], y_cls[tr_idx], y_thr[tr_idx]

X_f_te, X_50_te, X_200_te, X_500_te = X_f[te_idx], X_50[te_idx], X_200[te_idx], X_500[te_idx]
y_reg_te, y_cls_te, y_thr_te = y_reg[te_idx], y_cls[te_idx], y_thr[te_idx]

# Shape dÃ¼zeltmesi: (N,) -> (N, 1) binary classification iÃ§in
y_thr_tr = y_thr_tr.reshape(-1, 1)
y_thr_te = y_thr_te.reshape(-1, 1)

print(f"Train: {len(X_f_tr)}, Test: {len(X_f_te)}")
print(f"âœ… Veri hazÄ±r")

# =============================================================================
# ULTRA DEEP MODEL - 2-3X DERÄ°NLÄ°K
# =============================================================================
print("\nğŸ—ï¸ Ultra deep model oluÅŸturuluyor...")
n_f = X_f_tr.shape[1]

inp_f = layers.Input((n_f,), name='features')
inp_50 = layers.Input((50, 1), name='seq50')
inp_200 = layers.Input((200, 1), name='seq200')
inp_500 = layers.Input((500, 1), name='seq500')

# N-BEATS (ULTRA DERÄ°N)
def ultra_nbeats(x, units, blocks, name):
    for i in range(blocks):
        x = layers.Dense(units, activation='relu', kernel_regularizer='l2')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
    return x

# KÄ±sa sequence (50)
nb_s = layers.Flatten()(inp_50)
nb_s = ultra_nbeats(nb_s, 128, 7, 's')  # 64->128, 5->7
nb_s = layers.Dense(128, activation='relu')(nb_s)
nb_s = layers.Dropout(0.2)(nb_s)

# Orta sequence (200)
nb_m = layers.Flatten()(inp_200)
nb_m = ultra_nbeats(nb_m, 256, 10, 'm')  # 128->256, 7->10
nb_m = layers.Dense(256, activation='relu')(nb_m)
nb_m = layers.Dropout(0.2)(nb_m)

# Uzun sequence (500)
nb_l = layers.Flatten()(inp_500)
nb_l = ultra_nbeats(nb_l, 512, 12, 'l')  # 256->512, 9->12
nb_l = layers.Dense(512, activation='relu')(nb_l)
nb_l = layers.Dropout(0.2)(nb_l)

nb_all = layers.Concatenate()([nb_s, nb_m, nb_l])

# TCN (ULTRA DERÄ°N)
def ultra_tcn_block(x, filters, dilation, name):
    conv = layers.Conv1D(filters, 3, dilation_rate=dilation, padding='causal', 
                        activation='relu', kernel_regularizer='l2')(x)
    conv = layers.BatchNormalization()(conv)
    conv = layers.Dropout(0.2)(conv)
    residual = layers.Conv1D(filters, 1, padding='same')(x) if x.shape[-1] != filters else x
    return layers.Add()([conv, residual])

tcn = inp_500
tcn = ultra_tcn_block(tcn, 128, 1, '1')
tcn = ultra_tcn_block(tcn, 128, 2, '2')
tcn = ultra_tcn_block(tcn, 256, 4, '3')
tcn = ultra_tcn_block(tcn, 256, 8, '4')
tcn = ultra_tcn_block(tcn, 512, 16, '5')
tcn = ultra_tcn_block(tcn, 512, 32, '6')
tcn = ultra_tcn_block(tcn, 512, 64, '7')
tcn = ultra_tcn_block(tcn, 1024, 128, '8')  # YENÄ°
tcn = ultra_tcn_block(tcn, 1024, 256, '9')  # YENÄ°
tcn = layers.GlobalAveragePooling1D()(tcn)
tcn = layers.Dense(1024, activation='relu')(tcn)
tcn = layers.Dropout(0.25)(tcn)

# MEGA FUSION (3x daha derin)
fus = layers.Concatenate()([inp_f, nb_all, tcn])
fus = layers.Dense(1024, activation='relu', kernel_regularizer='l2')(fus)
fus = layers.BatchNormalization()(fus)
fus = layers.Dropout(0.3)(fus)
fus = layers.Dense(512, activation='relu')(fus)
fus = layers.BatchNormalization()(fus)
fus = layers.Dropout(0.3)(fus)
fus = layers.Dense(256, activation='relu')(fus)
fus = layers.Dropout(0.25)(fus)
fus = layers.Dense(128, activation='relu')(fus)
fus = layers.Dropout(0.2)(fus)

# OUTPUTS (dedicated branches)
reg_branch = layers.Dense(128, activation='relu')(fus)
reg_branch = layers.Dropout(0.2)(reg_branch)
out_reg = layers.Dense(1, activation='linear', name='regression')(reg_branch)

cls_branch = layers.Dense(128, activation='relu')(fus)
cls_branch = layers.Dropout(0.2)(cls_branch)
out_cls = layers.Dense(3, activation='softmax', name='classification')(cls_branch)

thr_branch = layers.Dense(64, activation='relu')(fus)
thr_branch = layers.Dropout(0.2)(thr_branch)
thr_branch = layers.Dense(32, activation='relu')(thr_branch)
out_thr = layers.Dense(1, activation='sigmoid', name='threshold')(thr_branch)

model = models.Model([inp_f, inp_50, inp_200, inp_500], [out_reg, out_cls, out_thr])
print(f"âœ… ULTRA DEEP Model: {model.count_params():,} parametre (eski: ~2M)")

# =============================================================================
# THRESHOLD KILLER LOSS - 100X CEZA!
# =============================================================================
def threshold_killer_loss(y_true, y_pred):
    """1.5 altÄ± yanlÄ±ÅŸ tahmine Ã‡OK BÃœYÃœK CEZA"""
    mae = K.abs(y_true - y_pred)
    
    # 1.5 altÄ±yken Ã¼stÃ¼ tahmin = 12x ceza (PARA KAYBI!) - 3. Tur: 15â†’12 (Dengeli)
    false_positive = K.cast(
        tf.logical_and(y_true < 1.5, y_pred >= 1.5),
        'float32'
    ) * 12.0
    
    # 1.5 Ã¼stÃ¼yken altÄ± tahmin = 6x ceza - 3. Tur: 8â†’6 (Dengeli)
    false_negative = K.cast(
        tf.logical_and(y_true >= 1.5, y_pred < 1.5),
        'float32'
    ) * 6.0
    
    # Kritik bÃ¶lge (1.4-1.6) = 10x ceza - 3. Tur: 12â†’10 (Hassas BÃ¶lge)
    critical_zone = K.cast(
        tf.logical_and(y_true >= 1.4, y_true <= 1.6),
        'float32'
    ) * 10.0
    
    weight = K.maximum(K.maximum(false_positive, false_negative), critical_zone)
    weight = K.maximum(weight, 1.0)
    
    return K.mean(mae * weight)

# ULTRA FOCAL LOSS - gamma=5.0 (Ã§ok agresif!)
def ultra_focal_loss(gamma=3.0, alpha=0.85):
    """Focal loss - 2. Tur: gamma 5.0â†’3.0 (daha yumuÅŸak)"""
    def loss(y_true, y_pred):
        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())
        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        focal_weight = alpha * K.pow(1 - pt, gamma)
        return -K.mean(focal_weight * K.log(pt))
    return loss

def create_weighted_binary_crossentropy(weight_0, weight_1):
    """
    SÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ±nÄ± doÄŸrudan iÃ§eren weighted binary crossentropy loss fonksiyonu
    
    Args:
        weight_0: 1.5 altÄ± (class 0) iÃ§in aÄŸÄ±rlÄ±k
        weight_1: 1.5 Ã¼stÃ¼ (class 1) iÃ§in aÄŸÄ±rlÄ±k
    
    Returns:
        AÄŸÄ±rlÄ±klÄ± binary crossentropy loss fonksiyonu
    """
    def loss(y_true, y_pred):
        # Binary crossentropy hesapla
        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())
        bce = -(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))
        
        # Class weight'leri uygula
        # y_true = 1 ise weight_1, y_true = 0 ise weight_0 kullan
        weights = y_true * weight_1 + (1 - y_true) * weight_0
        
        # AÄŸÄ±rlÄ±klÄ± loss'u dÃ¶ndÃ¼r
        return K.mean(bce * weights)
    
    return loss

# CLASS WEIGHTS - 7X (1.5 altÄ± iÃ§in!) - LAZY LEARNING'Ä° Ã–NLEMEK Ä°Ã‡Ä°N
# y_thr_tr shape (N, 1) olduÄŸu iÃ§in flatten etmeliyiz
c0 = (y_thr_tr.flatten() == 0).sum()
c1 = (y_thr_tr.flatten() == 1).sum()
TARGET_MULTIPLIER = 7.0  # 3. Tur: 2.5 â†’ 7.0 (180% artÄ±ÅŸ - azÄ±nlÄ±k sÄ±nÄ±fÄ±na odaklanma)
w0 = (len(y_thr_tr) / (2 * c0)) * TARGET_MULTIPLIER
w1 = len(y_thr_tr) / (2 * c1)

print(f"\nğŸ¯ CLASS WEIGHTS:")
print(f"1.5 altÄ± (0): {w0:.2f}x (eski: ~2.5x, Ã¶nceki: ~14.7x)")
print(f"1.5 Ã¼stÃ¼ (1): {w1:.2f}x")
print(f"\nâš¡ 1.5 altÄ± Ã¶rnekler {w0:.1f}x daha Ã¶nemli (dengeli hale getirildi!)")

# LEARNING RATE SCHEDULE - DÃ¼ÅŸÃ¼rÃ¼ldÃ¼ ve Ã¶ne Ã§ekildi
initial_lr = 0.00005  # 2. Tur: 0.0001 â†’ 0.00005 (50% azalma, daha hassas)
def lr_schedule(epoch, lr):
    if epoch < 50:    # Ã–ne Ã§ekildi: 200 â†’ 50
        return initial_lr
    elif epoch < 150: # Ã–ne Ã§ekildi: 500 â†’ 150
        return initial_lr * 0.5
    elif epoch < 300: # Ã–ne Ã§ekildi: 800 â†’ 300
        return initial_lr * 0.1
    else:
        return initial_lr * 0.05

# COMPILE - WEIGHTED BCE Ä°LE LAZY LEARNING Ã–NLENDÄ°
model.compile(
    optimizer=Adam(initial_lr),
    loss={
        'regression': threshold_killer_loss,
        'classification': 'categorical_crossentropy',
        'threshold': create_weighted_binary_crossentropy(w0, w1)
    },
    loss_weights={
        'regression': 0.25,
        'classification': 0.15,
        'threshold': 0.60  # 0.5 -> 0.6
    },
    metrics={
        'regression': ['mae'],
        'classification': ['accuracy'],
        'threshold': ['accuracy', 'binary_crossentropy']
    }
)

print("\nâœ… Model compiled (4. DÃ¼zeltme - Weighted BCE ile Lazy Learning Ã–nlendi):")
print(f"- Threshold Killer Loss (12x ceza - dengeli)")
print(f"- Weighted Binary Crossentropy (class weight doÄŸrudan entegre)")
print(f"- Class weight: {w0:.1f}x (azÄ±nlÄ±k sÄ±nÄ±fÄ±na odaklanma)")
print(f"- Initial LR: {initial_lr} (hassas Ã¶ÄŸrenme)")

# =============================================================================
# ULTRA CALLBACKS
# =============================================================================
class UltraMetricsCallback(callbacks.Callback):
    def __init__(self):
        super().__init__()
        self.best_below_acc = 0
        
    def on_epoch_end(self, epoch, logs=None):
        if epoch % 5 == 0:
            p = self.model.predict([X_f_tr[:1000], X_50_tr[:1000], X_200_tr[:1000], X_500_tr[:1000]], verbose=0)[2].flatten()
            pb = (p >= 0.5).astype(int)
            tb = y_thr_tr[:1000].flatten().astype(int)
            
            below_mask = tb == 0
            above_mask = tb == 1
            
            below_acc = (pb[below_mask] == tb[below_mask]).mean() if below_mask.sum() > 0 else 0
            above_acc = (pb[above_mask] == tb[above_mask]).mean() if above_mask.sum() > 0 else 0
            
            false_positive = ((pb == 1) & (tb == 0)).sum()
            total_below = below_mask.sum()
            risk = false_positive / total_below if total_below > 0 else 0
            
            print(f"\nğŸ“Š Epoch {epoch+1}:")
            print(f"  ğŸ”´ 1.5 ALTI: {below_acc*100:.1f}% (Hedef: 80%+)")
            print(f"  ğŸŸ¢ 1.5 ÃœSTÃœ: {above_acc*100:.1f}%")
            print(f"  ğŸ’° Para kaybÄ± riski: {risk*100:.1f}% (Hedef: <15%)")
            
            if below_acc > self.best_below_acc:
                self.best_below_acc = below_acc
                print(f"  âœ¨ YENÄ° REKOR! En iyi 1.5 altÄ±: {below_acc*100:.1f}%")

ultra_metrics = UltraMetricsCallback()

cb = [
    callbacks.ModelCheckpoint(
        'jetx_ultra_best.h5',
        monitor='val_threshold_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    callbacks.EarlyStopping(
        monitor='val_threshold_accuracy',
        patience=100,  # 40 -> 100
        mode='max',
        restore_best_weights=True,
        verbose=1
    ),
    callbacks.LearningRateScheduler(lr_schedule, verbose=1),
    callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=10,  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 20 â†’ 10 (daha hÄ±zlÄ± tepki)
        min_lr=1e-8,
        verbose=1
    ),
    ultra_metrics
]

print("âœ… Ultra callbacks hazÄ±r:")
print(f"- Patience: 100 epoch (eski: 40)")
print(f"- LR schedule: 0.0001 -> 0.000005 (dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ ve Ã¶ne Ã§ekildi)")
print(f"- ReduceLR patience: 10 (dÃ¼ÅŸÃ¼rÃ¼ldÃ¼)")
print(f"- Custom metrics tracking")

# =============================================================================
# ULTRA AGGRESSIVE TRAINING - 1000 EPOCH!
# =============================================================================
print("\n" + "="*70)
print("ğŸ”¥ ULTRA AGGRESSIVE TRAINING BAÅLIYOR!")
print("="*70)
print(f"Epochs: 1000 (eski: 300)")
print(f"Batch size: 4 (eski: 16) - Ã‡ok yavaÅŸ ama Ã§ok iyi!")
print(f"Patience: 100 (eski: 40)")
print(f"Class weight: {w0:.1f}x (eski: 2.5x)")
print(f"Focal gamma: 3.0 (yumuÅŸak, dengeli)")
print(f"\nâ±ï¸ BEKLENEN SÃœRE: 3-5 saat (GPU ile)")
print(f"ğŸ’¡ Model 5 dakikada bitiyorsa bir sorun var!")
print("="*70 + "\n")

hist = model.fit(
    [X_f_tr, X_50_tr, X_200_tr, X_500_tr],
    {
        'regression': y_reg_tr, 
        'classification': y_cls_tr, 
        'threshold': y_thr_tr
    },
    epochs=1000,  # 300 -> 1000
    batch_size=4,  # 16 -> 4
    validation_split=0.2,
    callbacks=cb,
    verbose=1
)

print("\nâœ… EÄŸitim tamamlandÄ±!")
print(f"Toplam epoch: {len(hist.history['loss'])}")

# =============================================================================
# DETAYLI EVALUATION
# =============================================================================
print("\n" + "="*70)
print("ğŸ“Š TEST SETÄ° DEÄERLENDÄ°RMESÄ°")
print("="*70)

pred = model.predict([X_f_te, X_50_te, X_200_te, X_500_te], verbose=0)
p_reg = pred[0].flatten()
p_cls = pred[1]
p_thr = pred[2].flatten()

mae = mean_absolute_error(y_reg_te, p_reg)
rmse = np.sqrt(mean_squared_error(y_reg_te, p_reg))

thr_true = (y_reg_te >= 1.5).astype(int)
thr_pred = (p_thr >= 0.5).astype(int)
thr_acc = accuracy_score(thr_true, thr_pred)

below_mask = thr_true == 0
above_mask = thr_true == 1
below_acc = accuracy_score(thr_true[below_mask], thr_pred[below_mask]) if below_mask.sum() > 0 else 0
above_acc = accuracy_score(thr_true[above_mask], thr_pred[above_mask]) if above_mask.sum() > 0 else 0

cls_true = np.argmax(y_cls_te, axis=1)
cls_pred = np.argmax(p_cls, axis=1)
cls_acc = accuracy_score(cls_true, cls_pred)

cm = confusion_matrix(thr_true, thr_pred)

print(f"\nğŸ“ˆ REGRESSION:")
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")

print(f"\nğŸ¯ THRESHOLD (1.5x):")
print(f"Genel Accuracy: {thr_acc*100:.2f}%")

print(f"\nğŸ”´ 1.5 ALTI (KRÄ°TÄ°K!):")
print(f"DoÄŸruluk: {below_acc*100:.2f}%")
if below_acc >= 0.80:
    print("âœ… âœ… âœ… HEDEF AÅILDI! Para kaybÄ± riski minimize edildi!")
elif below_acc >= 0.75:
    print("âœ… âœ… Hedefin Ã§ok yakÄ±nÄ±nda!")
elif below_acc >= 0.70:
    print("âœ… Ä°yi ama hedefin altÄ±nda")
else:
    print("âš ï¸ Hedefin altÄ±nda - daha fazla eÄŸitim gerekebilir")

print(f"\nğŸŸ¢ 1.5 ÃœSTÃœ:")
print(f"DoÄŸruluk: {above_acc*100:.2f}%")

print(f"\nğŸ“ KATEGORÄ° CLASSIFICATION:")
print(f"Accuracy: {cls_acc*100:.2f}%")

print(f"\nğŸ“‹ CONFUSION MATRIX:")
print(f"                  Tahmin")
print(f"GerÃ§ek    1.5 AltÄ± | 1.5 ÃœstÃ¼")
print(f"1.5 AltÄ±  {cm[0,0]:6d}   | {cm[0,1]:6d}  âš ï¸ PARA KAYBI")
print(f"1.5 ÃœstÃ¼  {cm[1,0]:6d}   | {cm[1,1]:6d}")

if cm[0,0] + cm[0,1] > 0:
    fpr = cm[0,1] / (cm[0,0] + cm[0,1])
    print(f"\nğŸ’° PARA KAYBI RÄ°SKÄ°: {fpr*100:.1f}%")
    if fpr < 0.15:
        print("âœ… âœ… âœ… MÃœKEMMEL! Risk %15 altÄ±nda!")
    elif fpr < 0.20:
        print("âœ… âœ… Ä°YÄ°! Risk dÃ¼ÅŸÃ¼k")
    elif fpr < 0.30:
        print("âœ… Kabul edilebilir")
    else:
        print("âŒ YÃ¼ksek risk!")

print(f"\nğŸ“Š DETAYLI RAPOR:")
print(classification_report(thr_true, thr_pred, target_names=['1.5 AltÄ±', '1.5 ÃœstÃ¼']))

# =============================================================================
# KAYDET & Ä°NDÄ°R
# =============================================================================
print("\nğŸ’¾ Model ve dosyalar kaydediliyor...")

model.save('jetx_ultra_model.h5')
joblib.dump(scaler, 'scaler_ultra.pkl')

import json
info = {
    'model': 'ULTRA_AGGRESSIVE_NBEATS_TCN',
    'version': '2.0_ULTRA',
    'params': int(model.count_params()),
    'total_epochs': len(hist.history['loss']),
    'batch_size': 4,
    'class_weight_below_15': float(w0),
    'focal_gamma': 5.0,
    'metrics': {
        'threshold_accuracy': float(thr_acc),
        'below_15_accuracy': float(below_acc),
        'above_15_accuracy': float(above_acc),
        'class_accuracy': float(cls_acc),
        'mae': float(mae),
        'rmse': float(rmse),
        'money_loss_risk': float(fpr) if cm[0,0] + cm[0,1] > 0 else 0.0
    },
    'training_config': {
        'epochs': 1000,
        'batch_size': 4,
        'patience': 100,
        'initial_lr': float(initial_lr),  # 0.00005 (hassas Ã¶ÄŸrenme iÃ§in dÃ¼ÅŸÃ¼k LR)
        'class_weight': f'{w0:.1f}x'
    }
}

with open('ultra_model_info.json', 'w') as f:
    json.dump(info, f, indent=2)

print("âœ… Dosyalar kaydedildi:")
print("- jetx_ultra_model.h5")
print("- scaler_ultra.pkl")
print("- ultra_model_info.json")

print(f"\nğŸ“Š Model Bilgisi:")
print(json.dumps(info, indent=2))

# Google Colab'da indir
try:
    from google.colab import files
    files.download('jetx_ultra_model.h5')
    files.download('scaler_ultra.pkl')
    files.download('ultra_model_info.json')
    print("\nâœ… Dosyalar indirildi!")
except:
    print("\nâš ï¸ Colab dÄ±ÅŸÄ±nda - dosyalar sadece kaydedildi")

# Final deÄŸerlendirme
print("\n" + "="*70)
print("ğŸ‰ ULTRA AGGRESSIVE MODEL TAMAMLANDI!")
print("="*70)

if below_acc >= 0.80 and fpr < 0.15:
    print("âœ… âœ… âœ… TÃœM HEDEFLER AÅILDI!")
    print(f"1.5 ALTI: {below_acc*100:.1f}% (Hedef: 80%+)")
    print(f"Para kaybÄ±: {fpr*100:.1f}% (Hedef: <15%)")
    print("\nğŸš€ Model artÄ±k 1.5 altÄ± tahmin yapabilir!")
elif below_acc >= 0.75:
    print("âœ… âœ… Hedefin Ã§ok yakÄ±nÄ±nda!")
    print("Biraz daha eÄŸitim ile hedefi aÅŸabilir")
elif below_acc >= 0.70:
    print("âœ… Ä°yi performans ama hedefin altÄ±nda")
    print("Ã–neriler:")
    print("- Daha fazla veri toplayÄ±n")
    print("- Epoch sayÄ±sÄ±nÄ± artÄ±rÄ±n (1500-2000)")
    print("- Batch size'Ä± 2'ye dÃ¼ÅŸÃ¼rÃ¼n")
else:
    print("âš ï¸ Hedefin altÄ±nda")
    print("Model daha fazla eÄŸitime ihtiyaÃ§ duyabilir")

print("\nğŸ“ Sonraki adÄ±m:")
print("Bu dosyalarÄ± lokal projenize ekleyin ve tahminlere baÅŸlayÄ±n!")
print("="*70)
