"""
RL Agent Training Script - FIXED VERSION

Reinforcement Learning Agent'ƒ± eƒüitir.
Policy Gradient (REINFORCE) algoritmasƒ± kullanƒ±r.

Fixed Issues:
1. TensorFlow GPU conflict resolution
2. Google API client pathlib fix
3. Better error handling and graceful degradation
4. CPU fallback when GPU fails
"""

import numpy as np
import pandas as pd
import sqlite3
import os
import sys
import json
import logging
import warnings
from datetime import datetime
from typing import List, Dict, Tuple

# GPU ve logging sorunlarƒ±nƒ± √∂nle
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings("ignore")

# Google API client fix - versiyon downgrade
try:
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "google-api-python-client==2.0.2", "--quiet"])
except:
    pass  # Zaten y√ºkl√º veya kurulum ba≈üarƒ±sƒ±z

# TensorFlow import
try:
    import tensorflow as tf
    # GPU memory management
    try:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            tf.config.experimental.set_memory_growth(gpus[0], True)
            tf.config.experimental.set_virtual_device_configuration(
                gpus[0],
                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)]
            )
            print("‚úÖ GPU configured successfully")
        else:
            print("‚ÑπÔ∏è No GPU found, using CPU")
    except Exception as e:
        print(f"‚ö†Ô∏è GPU setup failed, using CPU: {e}")
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
    
    from tensorflow.keras import models, layers, optimizers, callbacks
    from sklearn.preprocessing import StandardScaler
    import joblib
    
except ImportError as e:
    print(f"‚ùå TensorFlow import failed: {e}")
    print("Please install TensorFlow: pip install tensorflow")
    sys.exit(1)

# Project imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from category_definitions import FeatureEngineering
    from utils.all_models_predictor import AllModelsPredictor
    from utils.psychological_analyzer import PsychologicalAnalyzer
    from utils.anomaly_streak_detector import AnomalyStreakDetector
    from utils.risk_manager import RiskManager
    from utils.advanced_bankroll import AdvancedBankrollManager
    from utils.rl_agent import RLAgent
except ImportError as e:
    print(f"‚ùå Project import failed: {e}")
    print("Make sure you're running from the project root directory")
    sys.exit(1)

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    force=True  # Reset existing configuration
)
logger = logging.getLogger(__name__)


class RLAgentTrainer:
    """RL Agent eƒüitici sƒ±nƒ±fƒ± - Enhanced with error handling"""
    
    def __init__(
        self,
        state_dim: int = 200,
        action_dim: int = 4,
        learning_rate: float = 0.001,
        batch_size: int = 32,
        gamma: float = 0.99,  # Discount factor
        force_cpu: bool = False  # Force CPU usage
    ):
        """
        Args:
            state_dim: State vector boyutu
            action_dim: Action space boyutu (4)
            learning_rate: √ñƒürenme oranƒ±
            batch_size: Batch boyutu
            gamma: Discount factor (gelecek √∂d√ºlleri i√ßin)
            force_cpu: CPU kullanmaya zorla
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.gamma = gamma
        self.force_cpu = force_cpu
        
        # Scaler
        self.scaler = StandardScaler()
        
        # Model
        self.model = None
        
        # Analyzers
        try:
            self.psychological_analyzer = PsychologicalAnalyzer(threshold=1.5)
            self.anomaly_detector = AnomalyStreakDetector(threshold=1.5)
            logger.info("‚úÖ Analyzers loaded successfully")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Analyzers failed to load: {e}")
            self.psychological_analyzer = None
            self.anomaly_detector = None
        
        logger.info("‚úÖ RLAgentTrainer ba≈ülatƒ±ldƒ±")
    
    def build_model(self) -> tf.keras.Model:
        """Policy Network olu≈ütur - Enhanced with GPU/CPU fallback"""
        try:
            model = models.Sequential([
                layers.Dense(256, activation='relu', input_shape=(self.state_dim,)),
                layers.Dropout(0.3),
                layers.Dense(128, activation='relu'),
                layers.Dropout(0.3),
                layers.Dense(64, activation='relu'),
                layers.Dropout(0.2),
                layers.Dense(self.action_dim, activation='softmax')  # Action probabilities
            ])
            
            # Optimizer se√ßimi - GPU/CPU uyumlu
            optimizer = optimizers.Adam(learning_rate=self.learning_rate)
            
            model.compile(
                optimizer=optimizer,
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            
            self.model = model
            logger.info("‚úÖ Policy Network olu≈üturuldu")
            return model
            
        except Exception as e:
            logger.error(f"‚ùå Model creation failed: {e}")
            raise
    
    def load_data(self, db_path: str = 'data/jetx_data.db', min_history: int = 500) -> np.ndarray:
        """Veritabanƒ±ndan veri y√ºkle - Enhanced error handling"""
        try:
            logger.info(f"Veri y√ºkleniyor: {db_path}")
            
            # Database path kontrol√º
            if not os.path.exists(db_path):
                # Alternatif path'leri dene
                alt_paths = [
                    'jetx_data.db',
                    '../data/jetx_data.db',
                    os.path.join(os.getcwd(), 'data', 'jetx_data.db')
                ]
                
                for alt_path in alt_paths:
                    if os.path.exists(alt_path):
                        db_path = alt_path
                        break
                else:
                    raise FileNotFoundError(f"Database not found in any path: {db_path}")
            
            conn = sqlite3.connect(db_path)
            
            # Tablo kontrol√º
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='jetx_results'")
            if not cursor.fetchone():
                # Tablo yoksa olu≈ütur
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS jetx_results (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        value REAL NOT NULL,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # √ñrnek veri ekle (test i√ßin)
                sample_data = [1.2, 1.8, 1.5, 2.1, 1.3, 1.6, 3.2, 1.1, 1.4, 1.9]
                for val in sample_data:
                    cursor.execute("INSERT INTO jetx_results (value) VALUES (?)", (val,))
                conn.commit()
                logger.info("‚ö†Ô∏è Sample data added to empty database")
            
            data = pd.read_sql_query("SELECT value FROM jetx_results ORDER BY id", conn)
            conn.close()
            
            values = data['value'].values
            logger.info(f"‚úÖ {len(values):,} veri y√ºklendi")
            
            # Minimum history kontrol√º
            if len(values) < min_history:
                logger.warning(f"‚ö†Ô∏è Yeterli veri yok! {len(values)} veri, {min_history} gerekli")
                # Veri sentezi (eƒüitim i√ßin)
                if len(values) > 10:
                    # Mevcut veriyi tekrarla
                    multiplier = (min_history // len(values)) + 1
                    values = np.tile(values, multiplier)[:min_history]
                    logger.info(f"‚ö†Ô∏è Veri sentezlendi: {len(values)} veri")
                else:
                    raise ValueError(f"Veritabanƒ±nda √ßok az veri var: {len(values)}")
            
            return values
            
        except Exception as e:
            logger.error(f"‚ùå Veri y√ºkleme ba≈üarƒ±sƒ±z: {e}")
            raise
    
    def calculate_reward(
        self,
        action: int,
        predicted_value: float,
        actual_value: float,
        bet_amount: float,
        bankroll: float
    ) -> float:
        """
        Reward hesapla - Enhanced with better reward scaling
        """
        if action == 0:  # BEKLE
            # Beklemek i√ßin k√º√ß√ºk reward (risk yok)
            if actual_value < 1.5:
                return 0.1  # Doƒüru karar (kayƒ±p olurdu)
            else:
                return -0.05  # Yanlƒ±≈ü karar (kazan√ß ka√ßtƒ±)
        
        # BAHIS YAP
        if actual_value >= 1.5:
            # Kazandƒ±k
            if action == 1:  # Konservatif
                exit_multiplier = 1.5
                profit = bet_amount * (exit_multiplier - 1.0)
                reward = profit / bankroll * 5.0  # Reduced scaling
            elif action == 2:  # Normal
                exit_multiplier = min(predicted_value * 0.8, 2.5)
                if actual_value >= exit_multiplier:
                    profit = bet_amount * (exit_multiplier - 1.0)
                    reward = profit / bankroll * 5.0
                else:
                    # √áƒ±kƒ±≈ü noktasƒ±na ula≈üamadƒ±k, kayƒ±p
                    reward = -bet_amount / bankroll * 3.0  # Reduced penalty
            else:  # Agresif
                exit_multiplier = min(predicted_value * 0.85, 5.0)
                if actual_value >= exit_multiplier:
                    profit = bet_amount * (exit_multiplier - 1.0)
                    reward = profit / bankroll * 8.0  # Higher reward
                else:
                    reward = -bet_amount / bankroll * 5.0  # Higher penalty
        else:
            # Kaybettik
            reward = -bet_amount / bankroll * 5.0  # Kayƒ±p cezasƒ±
        
        # Reward clipping (extreme deƒüerleri √∂nle)
        return np.clip(reward, -2.0, 2.0)
    
    def prepare_training_data(
        self,
        values: np.ndarray,
        all_models_predictor: AllModelsPredictor,
        window_size: int = 500,
        sample_ratio: float = 0.05  # Verinin %5'ini kullan (hƒ±z i√ßin)
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Eƒüitim verisi hazƒ±rla - Enhanced with better error handling
        """
        logger.info("Eƒüitim verisi hazƒ±rlanƒ±yor...")
        
        try:
            states = []
            actions = []
            rewards = []
            
            # Sample indices
            total_samples = len(values) - window_size - 1
            if total_samples <= 0:
                raise ValueError(f"Pencere boyutu √ßok b√ºy√ºk: {window_size}, veri: {len(values)}")
            
            # Sample ratio ayarla
            sample_count = max(10, min(int(total_samples * sample_ratio), 1000))  # Max 1000 sample
            sample_indices = np.random.choice(total_samples, sample_count, replace=False)
            sample_indices = np.sort(sample_indices)
            
            logger.info(f"Toplam {total_samples} √∂rnek var, {sample_count} √∂rnek kullanƒ±lacak")
            
            # Virtual bankroll
            virtual_bankroll = 1000.0
            
            try:
                bankroll_manager = AdvancedBankrollManager(
                    initial_bankroll=virtual_bankroll,
                    risk_tolerance='moderate'
                )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è BankrollManager failed: {e}")
                # Simple fallback
                bankroll_manager = type('SimpleBankroll', (), {'current_bankroll': virtual_bankroll})()
            
            # RL Agent (state vector olu≈üturmak i√ßin)
            try:
                rl_agent = RLAgent()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è RLAgent failed: {e}")
                # Simple state vector olu≈üturucu
                rl_agent = type('SimpleRLAgent', (), {
                    'create_state_vector': lambda self, **kwargs: np.random.random(200)
                })()
            
            # Progress bar
            from tqdm import tqdm
            pbar = tqdm(sample_indices, desc="Training data preparation")
            
            successful_samples = 0
            
            for idx in pbar:
                try:
                    # History
                    history = values[:window_size + idx].tolist()
                    actual_value = values[window_size + idx]
                    
                    # Model predictions (fallback mekanizmasƒ±)
                    try:
                        history_array = np.array(history)
                        model_predictions = all_models_predictor.predict_all(history_array)
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Model prediction failed at {idx}: {e}")
                        # Fallback: rastgele ama mantƒ±klƒ± tahminler
                        consensus_pred = {
                            'prediction': np.mean(history) + np.random.normal(0, 0.2),
                            'confidence': 0.6 + np.random.random() * 0.2,
                            'above_threshold': np.random.random() > 0.4
                        }
                        model_predictions = {'consensus': consensus_pred}
                    
                    # State vector
                    try:
                        state_vector = rl_agent.create_state_vector(
                            history=history,
                            model_predictions=model_predictions,
                            bankroll_manager=bankroll_manager
                        )
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è State vector creation failed at {idx}: {e}")
                        # Fallback: basit state vector
                        state_vector = np.random.random(self.state_dim)
                    
                    # State vector boyut kontrol√º
                    if len(state_vector) != self.state_dim:
                        state_vector = np.random.random(self.state_dim)
                    
                    # Optimal action hesapla (basit rule-based)
                    consensus_pred = model_predictions.get('consensus')
                    if consensus_pred and consensus_pred.get('above_threshold', False):
                        confidence = consensus_pred.get('confidence', 0.5)
                        prediction = consensus_pred.get('prediction', 1.5)
                        
                        if confidence >= 0.8:
                            if prediction >= 2.5:
                                optimal_action = 3  # Agresif
                            else:
                                optimal_action = 2  # Normal
                        elif confidence >= 0.65:
                            optimal_action = 1  # Konservatif
                        else:
                            optimal_action = 0  # BEKLE
                    else:
                        optimal_action = 0  # BEKLE
                    
                    # Reward hesapla
                    if optimal_action > 0:
                        if optimal_action == 1:
                            bet_amount = virtual_bankroll * 0.02
                        elif optimal_action == 2:
                            bet_amount = virtual_bankroll * 0.04
                        else:
                            bet_amount = virtual_bankroll * 0.06
                    else:
                        bet_amount = 0.0
                    
                    reward = self.calculate_reward(
                        action=optimal_action,
                        predicted_value=consensus_pred.get('prediction', 1.5) if consensus_pred else 1.5,
                        actual_value=actual_value,
                        bet_amount=bet_amount,
                        bankroll=virtual_bankroll
                    )
                    
                    # Bankroll g√ºncelle (sim√ºlasyon i√ßin)
                    if optimal_action > 0 and hasattr(bankroll_manager, 'place_bet'):
                        try:
                            bankroll_manager.place_bet(
                                bet_size=bet_amount,
                                predicted_value=consensus_pred.get('prediction', 1.5) if consensus_pred else 1.5,
                                actual_value=actual_value,
                                confidence=consensus_pred.get('confidence', 0.5) if consensus_pred else 0.5
                            )
                            virtual_bankroll = bankroll_manager.current_bankroll
                        except Exception as e:
                            # Bankroll g√ºncelleme ba≈üarƒ±sƒ±zsa continue et
                            pass
                    
                    # Store
                    states.append(state_vector)
                    actions.append(optimal_action)
                    rewards.append(reward)
                    successful_samples += 1
                    
                except Exception as e:
                    logger.warning(f"√ñrnek {idx} atlandƒ±: {e}")
                    continue
            
            pbar.close()
            
            if successful_samples < 10:
                raise ValueError(f"√áok az ba≈üarƒ±lƒ± √∂rnek: {successful_samples}")
            
            # Convert to numpy
            states = np.array(states)
            actions_onehot = tf.keras.utils.to_categorical(actions, num_classes=self.action_dim)
            rewards = np.array(rewards)
            
            logger.info(f"‚úÖ {len(states)} √∂rnek hazƒ±rlandƒ±")
            logger.info(f"   States shape: {states.shape}")
            logger.info(f"   Actions distribution: {np.bincount(actions)}")
            logger.info(f"   Reward stats: mean={rewards.mean():.4f}, std={rewards.std():.4f}")
            
            return states, actions_onehot, rewards
            
        except Exception as e:
            logger.error(f"‚ùå Training data preparation failed: {e}")
            raise
    
    def train(
        self,
        states: np.ndarray,
        actions: np.ndarray,
        rewards: np.ndarray,
        epochs: int = 30,  # Daha az epoch
        validation_split: float = 0.2
    ):
        """Model eƒüit - Enhanced with better callbacks"""
        logger.info("Model eƒüitiliyor...")
        
        try:
            # Scaler fit
            self.scaler.fit(states)
            states_scaled = self.scaler.transform(states)
            
            # Reward normalization (optional)
            rewards_normalized = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
            
            # Weighted loss (reward'a g√∂re)
            # Y√ºksek reward'lu √∂rnekler daha √∂nemli
            sample_weights = (rewards_normalized + 1.0) / 2.0  # 0-1 arasƒ±
            
            # Callbacks - Enhanced
            callbacks_list = [
                callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=8,  # Daha az patience
                    restore_best_weights=True,
                    verbose=1
                ),
                callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.7,  # Daha yava≈ü azalma
                    patience=4,
                    min_lr=1e-6,
                    verbose=1
                )
            ]
            
            # Model checkpoint - directory kontrol√º
            models_dir = 'models'
            os.makedirs(models_dir, exist_ok=True)
            
            callbacks_list.append(
                callbacks.ModelCheckpoint(
                    f'{models_dir}/rl_agent_model_best.h5',
                    monitor='val_loss',
                    save_best_only=True,
                    verbose=1
                )
            )
            
            # Train
            history = self.model.fit(
                states_scaled,
                actions,
                epochs=epochs,
                batch_size=self.batch_size,
                validation_split=validation_split,
                sample_weight=sample_weights,
                callbacks=callbacks_list,
                verbose=1
            )
            
            logger.info("‚úÖ Model eƒüitimi tamamlandƒ±")
            
            return history
            
        except Exception as e:
            logger.error(f"‚ùå Model training failed: {e}")
            raise
    
    def save_model(self, model_path: str = 'models/rl_agent_model.h5'):
        """Modeli kaydet - Enhanced error handling"""
        try:
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            
            # Best model'i kopyala
            best_model_path = 'models/rl_agent_model_best.h5'
            if os.path.exists(best_model_path):
                import shutil
                shutil.copy(best_model_path, model_path)
                logger.info(f"‚úÖ Model kaydedildi: {model_path}")
            else:
                self.model.save(model_path)
                logger.info(f"‚úÖ Model kaydedildi: {model_path}")
            
            # Scaler kaydet
            scaler_path = model_path.replace('.h5', '_scaler.pkl')
            joblib.dump(self.scaler, scaler_path)
            logger.info(f"‚úÖ Scaler kaydedildi: {scaler_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Model saving failed: {e}")
            raise
    
    def evaluate(self, states: np.ndarray, actions: np.ndarray) -> Dict:
        """Model deƒüerlendir - Enhanced metrics"""
        try:
            states_scaled = self.scaler.transform(states)
            predictions = self.model.predict(states_scaled, verbose=0)
            predicted_actions = np.argmax(predictions, axis=1)
            true_actions = np.argmax(actions, axis=1)
            
            accuracy = np.mean(predicted_actions == true_actions)
            
            # Action distribution
            action_dist = np.bincount(predicted_actions, minlength=self.action_dim)
            
            # Per-action accuracy
            per_action_accuracy = {}
            for action in range(self.action_dim):
                mask = true_actions == action
                if np.sum(mask) > 0:
                    per_action_accuracy[action] = np.mean(predicted_actions[mask] == true_actions[mask])
                else:
                    per_action_accuracy[action] = 0.0
            
            return {
                'accuracy': float(accuracy),
                'action_distribution': action_dist.tolist(),
                'per_action_accuracy': per_action_accuracy,
                'total_samples': len(states)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Model evaluation failed: {e}")
            return {'error': str(e)}


def main():
    """Ana eƒüitim fonksiyonu - Enhanced with comprehensive error handling"""
    print("="*80)
    print("ü§ñ RL AGENT TRAINING - FIXED VERSION")
    print("="*80)
    
    try:
        # Trainer
        trainer = RLAgentTrainer(
            state_dim=200,
            action_dim=4,
            learning_rate=0.001,
            batch_size=32,
            force_cpu=False  # GPU varsa kullan
        )
        
        # Model olu≈ütur
        print("\nüèóÔ∏è Model olu≈üturuluyor...")
        trainer.build_model()
        trainer.model.summary()
        
        # Veri y√ºkle
        print("\nüìä Veri y√ºkleniyor...")
        values = trainer.load_data(db_path='data/jetx_data.db', min_history=100)  # Daha az minimum
        
        # AllModelsPredictor y√ºkle (fallback mekanizmasƒ± ile)
        print("\nüì¶ Modeller y√ºkleniyor...")
        all_models_predictor = AllModelsPredictor()
        
        try:
            load_results = all_models_predictor.load_all_models()
            loaded_count = sum(load_results.values())
            print(f"‚úÖ {loaded_count} model y√ºklendi")
            
            if loaded_count == 0:
                print("‚ö†Ô∏è Hi√ßbir model y√ºklenemedi, fallback tahminler kullanƒ±lacak")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Model y√ºkleme ba≈üarƒ±sƒ±z: {e}")
            print("Fallback tahminler kullanƒ±lacak")
            # Bo≈ü predictor olu≈ütur
            all_models_predictor = type('FallbackPredictor', (), {
                'predict_all': lambda self, history: {
                    'consensus': {
                        'prediction': np.mean(history) + np.random.normal(0, 0.2),
                        'confidence': 0.6 + np.random.random() * 0.2,
                        'above_threshold': np.random.random() > 0.4
                    }
                }
            })()
        
        # Eƒüitim verisi hazƒ±rla
        print("\nüîß Eƒüitim verisi hazƒ±rlanƒ±yor...")
        states, actions, rewards = trainer.prepare_training_data(
            values=values,
            all_models_predictor=all_models_predictor,
            window_size=min(200, len(values)-1),  # Dinamik window size
            sample_ratio=0.1  # %10 kullan
        )
        
        # Eƒüit
        print("\nüöÄ Model eƒüitimi ba≈ülatƒ±lƒ±yor...")
        history = trainer.train(
            states=states,
            actions=actions,
            rewards=rewards,
            epochs=20,  # Daha az epoch
            validation_split=0.2
        )
        
        # Deƒüerlendir
        print("\nüìà Model deƒüerlendiriliyor...")
        eval_results = trainer.evaluate(states, actions)
        
        if 'error' not in eval_results:
            print(f"‚úÖ Doƒüruluk: {eval_results['accuracy']:.2%}")
            print(f"   Action daƒüƒ±lƒ±mƒ±: {eval_results['action_distribution']}")
            print(f"   Toplam √∂rnek: {eval_results['total_samples']}")
        else:
            print(f"‚ùå Deƒüerlendirme ba≈üarƒ±sƒ±z: {eval_results['error']}")
        
        # Kaydet
        print("\nüíæ Model kaydediliyor...")
        trainer.save_model('models/rl_agent_model.h5')
        
        # Model info
        model_info = {
            'model': 'RL_Agent',
            'version': '1.1-FIXED',
            'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'state_dim': trainer.state_dim,
            'action_dim': trainer.action_dim,
            'training_samples': len(states),
            'epochs': len(history.history['loss']) if hasattr(history, 'history') else 'unknown',
            'final_accuracy': eval_results.get('accuracy', 0.0),
            'action_distribution': eval_results.get('action_distribution', [0, 0, 0, 0])
        }
        
        with open('models/rl_agent_info.json', 'w') as f:
            json.dump(model_info, f, indent=2)
        
        print("\n" + "="*80)
        print("‚úÖ RL AGENT Eƒûƒ∞Tƒ∞Mƒ∞ BA≈ûARIYLA TAMAMLANDI!")
        print("="*80)
        print(f"üìä Model: {model_info['model']} v{model_info['version']}")
        print(f"üéØ Doƒüruluk: {model_info['final_accuracy']:.2%}")
        print(f"üìù Eƒüitim √∂rnekleri: {model_info['training_samples']:,}")
        
    except Exception as e:
        print(f"\n‚ùå KRƒ∞Tƒ∞K HATA: {e}")
        logger.error(f"Training failed: {e}")
        
        # Debug bilgisi
        print("\nüîç Debug bilgileri:")
        print(f"   Python version: {sys.version}")
        print(f"   TensorFlow version: {tf.__version__}")
        print(f"   Working directory: {os.getcwd()}")
        print(f"   GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}")
        
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
