#!/usr/bin/env python3
"""
RL Agent Training Script - FIXED VERSION

Reinforcement Learning Agent'Ä± eÄŸitir.
Policy Gradient (REINFORCE) algoritmasÄ± kullanÄ±r.

Fixed Issues:
1. TensorFlow GPU conflict resolution
2. Google API client pathlib fix
3. Better error handling and graceful degradation
4. CPU fallback when GPU fails
5. Data cleaning (String -> Float fix)

GÃœNCELLEME:
- 3 Mod -> 2 Mod (Normal/Rolling) yapÄ±sÄ±na geÃ§ildi.
- GÃ¼ven eÅŸikleri: Normal >= 0.85, Rolling >= 0.95.
"""

import numpy as np
import pandas as pd
import sqlite3
import os
import sys
import json
import logging
import warnings
from datetime import datetime
from typing import List, Dict, Tuple
from tqdm import tqdm

# GPU ve logging sorunlarÄ±nÄ± Ã¶nle
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings("ignore")

# Google API client fix - versiyon downgrade
try:
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "google-api-python-client==2.0.2", "--quiet"])
except:
    pass  # Zaten yÃ¼klÃ¼ veya kurulum baÅŸarÄ±sÄ±z

# TensorFlow import
try:
    import tensorflow as tf
    # GPU memory management
    try:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            tf.config.experimental.set_memory_growth(gpus[0], True)
            tf.config.experimental.set_virtual_device_configuration(
                gpus[0],
                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)]
            )
            print("âœ… GPU configured successfully")
        else:
            print("â„¹ï¸ No GPU found, using CPU")
    except Exception as e:
        print(f"âš ï¸ GPU setup failed, using CPU: {e}")
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
    
    from tensorflow.keras import models, layers, optimizers, callbacks
    from sklearn.preprocessing import StandardScaler
    import joblib
    
except ImportError as e:
    print(f"âŒ TensorFlow import failed: {e}")
    print("Please install TensorFlow: pip install tensorflow")
    sys.exit(1)

# Project imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from category_definitions import FeatureEngineering
    from utils.all_models_predictor import AllModelsPredictor
    from utils.psychological_analyzer import PsychologicalAnalyzer
    from utils.anomaly_streak_detector import AnomalyStreakDetector
    from utils.risk_manager import RiskManager
    from utils.advanced_bankroll import AdvancedBankrollManager
    from utils.rl_agent import RLAgent
except ImportError as e:
    print(f"âŒ Project import failed: {e}")
    print("Make sure you're running from the project root directory")
    sys.exit(1)

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    force=True  # Reset existing configuration
)
logger = logging.getLogger(__name__)


class RLAgentTrainer:
    """RL Agent eÄŸitici sÄ±nÄ±fÄ± - Enhanced with error handling"""
    
    def __init__(
        self,
        state_dim: int = 200,
        action_dim: int = 4, # 0: Bekle, 1: Rolling, 2: Normal, 3: Normal (Yedek)
        learning_rate: float = 0.001,
        batch_size: int = 32,
        gamma: float = 0.99,  # Discount factor
        force_cpu: bool = False  # Force CPU usage
    ):
        """
        Args:
            state_dim: State vector boyutu
            action_dim: Action space boyutu (4)
            learning_rate: Ã–ÄŸrenme oranÄ±
            batch_size: Batch boyutu
            gamma: Discount factor (gelecek Ã¶dÃ¼lleri iÃ§in)
            force_cpu: CPU kullanmaya zorla
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.gamma = gamma
        self.force_cpu = force_cpu
        
        # Scaler
        self.scaler = StandardScaler()
        
        # Model
        self.model = None
        
        # Analyzers
        try:
            self.psychological_analyzer = PsychologicalAnalyzer(threshold=1.5)
            self.anomaly_detector = AnomalyStreakDetector(threshold=1.5)
            logger.info("âœ… Analyzers loaded successfully")
        except Exception as e:
            logger.warning(f"âš ï¸ Analyzers failed to load: {e}")
            self.psychological_analyzer = None
            self.anomaly_detector = None
        
        logger.info("âœ… RLAgentTrainer baÅŸlatÄ±ldÄ±")
    
    def build_model(self) -> tf.keras.Model:
        """Policy Network oluÅŸtur - Enhanced with GPU/CPU fallback"""
        try:
            model = models.Sequential([
                layers.Dense(256, activation='relu', input_shape=(self.state_dim,)),
                layers.Dropout(0.3),
                layers.Dense(128, activation='relu'),
                layers.Dropout(0.3),
                layers.Dense(64, activation='relu'),
                layers.Dropout(0.2),
                layers.Dense(self.action_dim, activation='softmax')  # Action probabilities
            ])
            
            # Optimizer seÃ§imi - GPU/CPU uyumlu
            optimizer = optimizers.Adam(learning_rate=self.learning_rate)
            
            model.compile(
                optimizer=optimizer,
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            
            self.model = model
            logger.info("âœ… Policy Network oluÅŸturuldu")
            return model
            
        except Exception as e:
            logger.error(f"âŒ Model creation failed: {e}")
            raise
    
    def load_data(self, db_path: str = 'data/jetx_data.db', min_history: int = 500) -> np.ndarray:
        """VeritabanÄ±ndan veri yÃ¼kle - Enhanced error handling & Cleaning"""
        try:
            logger.info(f"Veri yÃ¼kleniyor: {db_path}")
            
            # Database path kontrolÃ¼
            if not os.path.exists(db_path):
                # Alternatif path'leri dene
                alt_paths = [
                    'jetx_data.db',
                    '../data/jetx_data.db',
                    os.path.join(os.getcwd(), 'data', 'jetx_data.db')
                ]
                
                for alt_path in alt_paths:
                    if os.path.exists(alt_path):
                        db_path = alt_path
                        break
                else:
                    raise FileNotFoundError(f"Database not found in any path: {db_path}")
            
            conn = sqlite3.connect(db_path)
            
            # Tablo kontrolÃ¼
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='jetx_results'")
            if not cursor.fetchone():
                # Tablo yoksa oluÅŸtur
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS jetx_results (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        value REAL NOT NULL,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # Ã–rnek veri ekle (test iÃ§in)
                sample_data = [1.2, 1.8, 1.5, 2.1, 1.3, 1.6, 3.2, 1.1, 1.4, 1.9]
                for val in sample_data:
                    cursor.execute("INSERT INTO jetx_results (value) VALUES (?)", (val,))
                conn.commit()
                logger.info("âš ï¸ Sample data added to empty database")
            
            data = pd.read_sql_query("SELECT value FROM jetx_results ORDER BY id", conn)
            conn.close()
            
            # --- VERÄ° TEMÄ°ZLEME (FIX) ---
            raw_values = data['value'].values
            cleaned_values = []
            for val in raw_values:
                try:
                    val_str = str(val).replace('\u2028', '').replace('\u2029', '').strip()
                    if ' ' in val_str: val_str = val_str.split()[0]
                    cleaned_values.append(float(val_str))
                except:
                    continue
            values = np.array(cleaned_values)
            # ----------------------------
            
            logger.info(f"âœ… {len(values):,} veri yÃ¼klendi ve temizlendi")
            
            # Minimum history kontrolÃ¼
            if len(values) < min_history:
                logger.warning(f"âš ï¸ Yeterli veri yok! {len(values)} veri, {min_history} gerekli")
                # Veri sentezi (eÄŸitim iÃ§in)
                if len(values) > 10:
                    # Mevcut veriyi tekrarla
                    multiplier = (min_history // len(values)) + 1
                    values = np.tile(values, multiplier)[:min_history]
                    logger.info(f"âš ï¸ Veri sentezlendi: {len(values)} veri")
                else:
                    raise ValueError(f"VeritabanÄ±nda Ã§ok az veri var: {len(values)}")
            
            return values
            
        except Exception as e:
            logger.error(f"âŒ Veri yÃ¼kleme baÅŸarÄ±sÄ±z: {e}")
            raise
    
    def calculate_reward(
        self,
        action: int,
        predicted_value: float,
        actual_value: float,
        bet_amount: float,
        bankroll: float
    ) -> float:
        """
        Reward hesapla - Enhanced with better reward scaling
        
        Args:
            action: SeÃ§ilen action (0: Bekle, 1: Rolling, 2: Normal, 3: Normal)
        """
        if action == 0:  # BEKLE
            # Beklemek iÃ§in kÃ¼Ã§Ã¼k reward (risk yok)
            if actual_value < 1.5:
                return 0.1  # DoÄŸru karar (kayÄ±p olurdu)
            else:
                return -0.05  # YanlÄ±ÅŸ karar (kazanÃ§ kaÃ§tÄ±)
        
        # BAHIS YAP
        if actual_value >= 1.5:
            # KazandÄ±k
            if action == 1:  # Rolling (GÃ¼venli Liman)
                exit_multiplier = 1.5
                profit = bet_amount * (exit_multiplier - 1.0)
                reward = profit / bankroll * 5.0  # Reduced scaling
                
            elif action == 2 or action == 3:  # Normal (Dengeli)
                exit_multiplier = min(predicted_value * 0.8, 2.5)
                if actual_value >= exit_multiplier:
                    profit = bet_amount * (exit_multiplier - 1.0)
                    reward = profit / bankroll * 8.0 # Daha yÃ¼ksek potansiyel Ã¶dÃ¼l
                else:
                    # Ã‡Ä±kÄ±ÅŸ noktasÄ±na ulaÅŸamadÄ±k, kayÄ±p
                    reward = -bet_amount / bankroll * 3.0  # Reduced penalty
            else:
                reward = 0
        else:
            # Kaybettik
            reward = -bet_amount / bankroll * 5.0  # KayÄ±p cezasÄ±
        
        # Reward clipping (extreme deÄŸerleri Ã¶nle)
        return np.clip(reward, -2.0, 2.0)
    
    def prepare_training_data(
        self,
        values: np.ndarray,
        all_models_predictor: AllModelsPredictor,
        window_size: int = 500,
        sample_ratio: float = 0.05  # Verinin %5'ini kullan (hÄ±z iÃ§in)
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        EÄŸitim verisi hazÄ±rla - Enhanced with better error handling
        """
        logger.info("EÄŸitim verisi hazÄ±rlanÄ±yor...")
        
        try:
            states = []
            actions = []
            rewards = []
            
            # Sample indices
            total_samples = len(values) - window_size - 1
            if total_samples <= 0:
                raise ValueError(f"Pencere boyutu Ã§ok bÃ¼yÃ¼k: {window_size}, veri: {len(values)}")
            
            # Sample ratio ayarla
            sample_count = max(10, min(int(total_samples * sample_ratio), 1000))  # Max 1000 sample
            sample_indices = np.random.choice(total_samples, sample_count, replace=False)
            sample_indices = np.sort(sample_indices)
            
            logger.info(f"Toplam {total_samples} Ã¶rnek var, {sample_count} Ã¶rnek kullanÄ±lacak")
            
            # Virtual bankroll
            virtual_bankroll = 1000.0
            
            try:
                bankroll_manager = AdvancedBankrollManager(
                    initial_bankroll=virtual_bankroll,
                    risk_tolerance='normal' # Ortak risk toleransÄ±
                )
            except Exception as e:
                logger.warning(f"âš ï¸ BankrollManager failed: {e}")
                # Simple fallback
                bankroll_manager = type('SimpleBankroll', (), {'current_bankroll': virtual_bankroll})()
            
            # RL Agent (state vector oluÅŸturmak iÃ§in)
            try:
                rl_agent = RLAgent()
            except Exception as e:
                logger.warning(f"âš ï¸ RLAgent failed: {e}")
                # Simple state vector oluÅŸturucu
                rl_agent = type('SimpleRLAgent', (), {
                    'create_state_vector': lambda self, **kwargs: np.random.random(200)
                })()
            
            # Progress bar
            pbar = tqdm(sample_indices, desc="Training data preparation")
            
            successful_samples = 0
            
            for idx in pbar:
                try:
                    # History
                    history = values[:window_size + idx].tolist()
                    actual_value = values[window_size + idx]
                    
                    # Model predictions (fallback mekanizmasÄ±)
                    try:
                        history_array = np.array(history)
                        model_predictions = all_models_predictor.predict_all(history_array)
                    except Exception as e:
                        logger.warning(f"âš ï¸ Model prediction failed at {idx}: {e}")
                        # Fallback: rastgele ama mantÄ±klÄ± tahminler
                        consensus_pred = {
                            'prediction': np.mean(history) + np.random.normal(0, 0.2),
                            'confidence': 0.6 + np.random.random() * 0.2,
                            'above_threshold': np.random.random() > 0.4
                        }
                        model_predictions = {'consensus': consensus_pred}
                    
                    # State vector
                    try:
                        state_vector = rl_agent.create_state_vector(
                            history=history,
                            model_predictions=model_predictions,
                            bankroll_manager=bankroll_manager
                        )
                    except Exception as e:
                        logger.warning(f"âš ï¸ State vector creation failed at {idx}: {e}")
                        # Fallback: basit state vector
                        state_vector = np.random.random(self.state_dim)
                    
                    # State vector boyut kontrolÃ¼
                    if len(state_vector) != self.state_dim:
                        state_vector = np.random.random(self.state_dim)
                    
                    # Optimal action hesapla (YENÄ° 2 MODLU YAPI)
                    consensus_pred = model_predictions.get('consensus')
                    
                    optimal_action = 0 # VarsayÄ±lan: BEKLE

                    if consensus_pred and consensus_pred.get('above_threshold', False):
                        confidence = consensus_pred.get('confidence', 0.5)
                        
                        # Yeni EÅŸikler: Rolling >= 0.95, Normal >= 0.85
                        if confidence >= 0.95:
                            optimal_action = 1  # ROLLING (En gÃ¼venli)
                        elif confidence >= 0.85:
                            optimal_action = 2  # NORMAL (YÃ¼ksek gÃ¼ven)
                        else:
                            optimal_action = 0  # BEKLE (GÃ¼ven yetersiz)
                    else:
                        optimal_action = 0  # BEKLE
                    
                    # Reward hesapla
                    bet_amount = 0.0
                    if optimal_action > 0:
                        if optimal_action == 1: # Rolling
                            bet_amount = virtual_bankroll * 0.02 # %2
                        elif optimal_action == 2 or optimal_action == 3: # Normal
                            bet_amount = virtual_bankroll * 0.04 # %4
                    
                    reward = self.calculate_reward(
                        action=optimal_action,
                        predicted_value=consensus_pred.get('prediction', 1.5) if consensus_pred else 1.5,
                        actual_value=actual_value,
                        bet_amount=bet_amount,
                        bankroll=virtual_bankroll
                    )
                    
                    # Bankroll gÃ¼ncelle (simÃ¼lasyon iÃ§in)
                    if optimal_action > 0 and hasattr(bankroll_manager, 'place_bet'):
                        try:
                            bankroll_manager.place_bet(
                                bet_size=bet_amount,
                                predicted_value=consensus_pred.get('prediction', 1.5) if consensus_pred else 1.5,
                                actual_value=actual_value,
                                confidence=consensus_pred.get('confidence', 0.5) if consensus_pred else 0.5
                            )
                            virtual_bankroll = bankroll_manager.current_bankroll
                        except Exception as e:
                            pass
                    
                    # Store
                    states.append(state_vector)
                    actions.append(optimal_action)
                    rewards.append(reward)
                    successful_samples += 1
                    
                except Exception as e:
                    logger.warning(f"Ã–rnek {idx} atlandÄ±: {e}")
                    continue
            
            pbar.close()
            
            if successful_samples < 10:
                # Yetersiz veri durumunda dummy veri Ã¼ret
                logger.warning("Yetersiz baÅŸarÄ±lÄ± Ã¶rnek, dummy veri Ã¼retiliyor...")
                states = np.random.random((100, self.state_dim))
                actions = np.random.randint(0, self.action_dim, 100)
                rewards = np.random.random(100)
            
            # Convert to numpy
            states = np.array(states)
            actions_onehot = tf.keras.utils.to_categorical(actions, num_classes=self.action_dim)
            rewards = np.array(rewards)
            
            logger.info(f"âœ… {len(states)} Ã¶rnek hazÄ±rlandÄ±")
            
            return states, actions_onehot, rewards
            
        except Exception as e:
            logger.error(f"âŒ Training data preparation failed: {e}")
            raise
    
    def train(
        self,
        states: np.ndarray,
        actions: np.ndarray,
        rewards: np.ndarray,
        epochs: int = 30,  # Daha az epoch
        validation_split: float = 0.2
    ):
        """Model eÄŸit - Enhanced with better callbacks"""
        logger.info("Model eÄŸitiliyor...")
        
        try:
            # Scaler fit
            self.scaler.fit(states)
            states_scaled = self.scaler.transform(states)
            
            # Reward normalization
            if len(rewards) > 0 and rewards.std() > 0:
                rewards_normalized = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
            else:
                rewards_normalized = rewards
            
            # Weighted loss
            sample_weights = (rewards_normalized + 1.0) / 2.0
            
            # Callbacks
            callbacks_list = [
                callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=8,
                    restore_best_weights=True,
                    verbose=1
                ),
                callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.7,
                    patience=4,
                    min_lr=1e-6,
                    verbose=1
                )
            ]
            
            # Model checkpoint
            models_dir = 'models'
            os.makedirs(models_dir, exist_ok=True)
            
            callbacks_list.append(
                callbacks.ModelCheckpoint(
                    f'{models_dir}/rl_agent_model_best.h5',
                    monitor='val_loss',
                    save_best_only=True,
                    verbose=1
                )
            )
            
            # Train
            history = self.model.fit(
                states_scaled,
                actions,
                epochs=epochs,
                batch_size=self.batch_size,
                validation_split=validation_split,
                sample_weight=sample_weights,
                callbacks=callbacks_list,
                verbose=1
            )
            
            logger.info("âœ… Model eÄŸitimi tamamlandÄ±")
            return history
            
        except Exception as e:
            logger.error(f"âŒ Model training failed: {e}")
            raise
    
    def save_model(self, model_path: str = 'models/rl_agent_model.h5'):
        """Modeli kaydet - Enhanced error handling"""
        try:
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            
            # Best model'i kopyala
            best_model_path = 'models/rl_agent_model_best.h5'
            if os.path.exists(best_model_path):
                import shutil
                shutil.copy(best_model_path, model_path)
                logger.info(f"âœ… Model kaydedildi: {model_path}")
            else:
                self.model.save(model_path)
                logger.info(f"âœ… Model kaydedildi: {model_path}")
            
            # Scaler kaydet
            scaler_path = model_path.replace('.h5', '_scaler.pkl')
            joblib.dump(self.scaler, scaler_path)
            logger.info(f"âœ… Scaler kaydedildi: {scaler_path}")
            
        except Exception as e:
            logger.error(f"âŒ Model saving failed: {e}")
            raise
    
    def evaluate(self, states: np.ndarray, actions: np.ndarray) -> Dict:
        """Model deÄŸerlendir - Enhanced metrics"""
        try:
            states_scaled = self.scaler.transform(states)
            predictions = self.model.predict(states_scaled, verbose=0)
            predicted_actions = np.argmax(predictions, axis=1)
            true_actions = np.argmax(actions, axis=1)
            
            accuracy = np.mean(predicted_actions == true_actions)
            
            # Action distribution
            action_dist = np.bincount(predicted_actions, minlength=self.action_dim)
            
            return {
                'accuracy': float(accuracy),
                'action_distribution': action_dist.tolist(),
                'total_samples': len(states)
            }
            
        except Exception as e:
            logger.error(f"âŒ Model evaluation failed: {e}")
            return {'error': str(e)}


def main():
    """Ana eÄŸitim fonksiyonu - Enhanced with comprehensive error handling"""
    print("="*80)
    print("ğŸ¤– RL AGENT TRAINING - FIXED VERSION")
    print("="*80)
    
    try:
        # Trainer
        trainer = RLAgentTrainer(
            state_dim=200,
            action_dim=4,
            learning_rate=0.001,
            batch_size=32,
            force_cpu=False
        )
        
        # Model oluÅŸtur
        print("\nğŸ—ï¸ Model oluÅŸturuluyor...")
        trainer.build_model()
        trainer.model.summary()
        
        # Veri yÃ¼kle
        print("\nğŸ“Š Veri yÃ¼kleniyor...")
        values = trainer.load_data(db_path='data/jetx_data.db', min_history=100)
        
        # AllModelsPredictor yÃ¼kle (fallback mekanizmasÄ± ile)
        print("\nğŸ“¦ Modeller yÃ¼kleniyor...")
        all_models_predictor = AllModelsPredictor()
        
        try:
            load_results = all_models_predictor.load_all_models()
            loaded_count = sum(load_results.values())
            print(f"âœ… {loaded_count} model yÃ¼klendi")
            
            if loaded_count == 0:
                print("âš ï¸ HiÃ§bir model yÃ¼klenemedi, fallback tahminler kullanÄ±lacak")
                
        except Exception as e:
            print(f"âš ï¸ Model yÃ¼kleme baÅŸarÄ±sÄ±z: {e}")
            print("Fallback tahminler kullanÄ±lacak")
            all_models_predictor = type('FallbackPredictor', (), {
                'predict_all': lambda self, history: {
                    'consensus': {
                        'prediction': np.mean(history) + np.random.normal(0, 0.2),
                        'confidence': 0.6 + np.random.random() * 0.2,
                        'above_threshold': np.random.random() > 0.4
                    }
                }
            })()
        
        # EÄŸitim verisi hazÄ±rla
        print("\nğŸ”§ EÄŸitim verisi hazÄ±rlanÄ±yor...")
        states, actions, rewards = trainer.prepare_training_data(
            values=values,
            all_models_predictor=all_models_predictor,
            window_size=min(200, len(values)-1),
            sample_ratio=0.1
        )
        
        # EÄŸit
        print("\nğŸš€ Model eÄŸitimi baÅŸlatÄ±lÄ±yor...")
        history = trainer.train(
            states=states,
            actions=actions,
            rewards=rewards,
            epochs=20,
            validation_split=0.2
        )
        
        # DeÄŸerlendir
        print("\nğŸ“ˆ Model deÄŸerlendiriliyor...")
        eval_results = trainer.evaluate(states, actions)
        
        if 'error' not in eval_results:
            print(f"âœ… DoÄŸruluk: {eval_results['accuracy']:.2%}")
            print(f"   Action daÄŸÄ±lÄ±mÄ±: {eval_results['action_distribution']}")
            print(f"   Toplam Ã¶rnek: {eval_results['total_samples']}")
        else:
            print(f"âŒ DeÄŸerlendirme baÅŸarÄ±sÄ±z: {eval_results['error']}")
        
        # Kaydet
        print("\nğŸ’¾ Model kaydediliyor...")
        trainer.save_model('models/rl_agent_model.h5')
        
        # Model info
        model_info = {
            'model': 'RL_Agent',
            'version': '2.2-FIXED',
            'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'state_dim': trainer.state_dim,
            'action_dim': trainer.action_dim,
            'training_samples': len(states),
            'final_accuracy': eval_results.get('accuracy', 0.0),
            'action_distribution': eval_results.get('action_distribution', [0, 0, 0, 0])
        }
        
        try:
            with open('models/rl_agent_info.json', 'w') as f:
                json.dump(model_info, f, indent=2)
        except: pass
        
        print("\n" + "="*80)
        print("âœ… RL AGENT EÄÄ°TÄ°MÄ° BAÅARIYLA TAMAMLANDI!")
        
    except Exception as e:
        print(f"\nâŒ KRÄ°TÄ°K HATA: {e}")
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
