"""
Focal Loss - Zor Ã–rneklere Odaklanan Loss Fonksiyonu
Lazy learning problemini Ã§Ã¶zmek iÃ§in kullanÄ±lÄ±r.

Focal Loss, kolay tahmin edilen Ã¶rneklerin loss'unu azaltÄ±rken,
zor Ã¶rneklerin loss'unu artÄ±rÄ±r. Bu sayede model minority class'Ä±
(1.5 altÄ±) daha iyi Ã¶ÄŸrenir.

Referans: https://arxiv.org/abs/1708.02002
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
from typing import Optional

# CatBoost iÃ§in numpy tabanlÄ± Focal Loss
class CatBoostFocalLoss(object):
    '''
    CatBoost iÃ§in Focal Loss implementasyonu.
    Gradyan ve Hessian'Ä± numpy kullanarak hesaplar.
    '''
    def __init__(self, alpha=0.75, gamma=2.0):
        self.alpha = alpha
        self.gamma = gamma

    def calc_ders_range(self, approxes, targets, weights):
        # approxes: modelin ham Ã§Ä±ktÄ±sÄ± (logits)
        # targets: gerÃ§ek etiketler (0 veya 1)
        
        # Sigmoid fonksiyonu ile olasÄ±lÄ±ÄŸa Ã§evir
        p = 1. / (1. + np.exp(-approxes))
        
        # Hata
        error = p - targets
        
        # Gradyan (loss'un birinci tÃ¼revi)
        # Standart LogLoss gradyanÄ±: p - y
        # Focal Loss modÃ¼lasyon faktÃ¶rÃ¼: alpha * (1-p)^gamma veya (1-alpha) * p^gamma
        # Ã–NEMLÄ°: targets=0 (azÄ±nlÄ±k sÄ±nÄ±fÄ± - 1.5 altÄ±) alpha ile aÄŸÄ±rlÄ±klandÄ±rÄ±lÄ±r
        grad_modulator = np.where(targets == 0, self.alpha * np.power(1. - p, self.gamma), (1. - self.alpha) * np.power(p, self.gamma))
        
        # Odaklanma teriminin tÃ¼revi
        focus_term_grad = np.where(targets == 1, -self.gamma * p * np.log(p), self.gamma * (1. - p) * np.log(1. - p))
        
        # Gradyan
        grad = grad_modulator * (error + focus_term_grad)

        # Hessian (loss'un ikinci tÃ¼revi)
        # Hessian'Ä± basitleÅŸtirmek iÃ§in p*(1-p) kullanÄ±yoruz (LogLoss'tan gelir)
        # Bu, eÄŸitimin stabilitesini artÄ±rÄ±r.
        # Ã–NEMLÄ°: targets=0 (azÄ±nlÄ±k sÄ±nÄ±fÄ±) alpha ile aÄŸÄ±rlÄ±klandÄ±rÄ±lÄ±r
        hess_modulator = np.where(targets == 0, self.alpha * np.power(1. - p, self.gamma - 1.) * (p * self.gamma * (1. - p) * np.log(p) + p - 1.), (1. - self.alpha) * np.power(p, self.gamma - 1.) * (1. - p * self.gamma * np.log(1. - p) - p))
        hess = p * (1. - p) * grad_modulator * hess_modulator

        return list(zip(grad, hess))


class FocalLoss(keras.losses.Loss):
    """
    Focal Loss implementasyonu
    
    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)
    
    Args:
        alpha: Minority class aÄŸÄ±rlÄ±ÄŸÄ± (0-1). VarsayÄ±lan: 0.75
        gamma: Odaklanma parametresi. YÃ¼ksek gamma = zor Ã¶rneklere daha fazla odaklan
               VarsayÄ±lan: 2.0
        from_logits: True ise model output logits, False ise probabilities
        label_smoothing: Label smoothing faktÃ¶rÃ¼ (0-1)
    """
    
    def __init__(
        self,
        alpha: float = 0.75,
        gamma: float = 2.0,
        from_logits: bool = False,
        label_smoothing: float = 0.0,
        name: str = 'focal_loss'
    ):
        super().__init__(name=name)
        self.alpha = alpha
        self.gamma = gamma
        self.from_logits = from_logits
        self.label_smoothing = label_smoothing
    
    def call(self, y_true, y_pred):
        """
        Loss hesapla
        
        Args:
            y_true: GerÃ§ek labels (0 veya 1)
            y_pred: Model predictions (probabilities veya logits)
            
        Returns:
            Focal loss deÄŸeri
        """
        # Label smoothing uygula
        if self.label_smoothing > 0:
            y_true = y_true * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing
        
        # Logits'ten probability'ye Ã§evir
        if self.from_logits:
            y_pred = tf.sigmoid(y_pred)
        
        # Numerical stability iÃ§in clip
        epsilon = keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
        
        # p_t hesapla: doÄŸru sÄ±nÄ±fÄ±n olasÄ±lÄ±ÄŸÄ±
        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        
        # Alpha_t hesapla: sÄ±nÄ±f aÄŸÄ±rlÄ±ÄŸÄ±
        # Ã–NEMLÄ°: y_true=0 (azÄ±nlÄ±k sÄ±nÄ±fÄ± - 1.5 altÄ±) alpha ile aÄŸÄ±rlÄ±klandÄ±rÄ±lÄ±r
        alpha_t = tf.where(tf.equal(y_true, 0), self.alpha, 1 - self.alpha)
        
        # Focal weight: (1 - p_t)^gamma
        focal_weight = tf.pow(1.0 - p_t, self.gamma)
        
        # Cross entropy
        cross_entropy = -tf.math.log(p_t)
        
        # Focal loss = alpha_t * focal_weight * cross_entropy
        focal_loss = alpha_t * focal_weight * cross_entropy
        
        return tf.reduce_mean(focal_loss)
    
    def get_config(self):
        """KonfigÃ¼rasyonu dÃ¶ndÃ¼r"""
        config = super().get_config()
        config.update({
            'alpha': self.alpha,
            'gamma': self.gamma,
            'from_logits': self.from_logits,
            'label_smoothing': self.label_smoothing
        })
        return config


def focal_loss_function(
    alpha: float = 0.75,
    gamma: float = 2.0,
    from_logits: bool = False
):
    """
    Focal loss function factory
    
    Args:
        alpha: Minority class weight
        gamma: Focusing parameter
        from_logits: If True, expects logits
        
    Returns:
        Focal loss function
    """
    def loss(y_true, y_pred):
        return FocalLoss(alpha=alpha, gamma=gamma, from_logits=from_logits)(y_true, y_pred)
    return loss


class BinaryFocalLoss(keras.losses.Loss):
    """
    Binary classification iÃ§in optimize edilmiÅŸ Focal Loss
    
    Progressive NN'nin threshold output'u iÃ§in kullanÄ±lÄ±r.
    """
    
    def __init__(
        self,
        alpha: float = 0.75,
        gamma: float = 2.0,
        name: str = 'binary_focal_loss'
    ):
        super().__init__(name=name)
        self.alpha = alpha
        self.gamma = gamma
    
    def call(self, y_true, y_pred):
        """Binary focal loss hesapla"""
        # Sigmoid uygula (eÄŸer logits ise)
        y_pred = tf.sigmoid(y_pred)
        
        # Clip for stability
        epsilon = keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
        
        # Binary cross entropy
        bce = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))
        
        # Focal weight
        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        focal_weight = tf.pow(1.0 - p_t, self.gamma)
        
        # Alpha weight
        # Ã–NEMLÄ°: y_true=0 (azÄ±nlÄ±k sÄ±nÄ±fÄ± - 1.5 altÄ±) alpha ile aÄŸÄ±rlÄ±klandÄ±rÄ±lÄ±r
        alpha_t = tf.where(tf.equal(y_true, 0), self.alpha, 1 - self.alpha)
        
        # Final loss
        focal_loss = alpha_t * focal_weight * bce
        
        return tf.reduce_mean(focal_loss)
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'alpha': self.alpha,
            'gamma': self.gamma
        })
        return config


class AdaptiveFocalLoss(keras.losses.Loss):
    """
    Adaptive Focal Loss - EÄŸitim sÄ±rasÄ±nda parametreleri otomatik ayarlar
    
    Gamma parametresini epoch'lara gÃ¶re ayarlar:
    - BaÅŸlangÄ±Ã§: DÃ¼ÅŸÃ¼k gamma (kolay Ã¶rneklere de odaklan)
    - Ortalar: Orta gamma (dengeli)
    - Son: YÃ¼ksek gamma (sadece zor Ã¶rneklere odaklan)
    """
    
    def __init__(
        self,
        alpha: float = 0.75,
        gamma_start: float = 1.0,
        gamma_end: float = 3.0,
        name: str = 'adaptive_focal_loss'
    ):
        super().__init__(name=name)
        self.alpha = alpha
        self.gamma_start = gamma_start
        self.gamma_end = gamma_end
        self.current_gamma = tf.Variable(gamma_start, trainable=False, dtype=tf.float32)
    
    def update_gamma(self, epoch: int, total_epochs: int):
        """
        Gamma'yÄ± epoch'a gÃ¶re gÃ¼ncelle
        
        Args:
            epoch: Mevcut epoch (0-based)
            total_epochs: Toplam epoch sayÄ±sÄ±
        """
        # Linear interpolation
        progress = epoch / max(1, total_epochs - 1)
        new_gamma = self.gamma_start + (self.gamma_end - self.gamma_start) * progress
        self.current_gamma.assign(new_gamma)
    
    def call(self, y_true, y_pred):
        """Adaptive focal loss hesapla"""
        # Sigmoid
        y_pred = tf.sigmoid(y_pred)
        
        # Clip
        epsilon = keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
        
        # p_t
        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        
        # alpha_t
        # Ã–NEMLÄ°: y_true=0 (azÄ±nlÄ±k sÄ±nÄ±fÄ± - 1.5 altÄ±) alpha ile aÄŸÄ±rlÄ±klandÄ±rÄ±lÄ±r
        alpha_t = tf.where(tf.equal(y_true, 0), self.alpha, 1 - self.alpha)
        
        # Focal weight (adaptive gamma kullan)
        focal_weight = tf.pow(1.0 - p_t, self.current_gamma)
        
        # Cross entropy
        cross_entropy = -tf.math.log(p_t)
        
        # Loss
        focal_loss = alpha_t * focal_weight * cross_entropy
        
        return tf.reduce_mean(focal_loss)
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'alpha': self.alpha,
            'gamma_start': self.gamma_start,
            'gamma_end': self.gamma_end
        })
        return config


class CombinedLoss(keras.losses.Loss):
    """
    Birden fazla loss'u birleÅŸtir
    
    Progressive NN iÃ§in:
    - Regression: MAE veya MSE
    - Classification: Categorical Crossentropy
    - Threshold: Focal Loss
    """
    
    def __init__(
        self,
        regression_loss: keras.losses.Loss,
        classification_loss: keras.losses.Loss,
        threshold_loss: keras.losses.Loss,
        regression_weight: float = 0.40,
        classification_weight: float = 0.15,
        threshold_weight: float = 0.45,
        name: str = 'combined_loss'
    ):
        super().__init__(name=name)
        self.regression_loss = regression_loss
        self.classification_loss = classification_loss
        self.threshold_loss = threshold_loss
        self.regression_weight = regression_weight
        self.classification_weight = classification_weight
        self.threshold_weight = threshold_weight
        
        # Normalize weights
        total = regression_weight + classification_weight + threshold_weight
        self.regression_weight /= total
        self.classification_weight /= total
        self.threshold_weight /= total
    
    def call(self, y_true, y_pred):
        """
        Combined loss hesapla
        
        NOT: Bu fonksiyon multi-output model iÃ§in kullanÄ±lÄ±r.
        y_true ve y_pred dict olmalÄ±: {'regression': ..., 'classification': ..., 'threshold': ...}
        """
        # Her output iÃ§in loss hesapla
        reg_loss = self.regression_loss(y_true['regression'], y_pred['regression'])
        cls_loss = self.classification_loss(y_true['classification'], y_pred['classification'])
        thr_loss = self.threshold_loss(y_true['threshold'], y_pred['threshold'])
        
        # Weighted sum
        total_loss = (
            self.regression_weight * reg_loss +
            self.classification_weight * cls_loss +
            self.threshold_weight * thr_loss
        )
        
        return total_loss
    
    def get_config(self):
        config = super().get_config()
        config.update({
            'regression_weight': self.regression_weight,
            'classification_weight': self.classification_weight,
            'threshold_weight': self.threshold_weight
        })
        return config


# Callback for Adaptive Focal Loss
class AdaptiveFocalLossCallback(keras.callbacks.Callback):
    """
    Adaptive Focal Loss iÃ§in callback
    
    Her epoch baÅŸÄ±nda gamma parametresini gÃ¼nceller
    """
    
    def __init__(self, adaptive_focal_loss: AdaptiveFocalLoss, total_epochs: int):
        super().__init__()
        self.adaptive_focal_loss = adaptive_focal_loss
        self.total_epochs = total_epochs
    
    def on_epoch_begin(self, epoch, logs=None):
        """Epoch baÅŸÄ±nda gamma'yÄ± gÃ¼ncelle"""
        self.adaptive_focal_loss.update_gamma(epoch, self.total_epochs)
        current_gamma = float(self.adaptive_focal_loss.current_gamma.numpy())
        print(f"\nðŸ“Š Adaptive Focal Loss: Gamma = {current_gamma:.3f}")


# Utility functions
def create_focal_loss(
    loss_type: str = 'focal',
    alpha: float = 0.75,
    gamma: float = 2.0,
    **kwargs
) -> keras.losses.Loss:
    """
    Focal loss factory function
    
    Args:
        loss_type: 'focal', 'binary_focal', 'adaptive_focal'
        alpha: Minority class weight
        gamma: Focusing parameter
        **kwargs: Additional parameters
        
    Returns:
        Loss instance
    """
    if loss_type == 'focal':
        return FocalLoss(alpha=alpha, gamma=gamma, **kwargs)
    elif loss_type == 'binary_focal':
        return BinaryFocalLoss(alpha=alpha, gamma=gamma)
    elif loss_type == 'adaptive_focal':
        gamma_start = kwargs.get('gamma_start', 1.0)
        gamma_end = kwargs.get('gamma_end', 3.0)
        return AdaptiveFocalLoss(alpha=alpha, gamma_start=gamma_start, gamma_end=gamma_end)
    else:
        raise ValueError(f"Unknown loss type: {loss_type}")


def test_focal_loss():
    """Focal loss test fonksiyonu"""
    print("ðŸ§ª Focal Loss Test")
    print("=" * 60)
    
    # Dummy data
    y_true = np.array([[0], [0], [1], [1], [0], [1]])
    y_pred_logits = np.array([[0.1], [0.2], [0.8], [0.9], [0.3], [0.7]])
    
    # Standard binary crossentropy
    bce = keras.losses.BinaryCrossentropy()
    bce_loss = bce(y_true, y_pred_logits)
    
    # Focal loss
    focal = FocalLoss(alpha=0.75, gamma=2.0)
    focal_loss_val = focal(y_true, y_pred_logits)
    
    print(f"Binary Crossentropy Loss: {bce_loss:.4f}")
    print(f"Focal Loss (Î±=0.75, Î³=2.0): {focal_loss_val:.4f}")
    print("\nâœ… Focal Loss Ã§alÄ±ÅŸÄ±yor!")
    print("=" * 60)


if __name__ == "__main__":
    test_focal_loss()
